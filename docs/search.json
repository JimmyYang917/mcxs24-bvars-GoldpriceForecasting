[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An Evidence-based Forecast: Gold as a Traditional Safe-Haven Investment",
    "section": "",
    "text": "Abstract. This research aims to explore future trends in Gold prices as a traditional safe-haven investment using a Bayesian VARs model. In the wake of the 2008 financial crisis and especially the 2019 global Covid-19 pandemic, the world economy appears to be on the brink of a looming risk: a world-wide economic recession. The concern over the risk of investment returns has become a primary focus for global investors and financial institutions. This unease has been further exacerbated by geopolitical conflicts such as the Russia-Ukraine war (2022) and the Israeli-Palestinian conflict (2023). Consequently, this research aims to provide a briefly discussion and data-driven forecast of traditional safe-haven assets: Gold, under current circumstances. Factors considered include emerging safe-haven investments, risk-free investment assets, comparable investments, market returns, inflation on both demand and supply sides, broad money supply (M2), interest rates, unemployment rates and market volatility.\nKeywords. Bayesian VARs, Gold price, Inflation, Interest rate, Unemployment, US Bond Yield, Safe-haven Assets, Forecasting, Volatility, R, Quarto."
  },
  {
    "objectID": "index.html#matrix-notation-for-the-model",
    "href": "index.html#matrix-notation-for-the-model",
    "title": "An Evidence-based Forecast: Gold as a Traditional Safe-Haven Investment",
    "section": "4.1 Matrix Notation for the model:",
    "text": "4.1 Matrix Notation for the model:\nMatrix form are used to simplify the notation and the derivations. Let \\(T\\) be the available sample size for the variable \\(y\\) and \\(K\\) be the sum of lags and constant term (\\(K = 1 + pN\\)). Define a identity matrix of order \\(T\\), \\(I_T\\), as well as following matrix:\n\\[\\begin{aligned}\n\nA=\n\\begin{bmatrix}\n\\mu_{0}' \\\\ A_{1}' \\\\\nA_{2} '\\\\.\\\\.\\\\.\\\\A_{p}'\n\\end{bmatrix}_{K \\times N}\n\nY=\n\\begin{bmatrix}\ny_{1}' \\\\ y_{2}' \\\\\ny_{3} '\\\\.\\\\.\\\\.\\\\y_{T}'\n\\end{bmatrix}_{T \\times N}\n\nx_t=\n\\begin{bmatrix}\n1 \\\\ y_{t-1}' \\\\\ny_{t-2} '\\\\.\\\\.\\\\.\\\\y_{t-p}'\n\\end{bmatrix}_{K \\times 1}\n\nX=\n\\begin{bmatrix}\nx_{1}' \\\\ x_{2}' \\\\\nx_{3} '\\\\.\\\\.\\\\.\\\\x_{T}'\n\\end{bmatrix}_{T \\times K}\n\nE=\n\\begin{bmatrix}\n\\epsilon_1' \\\\ \\epsilon_2' \\\\\n\\epsilon_3 '\\\\.\\\\.\\\\.\\\\\\epsilon_T'\n\\end{bmatrix}_{T \\times N}\n\n\\end{aligned}\\]\nThen the model can be written in a concise notation as:\n\\[\\begin{aligned}\n\nY &= X A +E\n\\\\\n\\\\\nE|X &\\sim \\mathcal{MN}_{T\\times N}(\\textbf{0},\\Sigma,I_T)\n\n\\end{aligned}\\]\nGiven that the density function of Matrix-variate Normal distribution \\(Z\\sim \\mathcal{MN}_{T\\times N}(M,Q,P)\\) is:\n\\[\\begin{aligned}\n\n\\mathcal{MN}_{T\\times N}(M,Q,P) & =c^{-1}_{mn}exp\\left\\{ -\\frac{1}{2}\\textbf{tr}\\left[ Q^{-1}\\left( Z-M \\right)'P^{-1}\\left( Z-M \\right) \\right] \\right\\}\\\\\nc_{mn} & = \\left( 2\\pi \\right)^{\\frac{TN}{2}}det\\left( Q \\right)^{\\frac{T}{2}}det\\left( P \\right)^{\\frac{N}{2}}\n\n\\end{aligned}\\]\nBase on the model above, we could first turn B Vars(p) model into B Vars(1) model and easily regress to have the parameter matrix. Then we could have a \\(t+h\\) period forward forecasting with increase of variance, in this case: \\(h\\) = 24.\nThe main focus of estimate output is the conditional mean of Gold price, which base on current information set \\(Y_{t-1}\\). It provide the average mean prediction of Gold price which investors and financial institutions interested in. Moreover, 1 standard deviation and 2 standard deviation will also produced in forecasting process to provide a 68% and 95% of confidence intervals of future Gold price movements in \\(h\\) periods base on current information set.\nFurthermore, different prior distribution might be used to provide different level of uncertainty of current environment(information set). Compare the difference of Gold price under different priors could help to prove the Gold as a high quality safe-haven investment and increase investors and financial institutions confidence and further expectations. (Competitors for golds might also be used under different priors, such as Nasdaq Index and short to mid-term treasury bills.)"
  },
  {
    "objectID": "index.html#likelihood-function",
    "href": "index.html#likelihood-function",
    "title": "An Evidence-based Forecast: Gold as a Traditional Safe-Haven Investment",
    "section": "4.2 Likelihood Function",
    "text": "4.2 Likelihood Function\nThe model equation imply the predictive density of the data vector \\(Y\\). We could consider the model equation is the linear transformation of the matrix-variate normal distribution \\(E\\). Therefore, the data vector also follows a matrix-variate normal distribution given by:\n\\[\\begin{aligned}\n\nY|X,A,\\Sigma &\\sim \\mathcal{MN}_{T\\times N}(XA,\\Sigma,I_T)\n\n\\end{aligned}\\]\nThis distribution determines the shape of the likelihood function that is defined as the sampling data density:\n\\[\\begin{aligned}\n\nL(A,\\Sigma | Y,X)\\equiv P(Y|X,A,\\Sigma)\n\n\\end{aligned}\\]\nThe likelihood function for the parameters estimation (\\(A,\\Sigma\\)), and after plugging in data in place of \\(Y,X\\), is considered a function of parameters \\(A\\) and \\(\\Sigma\\) is given by:\n\\[\\begin{aligned}\n\nL(A,\\Sigma | Y,X) &= \\left( 2\\pi \\right)^{-\\frac{TN}{2}}det\\left( \\Sigma \\right)^{-\\frac{T}{2}}exp\\left\\{ -\\frac{1}{2}\\sum_{t=1}^{T}\\epsilon_{t}'\\Sigma^{-1}\\epsilon_{t} \\right\\}\n\\\\\n&= \\left( 2\\pi \\right)^{-\\frac{TN}{2}}det\\left( \\Sigma \\right)^{-\\frac{T}{2}}exp\\left\\{ -\\frac{1}{2}\\sum_{t=1}^{T} \\left( y_{t}-A'x_{t} \\right)'\\Sigma^{-1}\\left( y_{t}-A'x_{t} \\right)\\right\\}\n\\\\\n&= \\left( 2\\pi \\right)^{-\\frac{TN}{2}}det\\left( \\Sigma \\right)^{-\\frac{T}{2}}exp\\left\\{ -\\frac{1}{2}vec\\left(\\left( Y-XA \\right)'\\right)' \\left( I_T\\otimes \\Sigma^{-1} vec\\left(\\left( Y-XA \\right)'\\right)\\right)\\right\\}\n\\\\\n&= \\left( 2\\pi \\right)^{-\\frac{TN}{2}}det\\left( \\Sigma \\right)^{-\\frac{T}{2}}exp\\left\\{ -\\frac{1}{2}\\textbf{tr}\\left[\\Sigma^{-1}\\left( Y-XA \\right)'I^{-1}_T\\left(Y-XA \\right) \\right] \\right\\}\n\\\\\n&= \\left( 2\\pi \\right)^{-\\frac{TN}{2}}det\\left( \\Sigma \\right)^{-\\frac{T}{2}}exp\\left\\{ -\\frac{1}{2}\\textbf{tr}\\left[\\Sigma^{-1}\\left(Y-XA\\right)'\\left(Y-XA\\right) \\right] \\right\\}\n\n\\end{aligned}\\]\nGiven that the trace:\n\n\\(\\textbf{tr}\\left( X \\right) = \\sum_{n=1}^{N}X_{nn} \\ \\ \\ \\ \\text{for a }N \\times N\\text{ matrix }X\\)\n\\(\\textbf{tr}\\left( ABCD \\right) = vec(D')'\\left( C'\\otimes A \\right)vec(B)\\)"
  },
  {
    "objectID": "index.html#prior-distribution",
    "href": "index.html#prior-distribution",
    "title": "An Evidence-based Forecast: Gold as a Traditional Safe-Haven Investment",
    "section": "4.3 Prior Distribution",
    "text": "4.3 Prior Distribution\nFor the given model, we assume that unknown parameters have following distributions(Stock and Watson 2001):\n\\[\\begin{aligned}\n\nA|\\Sigma &\\sim \\mathcal{MN}_{K\\times N}(M,\\Sigma,P)\n\\\\\n\\Sigma &\\sim \\mathcal{IW}_{N}(S,\\nu)\n\n\\end{aligned}\\]\nwhere \\(A_{K \\times N}|\\Sigma\\) follow a matrix-variate normal distribution:\n- \\(M\\) is the mean of matrix normal distribution\n- \\(\\Sigma_{N \\times N}\\)is the row specific covariance matrix\n- \\(P_{K \\times K}\\) is the column specific covariance matrix with the density given by:\n\\[\\begin{aligned}\n\n\\mathcal{MN}_{K\\times N}(M,\\Sigma,P) & =c^{-1}_{mn}exp\\left\\{ -\\frac{1}{2}\\textbf{tr}\\left[ \\Sigma^{-1}\\left( A-M \\right)'P^{-1}\\left( A-M \\right) \\right] \\right\\}\\\\\nc_{mn} & = \\left( 2\\pi \\right)^{\\frac{KN}{2}}det\\left( \\Sigma \\right)^{\\frac{K}{2}}det\\left( P \\right)^{\\frac{N}{2}}\n\n\\end{aligned}\\]\nAnd \\(\\Sigma\\) follow a Inverse Wishart distribution:\n- \\(S\\) is N Ã— N positive definite symmetric matrix called the scale matrix.\n- \\(\\nu\\) &gt; N + 2 denotes degrees of freedom. with the density given by:\n\\[\\begin{aligned}\n\n\\mathcal{IW}_{N}(S,\\nu) &= c^{-1}_{iw}det(\\Sigma)^{-\\frac{\\nu + N+1}{2}}exp\\left\\{ -\\frac{1}{2} \\textbf{tr}\\left[ \\Sigma^{-1} S\\right]\\right\\}\\\\\nc_{iw} &= 2^{\\frac{\\nu N}{2}}\\pi^{\\frac{N(N-1)}{4}}\\prod_{n=1}^{N}\\Gamma\\left( \\frac{\\nu + n + 1}{2} \\right)det(S)^{-\\frac{\\nu}{2}}\n\n\\end{aligned}\\]\nThen the joint distribution of (\\(A,\\Sigma\\)) is Normal-Inverse Wishart:\n\\[\\begin{aligned}\n\nP(A,\\Sigma) & \\sim \\mathcal{NIW}_{K \\times N}(M,P,S,\\nu)\n\n\\end{aligned}\\]\nwith the density given by:\n\\[\\begin{aligned}\n\nP(A,\\Sigma) &= c^{-1}_{nw}det(\\Sigma)^{-\\frac{\\nu + N + K + 1}{2}}\\\\\n&\\times exp\\left\\{ -\\frac{1}{2}\\textbf{tr}\\left[ \\Sigma^{-1}\\left( A-M \\right)'P^{-1}\\left( A-M \\right) \\right] \\right\\}\\\\\n&\\times exp\\left\\{ -\\frac{1}{2} \\textbf{tr}\\left[ \\Sigma^{-1} S\\right]\\right\\}\n\\\\\n\\\\\nc_{nw} &= 2^{\\frac{N(K + \\nu)}{2}}\\pi^{\\frac{N(N + 2K -1)}{4}}[\\prod_{n=1}^{N}\\Gamma\\left( \\frac{\\nu + 1 - n}{2} \\right)]det\\left( P \\right)^{\\frac{N}{2}}det(S)^{-\\frac{\\nu}{2}}\n\n\\end{aligned}\\]\n\n4.3.1 Natural-conjugate prior distribution\nLeads to joint posterior distribution for (\\(A,\\Sigma\\)) has the same form as prior:\n\\[\\begin{aligned}\n\nP(A,\\Sigma) &= P(A|\\Sigma)P(\\Sigma)\n\\\\\nA|\\Sigma &\\sim \\mathcal{MN}_{K\\times N}(\\underline{A},\\Sigma,\\underline{V})\n\\\\\n\\Sigma &\\sim \\mathcal{IW}_{N}(\\underline{S}, \\underline{\\nu})\n\n\\end{aligned}\\]\nwith the kernel given by:\n\\[\\begin{aligned}\n\nP(A,\\Sigma) &\\propto \\det(\\Sigma)^{-\\frac{N+K+\\underline{\\nu}+1}{2}} \\\\\n&\\times exp\\{-\\frac{1}{2}tr[\\Sigma^{-1}(A-\\underline{A}) \\underline{V}^{-1}(A-\\underline{A})]\\} \\\\\n&\\times exp\\{-\\frac{1}{2}tr[\\Sigma^{-1}\\underline{S}]\\}\n\n\\end{aligned}\\]"
  },
  {
    "objectID": "index.html#benchmark-model-with-minnesota-prior",
    "href": "index.html#benchmark-model-with-minnesota-prior",
    "title": "An Evidence-based Forecast: Gold as a Traditional Safe-Haven Investment",
    "section": "4.4 Benchmark model with Minnesota prior",
    "text": "4.4 Benchmark model with Minnesota prior\nIn real life, macroeconomic variables are more likely being unit-root non stationary and are well-characterized by a multivariate random walk processï¼š\n\\[\\begin{aligned}\n\ny_{t} = y_{t-1} + \\epsilon_t\n\n\\end{aligned}\\]\nTherefore, our benchmark model uses Minnesota prior(1984) based on random walk process for our Bayesian forecasting.\nSet the prior mean \\(A\\) to:\n\\[\\begin{aligned}\n\n\\underline{A}=\\left[ 0_{N\\times1} \\ \\ \\ \\  I_{N} \\ \\ \\ \\ \\ 0_{N\\times(p-1)N} \\right]'\n\n\\end{aligned}\\]\nwhich means the mean of first lag equal to 1 and mean of constant term and other lags are 0.\nSet the column specific prior covariance of \\(A\\) (prior shrinkage)to:\n\\[\\begin{aligned}\n\n\\underline{V} = diag\\left[ \\kappa_{2} \\quad \\kappa_{1}(\\textbf{P} ^{-2}\\otimes \\textbf{i}^{'}_{N}) \\right]\n\n\\end{aligned}\\]\nwhere:\n\n\\(\\textbf{P}\\) is the list of legs, \\(\\textbf{P} \\ \\ \\ =\\left[ 1 \\quad 2 \\quad 3\\quad ... \\quad p \\right]\\)\n\\(\\textbf{i}^{'}_{N}\\) is a \\(N \\times 1\\) vector of ones\n\\(\\kappa_{1}\\): overall shrinkage level for autoregressive slopes\n\\(\\kappa_{2}\\): overall shrinkage for the constant term\n\ntherefore, we could have the variance of \\(A\\):\n\\[\nVAR\\left[ vec\\left( A \\right) \\right]\\quad = \\quad \\Sigma \\ \\ \\otimes \\ \\ \\underline{V}\n\\]\n\n4.4.1 Bayesian Estimations\nBased on Bayesâ€™ Theorem:\n\\[\\begin{aligned}\n\nP(A|B) & =\\frac{P(B|A)P(A)}{P(B)}\n\n\\end{aligned}\\]\nWe could have the kernel of conditional joint posterior distribution \\(P(A,\\Sigma|Y,X)\\), which is the proportion of the product between conditional distribution of data \\(P(Y|X,A,\\Sigma)\\) and joint prior distribution \\(P(A,\\Sigma)\\):\n\\[\\begin{aligned}\n\nP(A,\\Sigma|Y,X) &= \\frac{P(Y|X,A,\\Sigma) \\ P(A,\\Sigma)}{P(Y)}\\\\\n&\\propto P(Y|X,A,\\Sigma) \\ P(A,\\Sigma)\\\\\n&\\propto L(A,\\Sigma | Y,X) \\ P(A|\\Sigma) \\ P(\\Sigma) \\\\\\\\\n\n\\end{aligned}\\]\nAnd the kernel of conditional joint posterior distribution is given by:\n\\[\\begin{aligned}\n\nP(A,\\Sigma|Y,X) &\\propto L(A,\\Sigma | Y,X) \\ P(A,\\Sigma)\\\\\n&\\propto det(\\Sigma)^{-\\frac{T}{2}} \\\\\n&\\times exp\\{-\\frac{1}{2}tr[\\Sigma^{-1}(Y-XA)'(Y-XA)]\\} \\\\\n&\\times \\det(\\Sigma)^{-\\frac{N+K+\\underline{\\nu}+1}{2}} \\\\\n&\\times exp\\{-\\frac{1}{2}tr[\\Sigma^{-1}(A-\\underline{A}) \\underline{V}^{-1}(A-\\underline{A})]\\} \\\\\n&\\times exp\\{-\\frac{1}{2}tr[\\Sigma^{-1}\\underline{S}]\\}\n\n\\end{aligned}\\]\nThen, the kernel could be represent as the normal-inverse Wishart distribution(WoÅºniak 2022):\n\\[\\begin{aligned}\n\nP(A,\\Sigma|Y,X) &\\sim \\mathcal{NIW}_{K\\times N}(\\bar{A},\\bar{V},\\bar{S},\\bar{\\nu})\n\\\\\n\\\\\n\\bar{V} &= (X'X + \\underline{V}^{-1})^{-1} \\\\\n\\bar{A} &= \\bar{V}(X'Y + \\underline{V}^{-1}\\underline{A}) \\\\\n\\bar{\\nu} &= T + \\underline{\\nu}\\\\\n\\bar{S} &= \\underline{S} + Y'Y + \\underline{A}'\\underline{V}^{-1}\\underline{A} - \\bar{A}'\\bar{V}^{-1}\\bar{A}\n\n\\end{aligned}\\]\n\n\n4.4.2 Gibbs sampler: Function Proofing\nConsider Bi-variate Gaussian random walk process:\n\\[\n\\begin{align}\ny_t &=\n\\begin{bmatrix}\ny_{t,1} \\\\\ny_{t,2}\n\\end{bmatrix} =\n\\begin{bmatrix}\ny_{t-1,1} \\\\\ny_{t-1,2}\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\epsilon_{t,1} \\\\\n\\epsilon_{t,2}\n\\end{bmatrix} \\\\\n\\epsilon_{t,1} &\\sim \\mathcal{N}(0,1)  \\\\\n\\epsilon_{t,2} &\\sim \\mathcal{N}(0,1)\n\\end{align}\n\\]\nVariables in Matrix Notation:\n\\[\nY = \\begin{bmatrix}\ny_2' \\\\\ny_3' \\\\\n\\vdots \\\\\ny_n'\n\\end{bmatrix},\n\\quad\nX = \\begin{bmatrix}\n1 \\quad y_1' \\\\\n1 \\quad y_2' \\\\\n\\vdots \\quad \\vdots \\\\\n1 \\quad y_{n-1}'\n\\end{bmatrix}\n\\]\nTherefore, we could basic set up for this Bi-variate Gaussian random walk process with Minnesota prior:\n\ne1 = cumsum(rnorm(1000, 0, sd=1))\ne2 = cumsum(rnorm(1000, 0, sd=1))\ne  = cbind(e1,e2)\n\n## Define data X, Y \nY = ts(e[2:nrow(e),], frequency=1)\nX = matrix(1,nrow(Y),1)\nX = cbind(X,e[2:nrow(e)-1,])\n\n\n## Test on basic model\nN           = ncol(Y)                          # N=2\np           = frequency(Y)\nA.hat       = solve(t(X)%*%X)%*%t(X)%*%Y\nSigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)\n\n# Prior distribution (with Minnesota prior)\nkappa.1             = 0.02^2                              # shrinkage for A1 to Ap\nkappa.2             = 200                                  # shrinkage for constant \nA.prior             = matrix(0,nrow(A.hat),ncol(A.hat))\nA.prior[2:(N + 1),] = diag(N)\nV.prior             = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))\nS.prior             = diag(diag(Sigma.hat))\nnu.prior            = N+2\nI.matrix            = diag(1,nrow(Y),nrow(Y))\n\nFunction below is posterior.draws:\n\n## Posterior sample draw function for basic model(posterior.draws)\nposterior.draws       = function (S, Y, X){\n  \n    # normal-inverse Wishard posterior parameters\n    V.bar.inv         = t(X)%*%X + diag(1/diag(V.prior))\n    V.bar             = solve(V.bar.inv)\n    A.bar             = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)\n    nu.bar            = nrow(Y) + nu.prior\n    S.bar             = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar\n    S.bar.inv         = solve(S.bar)\n    \n    # posterior draws \n    Sigma.posterior   = rWishart(1, df=nu.bar, Sigma=S.bar.inv)\n    Sigma.posterior   = apply(Sigma.posterior,3,solve)\n    Sigma.posterior   = array(Sigma.posterior,c(N,N,S))\n    A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))\n    L                 = t(chol(V.bar))\n    \n    for (s in 1:S){\n      A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])\n    }\n\n    output            = list(A.posterior=A.posterior, Sigma.posterior=Sigma.posterior)\n    return(output)\n}\n\n\nThe posterior mean of the \\(A\\) is:\n\n\n\n\n\n\n\nA\nSimulation Parameter Y1\nSimulation Parameter Y2\n\n\n\n\nConstant term\n0.0610\n0.0807\n\n\nY1 lag\n0.9893\n0.0048\n\n\nY2 lag\n0.0024\n0.9957\n\n\n\n\n\n\n\n\n\nThe posterior mean of the \\(\\Sigma\\) is:\n\n\n\n\n\n\n\nSigma\nSimulation Parameter Y1\nSimulation Parameter Y2\n\n\n\n\nY1 lag\n1.0052\n0.1171\n\n\nY2 lag\n0.1171\n0.9771"
  },
  {
    "objectID": "index.html#the-extended-model-laplace-distribution-of-error-term",
    "href": "index.html#the-extended-model-laplace-distribution-of-error-term",
    "title": "An Evidence-based Forecast: Gold as a Traditional Safe-Haven Investment",
    "section": "4.5 The extended model: Laplace distribution of error term",
    "text": "4.5 The extended model: Laplace distribution of error term\nThe Basic Model is the standard VARs model that assume the error terms \\(U\\) are independent and identically distributed(\\(iid\\)) as \\(U \\ \\sim N_{TN}(0 \\ , \\ \\Sigma)\\). In other formation, it could be presented as \\(vec(U) \\ \\sim N(0 \\ , \\ \\Sigma \\ \\otimes \\  I_{T})\\). Where \\(\\Sigma\\) is a \\(N\\times N\\) cross sectional covariance matrix, \\(I_{t}\\) is a \\(T\\times T\\) identity matrix present serial covariance, \\(\\otimes\\) is the Kronecker product and the operator \\(vec(.)\\) is vectorization that inverts the matrix into the column vector by stacking the columns.\nTherefore, we could consider a more general serial covariance structure:\n\\[\\begin{align}\n\nvec(U) \\ \\sim N(0 \\ , \\ \\Sigma \\ \\otimes \\  \\Omega)\\\\\n\n\\end{align}\\]\nWhere:\n\\[\\begin{align}\n\n\\Omega \\ &= \\ \\ diag\\left[ \\lambda_{1}, \\lambda_{2},...,\\lambda_{t}  \\right]\\\\\n\\lambda \\ &\\sim \\ Exp \\ (\\alpha)\\\\\n\n\\end{align}\\]\nAnd then, the distribution of error terms in extension model will be Laplace distribution instead of the normally distributed errors assumption. The Laplace distribution is well-suited for describing financial anomalies due to its sharp peaks and heavy tails. Utilizing this distribution enhances the modelâ€™s robustness against anomalies, making it particularly appropriate for financial time series analysis. Given that most of our variables are financial time series data, applying a Laplace distribution to the error term is more appropriate.\nFollowing Eltoft,Kim, and Lee 2006b, for covariance with a general Kronecker structure, if each \\({\\lambda}\\) has an independent exponential distribution with mean \\({\\alpha}\\), then marginally \\({U_t}\\) has a multivariate Laplace distribution with mean vector 0 and covariance matrix \\({\\alpha\\Sigma}\\).\n\\[\\begin{align}\n\nU &\\sim \\ Laplace(0 \\ , \\  \\alpha\\Sigma) \\\\\nU_t|\\lambda_t &\\sim \\mathcal{MN}(0 \\ , \\  \\Sigma \\ , \\ \\Omega) \\\\\n\\Omega \\ &= \\ \\ diag\\left[ \\lambda_{1}, \\lambda_{2},...,\\lambda_{t}  \\right] \\ = \\ \\lambda_t \\times \\textbf{I}_T\\\\\n\\lambda_t &\\sim \\ Exp(\\alpha)\n\n\\end{align}\\]\nTherefore, the prior distribution of lambda which is following exponential distribution defined as:\n\\[\\begin{align}\n\nP(\\lambda_t|\\alpha) &= \\frac{1}{\\alpha}exp\\{ -\\frac{1}{\\alpha}\\lambda_t \\}\\\\\n\n\\end{align} \\]\n\n\nFollowing graphs describe the difference between alpha values, we could see that as the mean, alpha, increase. The rates of exponetial function decrease.\n\n\n\n\n\n\n\n\n4.5.1 Bayesian Estimations\nThen, the kernel of the likelihood function could be rewritten as to:\n\\[\\begin{align}\n\nL(A,\\Sigma,\\Omega |Y,X) &\\propto \\det(\\Sigma)^{-\\frac{T}{2}} \\det(\\Omega)^{-\\frac{N}{2}} exp\\{-\\frac{1}{2}\\textbf{tr}[\\Sigma^{-1} (Y-XA)' \\Omega^{-1} (Y-XA) ]\\}\\\\\n&= \\det(\\Sigma)^{-\\frac{T}{2}} \\det(\\left[ \\lambda_{1}, \\lambda_{2},...,\\lambda_{t} \\right])^{-\\frac{N}{2}} exp\\{-\\frac{1}{2} \\textbf{tr}[\\Sigma^{-1} (Y-XA)' (\\left[ \\lambda_{1}, \\lambda_{2},...,\\lambda_{t} \\right])^{-1} (Y-XA) ]\\}\\\\\n&=\\det(\\Sigma)^{-\\frac{T}{2}}(\\prod^{T}_{t = 1} \\lambda_t)^{-\\frac{N}{2}} exp\\left\\{{-\\frac{1}{2}}\\sum^{T}_{t =1}{\\frac{1}{\\lambda_t}} \\textbf{tr}[(Y_t-X_tA)' \\Sigma^{-1}(Y_t-X_tA)]\\right\\}\\\\\n&=\\det(\\Sigma)^{-\\frac{T}{2}}\\prod^{T}_{t = 1} \\lambda_t^{-\\frac{N}{2}} exp\\left\\{{-\\frac{1}{2}}\\sum^{T}_{t =1}{\\frac{1}{\\lambda_t}} \\textbf{tr}[\\epsilon_t' \\Sigma^{-1}\\epsilon_t)]\\right\\}\\\\\n&=\\det(\\Sigma)^{-\\frac{T}{2}}\\prod^{T}_{t = 1}(\\lambda_t^{-\\frac{N}{2}} exp\\left\\{{-\\frac{1}{2}}{\\frac{1}{\\lambda_t}} \\textbf{tr}[\\epsilon_t' \\Sigma^{-1}\\epsilon_t])\\right\\})\n\n\\end{align}\\]\nTherefore at each time t, we have the likelihood function be a proportion of lambda:\n\\[\\begin{align}\n\n\\propto\\det(\\Sigma)^{-\\frac{T}{2}}\\lambda_t^{-\\frac{N}{2}} exp\\left\\{{-\\frac{1}{2}}{\\frac{1}{\\lambda_t}} \\textbf{tr}[\\epsilon_t' \\Sigma^{-1}\\epsilon_t])\\right\\}\n\n\\end{align}\\]\nFor joint posteriors distribution, \\(A\\), \\(\\Sigma\\) can then be derived using the likelihood and the prior distributions as follows:\n\\[\\begin{align}\n\nP(A,\\Sigma|Y,X) &\\propto L(A,\\Sigma,\\Omega|Y,X) \\ P(A,\\Sigma) \\\\\n\\\\\n&= \\det(\\Sigma)^{-\\frac{T}{2}} \\det(\\Omega)^{-\\frac{N}{2}} exp\\{-\\frac{1}{2} \\textbf{tr}[\\Sigma^{-1} (Y-XA)' \\Omega^{-1} (Y-XA) ]\\} \\\\\n&\\times \\det(\\Sigma)^{-\\frac{N+k+\\underline{\\nu}+1}{2}} exp\\{-\\frac{1}{2}\\textbf{tr}[\\Sigma^{-1}(A-\\underline{A})'(\\underline{V})^{-1}(A-\\underline{A})]\\} \\\\\n&\\times exp\\{-\\frac{1}{2}\\textbf{tr}[\\Sigma^{-1}\\underline{S}]\\} \\\\\n\n&= \\det(\\Sigma)^{-\\frac{T+N+K+\\underline{\\nu}+1}{2}} \\det(\\Omega)^{-\\frac{N}{2}} \\\\\n&\\times exp\\{-\\frac{1}{2} \\textbf{tr}[\\Sigma^{-1}(Y'\\Omega^{-1}Y - 2A'X'\\Omega^{-1}Y + A'X'\\Omega^{-1}XA \\\\\n&+ A'\\underline{V}^{-1}A -2A'\\underline{V}^{-1}\\underline{A} + \\underline{A}'\\underline{V}^{-1}\\underline{A} + \\underline{S})]\\}\n\n\\end{align}\\]\nThe kernel also could be rearranged in the form of the Normal-inverse Wishart distribution and given by:\n\\[\\begin{align}\n\nP(A,\\Sigma|Y,X,\\Omega) &\\sim \\mathcal{NIW}(\\bar{A},\\bar{V},\\bar{S},\\bar{\\nu}) \\\\\n&\\\\\n\\bar{V} &= (X'\\Omega^{-1}X + \\underline{V}^{-1})^{-1} \\\\\n\\bar{A} &= \\bar{V}(X'\\Omega^{-1}Y + \\underline{V}^{-1}\\underline{A}) \\\\\n\\bar{\\nu} &= T + \\underline{\\nu}\\\\\n\\bar{S} &= \\underline{S} + Y'\\Omega^{-1}Y + \\underline{A}'\\underline{V}^{-1}\\underline{A} - \\bar{A}'\\bar{V}^{-1}\\bar{A}\n\n\\end{align}\\]\nThe kernel of the fully conditional posterior distribution of \\(\\lambda_t\\) is then derived as follows:\n\\[\\begin{align}\n\nP(\\lambda_t|Y,X,A,\\Sigma) &\\propto L(A,\\Sigma,\\lambda_t|Y,X)P(\\lambda_t) \\\\\n\\\\\n&\\propto \\lambda_t^{-\\frac{N}{2}}exp({-\\frac{1}{2}}{\\frac{1}{\\lambda_t}}\\textbf{tr}[\\epsilon_t' \\Sigma^{-1}\\epsilon_t]) \\\\\n&\\times \\frac{1}{\\alpha}exp\\{ -\\frac{1}{\\alpha}\\lambda_t \\}\\\\\n&\\propto \\lambda_t^{-\\frac{N}{2}+1-1} exp\\{-\\frac{1}{2}[\\frac{\\textbf{tr}[\n\\epsilon_t' \\Sigma^{-1}\\epsilon_t]} {\\lambda_t} +\\frac{2}{\\alpha}\\lambda_t]\\}\n\n\\end{align}\\]\nThe above expression can be rearranged in the form of a Generalized inverse Gaussian distribution kernel as follows:\n\\[\\begin{align}\n\n\\lambda_t|Y,A,\\Sigma &\\sim \\mathcal{GIG}(a,b,p) \\\\\n\\\\\na &=\\frac{2}{\\alpha} \\\\\nb &= \\textbf{tr}[\\epsilon_t' \\Sigma^{-1}\\epsilon_t] \\\\\np &= -\\frac{N}{2}+1\n\n\\end{align}\\]\n\n\n4.5.2 Gibbs Sampler: Function proving\nThe Gibbs sampler method will be applied to generate random draws from the full conditional posterior distribution:\n\nDraw \\(\\Sigma^{(s)}\\) from the \\(IW(\\bar{S},\\bar{\\nu})\\) distribution.\nDraw \\(A^{(s)}\\) from the \\(MN(\\bar{A},\\Sigma^{(s)}, \\bar{V})\\) distribution.\nDraw \\(\\lambda_t^{(s)}\\) from \\(GIG(a,b,p)\\).\n\nRepeat steps 1, step 2 and 3 for \\(S_1\\)+\\(S_2\\)times.\nDiscard the first draws that allowed the algorithm to converge to the stationary posterior distribution.\nOutput is \\(\\left\\{ {A^{(s)}, \\Sigma^{(s)}}, \\lambda_t^{(s)}\\right\\}^{S_1+S_2}_{s=S_1+1}\\).\nFunction below is posterior.draws.extended:\n\nposterior.draws.extended &lt;- function(total_S,Y, X){\n  S=total_S\n  T &lt;- nrow(Y)\n  N = ncol(Y) \n  K = 1 + (p*N)\n  alpha &lt;- 2\n  lambda.0 &lt;- rexp(T, rate = 1/alpha)\n  lambda.priors = list(alpha = 2)\n  \n  # Initialize arrays to store posterior draws\n  Sigma.posterior.draws = array(NA, c(N,N,S))\n  A.posterior.draws = array(NA, c((1+p*N),N,S))\n  \n  lambda.posterior.draws = array(NA,c(T,S+1))\n  b = array(NA,c(T,S))\n  \n  #lambda.posterior.draws &lt;- array(NA,c(T,S+1))\n   for (s in 1:S){\n    \n    if (s == 1) {\n      lambda.s = lambda.0\n    } else {\n      lambda.s    = lambda.posterior.draws[,s]\n    }\n    \n    Omega.inv = diag(1/lambda.s)\n    \n    V.bar.inv.ext   = t(X)%*%Omega.inv%*%X + solve(V.prior)\n    V.bar.ext       = solve(V.bar.inv.ext)\n    A.bar.ext       = V.bar.ext%*%(t(X)%*%Omega.inv%*%Y + diag(1/diag(V.prior))%*%A.prior)\n    nu.bar.ext      = T + nu.prior\n    S.bar.ext       = S.prior + t(Y)%*%Omega.inv%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar.ext)%*%V.bar.inv.ext%*%A.bar.ext\n    S.bar.ext.inv   = solve(S.bar.ext)\n    \n    Sigma.inv.draw = rWishart(1, df = nu.bar.ext, Sigma = S.bar.ext.inv)[,,1]\n    Sigma.posterior.draws[,,s] = solve(Sigma.inv.draw)\n    A.posterior.draws[,,s] = matrix(mvtnorm::rmvnorm(1, mean=as.vector(A.bar.ext), sigma = Sigma.posterior.draws[,,s] %x% V.bar.ext), ncol=N)\n    \n    u.t = Y-X%*%A.posterior.draws[,,s]\n    #    ---- loop lambda posterior ----   #\n    c                      = -N/2 + 1          # N=12\n    a                      = 2 / lambda.priors$alpha\n    for (x in 1:T){\n      b                  = sum(diag(t((u.t)[x,])%*%Sigma.inv.draw%*%(u.t)[x,]))\n      lambda.posterior.draws[x,s+1] = GIGrvg::rgig(1, lambda = c, chi = b, psi = a)\n    } # END x loop\n  } # END s loop\n  #}\n  \n  output                 = (list(A.posterior.exten = A.posterior.draws[,,S1:total_S], \n                                Sigma.posterior.exten = Sigma.posterior.draws[,,S1:total_S], \n                                lambda.posterior.exten = lambda.posterior.draws[,(S1+1):(S+1)]))\n  \n}\n\n# conduct simulation\nposterior.extended = posterior.draws.extended(total_S = total_S, Y=Y, X=X)\n\n\n\n\nPosterior mean of the autoregressive coefficient matrix A\n\n\n\nSimulation_Y1\nSimulation_Y2\n\n\n\n\nConstant\n0.0536249\n0.1065899\n\n\nY1-Lag\n0.9909827\n0.0086799\n\n\nY2-Lag\n0.0025720\n0.9939169\n\n\n\n\n\n\n\n\nPosterior mean of the covariance matrix Sigma\n\n\n\nSimulation_Y1\nSimulation_Y2\n\n\n\n\nY1-Lag\n0.6165036\n0.0538747\n\n\nY2-Lag\n0.0538747\n0.6587449\n\n\n\n\n\n\n\n\nPosterior Mean of Lambda\n\n\n\nMean\n\n\n\n\nLambda\n2.007751"
  },
  {
    "objectID": "index.html#stochastic-volatility-heteroskedasticity",
    "href": "index.html#stochastic-volatility-heteroskedasticity",
    "title": "An Evidence-based Forecast: Gold as a Traditional Safe-Haven Investment",
    "section": "4.6 Stochastic Volatility Heteroskedasticity",
    "text": "4.6 Stochastic Volatility Heteroskedasticity\nHowever, in the real world, the shocks are often passing to next few periods. Therefore, for the same general serial covariance structure:\n\\[\\begin{align}\n\nvec(U) \\ \\sim N(0 \\ , \\ \\Sigma \\ \\otimes \\  \\Omega)\\\\\n\n\\end{align}\\]\nThis time we have \\(h_t\\) follows a simple stochastic volatility process:\n\\[\\begin{align}\n\n\\Omega \\ &= diag\\left[ \\sigma^2_1,..., \\sigma^2_T\\right]\\\\\n&=\\ diag\\left[ exp\\left\\{ h_1 \\right\\},exp\\left\\{ h_2 \\right\\},...,exp\\left\\{ h_T \\right\\}  \\right]\\\\\nh_t &= h_{t-1}+v_t\\\\\n\n\\end{align}\\]\nThen, the kernel of the likelihood function could be rewritten as to:\n\\[\\begin{align}\n\nL(A,\\Sigma,\\Omega |Y,X) &\\propto \\det(\\Sigma)^{-\\frac{T}{2}} \\det(\\Omega)^{-\\frac{N}{2}} exp\\{-\\frac{1}{2}\\textbf{tr}[\\Sigma^{-1} (Y-XA)' \\Omega^{-1} (Y-XA) ]\\}\\\\\n&=\\det(\\Sigma)^{-\\frac{T}{2}}\\left( \\prod^{T}_{t = 1} exp\\left\\{ h_t \\right\\} \\right)^{-\\frac{N}{2}} exp\\left\\{{-\\frac{1}{2}}\\sum^{T}_{t =1}exp\\left\\{ h_t \\right\\}^{-1} \\textbf{tr}[\\epsilon_t' \\Sigma^{-1}\\epsilon_t)]\\right\\}\\\\\n&=\\det(\\Sigma)^{-\\frac{T}{2}}exp\\left\\{{-\\frac{N}{2}}\\sum^{T}_{t =1}h_t{-\\frac{1}{2}}\\sum^{T}_{t =1}exp\\left\\{ h_t \\right\\}^{-1} \\textbf{tr}[\\epsilon_t' \\Sigma^{-1}\\epsilon_t)]\\right\\}\\\\\n\n\\end{align}\\]\n\n4.6.1 Matrix Notation for Stochastic Volatility model\nRecall the matrix notation:\n\\[\\begin{align}\n\nY&=XA \\  +  \\ E\\\\\nE|X &\\sim MN(0,\\Sigma,\\Omega) \\\\\n\\Omega \\ &=\\ diag\\left[ exp\\left\\{ h_1 \\right\\},exp\\left\\{ h_2 \\right\\},...,exp\\left\\{h_T \\right\\}  \\right]\\\\\nh_t &= h_{t-1}+\\sigma_{v}v_t\\\\\n\n\\end{align}\\]\nGiven that:\n\\[\\begin{align}\n\n\\epsilon_t &\\sim N(0,1)\\\\\nv_t&\\sim N(0,1)\\\\\n\n\\end{align}\\]\nWe could rewrite the equation as:\n\\[\\begin{align}\n\ny_t &= y_{t-1}A_1 \\ \\ + ...+y_{t-p}A_p+\\ \\ exp\\left\\{ \\frac{1}{2} h_t\\right\\}\\epsilon_t\\\\\ny_t - y_{t-1}A_1 \\ \\ + ...+y_{t-p}A_p &= exp\\left\\{ \\frac{1}{2} h_t\\right\\}\\epsilon_t\\\\\ny_{u.t}&=exp\\left\\{ \\frac{1}{2} h_t\\right\\}\\epsilon_t\\\\\n\n\\end{align}\\]\nThen, taking the square and the logarithm of both sides of the equation, we could have:\n\\[\\begin{align}\n\ny_{u.t}&=exp\\left\\{ \\frac{1}{2} h_t\\right\\}\\epsilon_t\\\\\nlog \\ y_{u.t}^2&=h_t + log \\ \\epsilon_t^2\\\\\n\\tilde{y}_t&=h_t+\\tilde{\\epsilon}_t\n\n\\end{align}\\]\nAnd then:\n\\[\\begin{align}\n\n\\tilde{\\epsilon}_t&\\sim \\log \\chi^2_1\n\n\\end{align}\\]\nDefine the following \\(T Ã— 1\\) matrices:\n\\[\\begin{aligned}\n\n\\tilde{y}=\n\\begin{bmatrix}\n\\tilde{y}_1'\\\\ \\tilde{y}_2'\n\\\\.\\\\.\\\\.\\\\\\tilde{y}'_T\n\\end{bmatrix}_{T \\times 1}\n\nh=\n\\begin{bmatrix}\nh_{1} \\\\ h_{2} \\\\\n.\\\\.\\\\.\\\\h_{T}\n\\end{bmatrix}_{T \\times 1}\n\n\\tilde{\\epsilon}=\n\\begin{bmatrix}\n\\tilde{\\epsilon}_1'\\\\ \\tilde{\\epsilon}_2' \\\\\n.\\\\.\\\\.\\\\\\tilde{\\epsilon}_T'\n\\end{bmatrix}_{T \\times 1}\n\nv=\n\\begin{bmatrix}\nv_{1} \\\\ v_{2} \\\\\n.\\\\.\\\\.\\\\v_{T}\n\\end{bmatrix}_{T \\times 1}\n\ne_{1.T}=\n\\begin{bmatrix}\n1 \\\\ 0 \\\\\n.\\\\.\\\\.\\\\0\n\\end{bmatrix}_{T \\times 1}\n\n\\end{aligned}\\]\nAnd a \\(T Ã— T\\) matrix, H:\n\\[\\begin{align}\n\nH = \\begin{pmatrix}\n1 &  &  &  &  & \\\\\n-1 & 1 &  &  &  & \\\\\n0 & -1 & 1 &  &  & \\\\\n\\vdots & \\ddots & \\ddots & \\ddots &  & \\\\\n0 & \\cdots & 0 & -1 & 1 \\\\\n\\end{pmatrix}_{T \\times T}\n\n\\end{align}\\]\nTherefore, we could have the simple matrix notation for Stochastic Volatility model:\n\\[\\begin{align}\n\n\\tilde{y}=h+\\tilde{\\varepsilon}\\\\\nHh=h_0e_{1.T}+\\sigma_vv\\\\\n\\tilde{\\epsilon}\\sim \\log \\chi^2_1\\\\\nv\\sim\\mathcal{N}_T(0_T,I_T)\\\\\n\n\\end{align}\\]\nAnd approximate the \\(\\log \\chi^2_1\\) distribution by a mixture of ten normal distributions given by:\n\\[\\begin{align}\n\n\\log \\chi^2_1\\approx \\sum_{m=1}^{10}Pr(s_t = m)\\mathcal{N}(\\mu_m,\\sigma^2_m)\n\n\\end{align}\\]\nwhere:\n\ns_t âˆˆ {1, . . . , 10} is a discrete-valued random indicator of the mixture component\n\\(\\mu_m,\\sigma^2_m\\) ,Pr(st = m) are predetermined\n\nTherefore, we could rewrite \\(\\tilde{\\epsilon}\\) in a Normal distribution:\n\\[\\begin{align}\n\n\\tilde{\\epsilon}|s \\sim\\mathcal{N}(\\mu_s,diag(\\sigma^2_s))\n\n\\end{align}\\]\n\n\n4.6.2 Priors distribution\nHierarchical prior structure is given by:\n\\[\\begin{align}\n\nP(h,s,h_0,\\sigma^2_v)=P(h|h_0,\\sigma^2_v)P(h_0)P(\\sigma^2_v)P(s)\n\n\\end{align}\\]\nTherefore, conditional prior distribution for \\(h\\) is:\n\\[\\begin{align}\n\nP(h|h_0,\\sigma^2_v)\\sim\\mathcal{N}(h_0H^{-1}e_{1.T},\\sigma^2_v(H'H)^{-1})\n\n\\end{align}\\]\nwith kernel given by:\n\\[\\begin{align}\n\n\\propto det(\\sigma^2_v\\textbf{I}_T)^{-\\frac{1}{2}}exp\\left( -\\frac{1}{2}\\sigma^{-2}_v\\left( Hh-h_0e_{1.T} \\right)'\\left( Hh-h_0e_{1.T} \\right) \\right)\n\n\\end{align}\\]\nPrior distribution for \\(h_0\\) is:\n\\[\\begin{align}\n\nP(h_0)\\sim\\mathcal{N}(0,\\underline{\\sigma}^2_h)\n\n\\end{align}\\]\nwith kernel given by:\n\\[\\begin{align}\n\n\\propto \\left( \\underline{\\sigma}^2_h \\right)^{-\\frac{1}{2}}exp\\left( -\\frac{1}{2}\\underline{\\sigma}^{-2}_hh_0h_0 \\right)\n\n\\end{align}\\]\nPrior distribution for \\(\\sigma^2_v\\) is:\n\\[\\begin{align}\n\nP(\\sigma^2_v)\\sim\\mathcal{IG2}(\\underline{s},\\underline{\\nu})\n\n\\end{align}\\]\nwith kernel given by:\n\\[\\begin{align}\n\n\\propto \\left( \\underline{\\sigma}^2_v \\right)^{-\\frac{\\underline{\\nu}+2}{2}}exp\\left( -\\frac{1}{2}\\frac{\\underline{s}}{\\underline{\\sigma}^{2}_v} \\right)\n\n\\end{align}\\]\nPrior distribution for \\(s\\) is:\n\\[\\begin{align}\n\nP(s_t)\\sim\\mathcal{Multinomial}(\\mathrm{\\left\\{ m \\right\\}}_{m=1}^{10},\\mathrm{\\left\\{Pr(s_t=m) \\right\\}}_{m=1}^{10})\n\n\\end{align}\\]\n\n\n4.6.3 Bayesian Estimations\nFor joint posteriors distribution, \\(A\\), \\(\\Sigma\\) can then be derived using the likelihood and the prior distributions as follows:\n\\[\\begin{align}\n\nP(A,\\Sigma|Y,X,\\Omega) &\\propto L(A,\\Sigma,\\Omega|Y,X) \\ P(A,\\Sigma) \\\\\n\\\\\n&= \\det(\\Sigma)^{-\\frac{T}{2}} \\det(\\Omega)^{-\\frac{N}{2}} exp\\{-\\frac{1}{2} \\textbf{tr}[\\Sigma^{-1} (Y-XA)' \\Omega^{-1} (Y-XA) ]\\} \\\\\n&\\times \\det(\\Sigma)^{-\\frac{N+k+\\underline{\\nu}+1}{2}} exp\\{-\\frac{1}{2}\\textbf{tr}[\\Sigma^{-1}(A-\\underline{A})'(\\underline{V})^{-1}(A-\\underline{A})]\\} \\\\\n&\\times exp\\{-\\frac{1}{2}\\textbf{tr}[\\Sigma^{-1}\\underline{S}]\\} \\\\\n\n&= \\det(\\Sigma)^{-\\frac{T+N+K+\\underline{\\nu}+1}{2}} \\det(\\Omega)^{-\\frac{N}{2}} \\\\\n&\\times exp\\{-\\frac{1}{2} \\textbf{tr}[\\Sigma^{-1}(Y'\\Omega^{-1}Y - 2A'X'\\Omega^{-1}Y + A'X'\\Omega^{-1}XA \\\\\n&+ A'\\underline{V}^{-1}A -2A'\\underline{V}^{-1}\\underline{A} + \\underline{A}'\\underline{V}^{-1}\\underline{A} + \\underline{S})]\\}\n\n\\end{align}\\]\nThen, we have same kernel presentation as Laplace Distribution and could be rearranged in the form of the Normal-inverse Wishart distribution and given by:\n\\[\\begin{align}\n\nP(A,\\Sigma|Y,X,\\Omega) &\\sim \\mathcal{NIW}(\\bar{A},\\bar{V},\\bar{S},\\bar{\\nu}) \\\\\n&\\\\\n\\bar{V} &= (X'\\Omega^{-1}X + \\underline{V}^{-1})^{-1} \\\\\n\\bar{A} &= \\bar{V}(X'\\Omega^{-1}Y + \\underline{V}^{-1}\\underline{A}) \\\\\n\\bar{\\nu} &= T + \\underline{\\nu}\\\\\n\\bar{S} &= \\underline{S} + Y'\\Omega^{-1}Y + \\underline{A}'\\underline{V}^{-1}\\underline{A} - \\bar{A}'\\bar{V}^{-1}\\bar{A}\n\n\\end{align}\\]\nThe kernel of the fully conditional posterior distribution of \\(h\\) is then derived as follows:\n\\[\\begin{align}\n\nP(h|h_0,\\sigma^2_v,s,\\tilde{y}) &\\propto L(h,h_0,\\sigma^2_v,s|\\tilde{y})P(h) \\\\\n&\\propto exp\\left( -\\frac{1}{2}\\left( h-(\\tilde{y}-\\mu_s) \\right)'diag\\left(\\sigma^2_s  \\right)^{-1}\\left( h-(\\tilde{y}-\\mu_s) \\right) \\right)\\\\\n&\\times exp\\left( -\\frac{1}{2}\\sigma^{-2}_v\\left( Hh-h_0e_{1.T} \\right)'\\left( Hh-h_0e_{1.T} \\right) \\right)\\\\\n\n\\end{align}\\]\nThe above expression can be rearranged in the form of a Normal distribution kernel as follows:\n\\[\\begin{align}\n\nP(h|h_0,\\sigma^2_v,s,\\tilde{y}) &\\sim \\mathcal{N}(\\overline{h},\\overline{V}_h)\\\\\n\\overline{V}_h&=\\left[ diag\\left(\\sigma^2_s \\right)^{-1}+ \\sigma^{-2}_vH'H\\right]^{-1}\\\\\n\\overline{h}&=\\overline{V}_h\\left[ diag\\left(\\sigma^2_s \\right)^{-1}(\\tilde{y}-\\mu_s)+ \\sigma^{-2}_vh_0e_{1.T}\\right]\\\\\n\n\\end{align}\\]\nThe kernel of the fully conditional posterior distribution of \\(h_0\\) is then derived as follows:\n\\[\\begin{align}\n\nP(h_0|h,\\sigma^2_v,s,\\tilde{y}) &\\propto L(h_0,\\sigma^2_v|h)P(h_0) \\\\\n&\\propto exp\\left( -\\frac{1}{2}\\sigma^{-2}_v\\left( Hh-h_0e_{1.T} \\right)'\\left( Hh-h_0e_{1.T} \\right) \\right)\\\\\n&\\times exp\\left( -\\frac{1}{2}\\underline{\\sigma}^{-2}_hh_0h_0 \\right)\n\n\\end{align}\\]\nThe above expression can be rearranged in the form of a Normal distribution kernel as follows:\n\\[\\begin{align}\n\nP(h_0|h,\\sigma^2_v,s,\\tilde{y}) &\\sim \\mathcal{N}(\\overline{h}_0,\\overline{\\sigma}^2_h)\\\\\n\\overline{\\sigma}^2_h&=\\left(\\underline{\\sigma}^{-2}_h+ \\sigma^{-2}_v\\right)^{-1}\\\\\n\\overline{h}_0&=\\overline{\\sigma}^2_h\\left(\\sigma^{-2}_ve_{1.T}'Hh\\right)^{-1}\\\\\n\n\\end{align}\\]\nThe kernel of the fully conditional posterior distribution of \\(\\sigma^2_v\\) is then derived as follows:\n\\[\\begin{align}\n\nP(\\sigma^2_v|h,h_0,s,\\tilde{y}) &\\propto L(h_0,\\sigma^2_v|h)P(\\sigma^2_v) \\\\\n&\\propto exp\\left( -\\frac{1}{2}\\sigma^{-2}_v\\left( Hh-h_0e_{1.T} \\right)'\\left( Hh-h_0e_{1.T} \\right) \\right)\\\\\n&\\times \\left( \\underline{\\sigma}^2_v \\right)^{-\\frac{\\underline{\\nu}+2}{2}}exp\\left( -\\frac{1}{2}\\frac{\\underline{s}}{\\underline{\\sigma}^{2}_v} \\right)\n\n\\end{align}\\]\nThe above expression can be rearranged in the form of a Inverse-Gamma 2 distribution kernel as follows:\n\\[\\begin{align}\n\nP(\\sigma^2_v|h,h_0,s,\\tilde{y}) &\\sim \\mathcal{IG2}(\\overline{s},\\overline{\\nu})\\\\\n\\overline{\\nu}&=\\underline{\\nu}+ T\\\\\n\\overline{s}&=\\underline{s}+\\left( Hh-h_0e_{1.T} \\right)'\\left( Hh-h_0e_{1.T} \\right)\\\\\n\n\\end{align}\\]\nThe fully conditional posterior distribution of \\(s\\) is a multinomial distribution with the probabilities proportional to:\n\\[\\begin{align}\n\n\\omega_{m.t} = Pr[s_t=m]p(\\tilde{y}|h_t,s_t=m),\\text{for} \\ m=1,...,10\n\n\\end{align}\\]\nFor each \\(t\\) and \\(m\\) obtain \\(\\omega_{m.t}\\) using parallel computations and compute the probabilities of the multinomial full conditional posterior distribution by:\n\\[\\begin{align}\n\nPr[s_t=m|\\tilde{y},h_t]=\\frac{\\omega_{m.t}}{\\sum_{i=1}^{10}\\omega_{m.t}}\n\n\\end{align}\\]\n\n\n4.6.4 Gibbs Sampler\nConsidering draws from this posterior involves a Gibbs sampler, which follows the following algorithm:\n\nInitialize \\(h^{(0)}\\), \\(s^{(0)}\\), and \\(\\sigma^{2(0)}_v\\)\nDraw \\(h_0^{(s)}\\sim\\mathcal{N}(\\bar{h}_0,\\bar{\\sigma}^2_h)\\)\nDraw \\(\\sigma_v^{2(s)}\\sim\\mathcal{IG2}(\\bar{s},\\bar{\\nu})\\)\nDraw \\(s_t^{(s)}\\sim\\mathcal{Multiomial}(\\{m\\}^{10}_{m=1},\\{Pr[s_t=m|\\tilde{y},h_t^{(s)}]\\}^{10}_{m=1})\\) for all \\(s_t\\) in \\(s\\).\nDraw \\(h^{(s)}\\sim\\mathcal{N}_T(\\bar{h},\\bar{V}_h)\\)\nDraw \\((A,\\Sigma)\\sim\\mathcal{MNIW}(\\bar{A},\\bar{V},\\bar{S},\\bar{\\nu})\\)\n\nSteps 2 to 7 are repeated for \\(S=S_1+S_2\\) draws, where \\(S_1\\) draws are discarded as burn-in and the latter \\(S_2\\) draws are kept as posterior draws.\nFirst Hierarchical draws based on following function SVcommon.Gibbs.iteration:\n\nSVcommon.Gibbs.iteration &lt;- function(aux, priors){\n  # A single iteration of the Gibbs sampler for the SV component\n\n  # aux is a list containing:\n  #   Y         - a TxN matrix\n  #   X         - a TxK matrix\n  #   H         - a Tx1 matrix                 \n  #   h0        - a scalar\n  #   sigma.v2  - a scalar\n  #   s         - a Tx1 matrix\n  #   A         - a KxN matrix\n  #   Sigma     - an NxN matrix\n  #   sigma2    - a Tx1 matrix\n  \n  # priors is a list containing:\n  #   h0.v      - a positive scalar\n  #   h0.m      - a scalar\n  #   sigmav.s  - a positive scalar\n  #   sigmav.nu - a positive scalar\n  #   HH        - a TxT matrix\n\n  T             = dim(aux$Y)[1]\n  N             = dim(aux$Y)[2]\n  alpha.st      = c(1.92677,1.34744,0.73504,0.02266,0-0.85173,-1.97278,-3.46788,-5.55246,-8.68384,-14.65000)\n  sigma.st      = c(0.11265,0.17788,0.26768,0.40611,0.62699,0.98583,1.57469,2.54498,4.16591,7.33342)\n  pi.st         = c(0.00609,0.04775,0.13057,0.20674,0.22715,0.18842,0.12047,0.05591,0.01575,0.00115)\n\n  Lambda        = solve(chol(aux$Sigma))\n  Z             = rowSums( ( aux$Y - aux$X %*% aux$A ) %*% Lambda ) / sqrt(N)\n  Y.tilde       = as.vector(log((Z + 0.0000001)^2))\n  Ytilde.alpha  = as.matrix(Y.tilde - alpha.st[as.vector(aux$s)])\n\n  # sampling initial condition\n  V.h0.bar      = 1/((1 / priors$h0.v) + (1 / aux$sigma.v2))\n  m.h0.bar      = V.h0.bar*((priors$h0.m / priors$h0.v) + (aux$H[1] / aux$sigma.v2))\n  h0.draw       = rnorm(1, mean = m.h0.bar, sd = sqrt(V.h0.bar))\n  aux$h0        = h0.draw    \n\n  # sampling sigma.v2\n  sigma.v2.s    = priors$sigmav.s + sum(c(aux$H[1] - aux$h0, diff(aux$H))^2)\n  sigma.v2.draw = sigma.v2.s / rchisq(1, priors$sigmav.nu + T)\n  aux$sigma.v2  = sigma.v2.draw    \n\n  # sampling auxiliary states\n  Pr.tmp        = simplify2array(lapply(1:10,function(x){\n    dnorm(Y.tilde, mean = as.vector(aux$H + alpha.st[x]), sd = sqrt(sigma.st[x]), log = TRUE) + log(pi.st[x])\n  }))\n  Pr            = t(apply(Pr.tmp, 1, function(x){exp(x - max(x)) / sum(exp(x - max(x)))}))\n  s.cum         = t(apply(Pr, 1, cumsum))\n  r             = matrix(rep(runif(T), 10), ncol = 10)\n  ss            = apply(s.cum &lt; r, 1, sum) + 1\n  aux$s         = as.matrix(ss)       \n\n\n  # sampling log-volatilities using functions for tridiagonal precision matrix\n  Sigma.s.inv   = diag(1 / sigma.st[as.vector(aux$s)])\n  D.inv         = Sigma.s.inv + (1 / aux$sigma.v2) * priors$HH\n  b             = as.matrix(Ytilde.alpha / sigma.st[as.vector(aux$s)] + (aux$h0/aux$sigma.v2)*diag(T)[,1])\n  lead.diag     = diag(D.inv)\n  sub.diag      = mgcv::sdiag(D.inv, -1)\n  D.chol        = mgcv::trichol(ld = lead.diag, sd = sub.diag)\n  D.L           = diag(D.chol$ld)\n  mgcv::sdiag(D.L,-1) = D.chol$sd\n  x             = as.matrix(rnorm(T))\n  a             = forwardsolve(D.L, b)\n  draw          = backsolve(t(D.L), a + x)\n  aux$H         = as.matrix(draw)         \n  aux$sigma2    = as.matrix(exp(draw))     \n\n  return(aux)\n}\n\nTherefore, we could have the function DrawPosterior.sv to draw \\(A\\) and \\(\\Sigma\\):\n\nDrawPosterior.sv &lt;- function (Y,X,S1,S2){\n  \n  N &lt;- ncol(Y)\n  K &lt;- ncol(X)\n  T &lt;- nrow(Y)\n  \n  A.posterior     &lt;- array(NA,c(K,N,(S1+S2)))\n  Sigma.posterior &lt;- array(NA,c(N,N,(S1+S2))) \n  H.posterior     &lt;- array(NA,c(nrow(Y),(S1+S2+1)))  \n  B.posterior     &lt;- array(NA,c(N,N,(S1+S2)))\n  B1.tilde.s      &lt;- array(NA,c(N,K,(S1+S2)))\n  \n  # Initialize h^{0}, which is  T by 1 matrix \n  H.posterior[,1]       &lt;- matrix(1,T,1) \n  \n  nu.bar                &lt;- nrow(Y) + nu.prior\n  \n  HH                    &lt;-  2*diag(T)\n  mgcv::sdiag(HH,-1)    &lt;-  -1\n  mgcv::sdiag(HH,1)     &lt;-  -1\n  \n  priors = list(HH       = HH,\n                h0.m     = 0,\n                h0.v     = 1,\n                sigmav.s = 1,\n                sigmav.nu= 1\n              )\n  \n  for (s in 1:(S1+S2)){\n    # STEP 1: draw (A Sigma).s from MNIW(A.bar,V.bar,S.bar,nu.bar)\n    # setting up parameters \n    \n    V.bar.inv       &lt;- t(X)%*%diag(1/H.posterior[,s])%*%X + diag(1/diag(V.prior))\n    V.bar           &lt;- solve(V.bar.inv)\n    A.bar           &lt;- V.bar%*%(t(X)%*%diag(1/H.posterior[,s])%*%Y + diag(1/diag(V.prior))%*%A.prior)\n    S.bar           &lt;- S.prior+t(Y)%*%diag(1/H.posterior[,s])%*%Y+t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar\n    \n    draw.sigma.inv  &lt;- solve(rWishart(1, df=nu.bar, Sigma=solve(S.bar))[,,1])\n    Sigma.posterior[,,s] = draw.sigma.inv\n      \n      # start drawing \n    cholSigma.s      &lt;- chol(Sigma.posterior[,,s])\n    A.posterior[,,s] &lt;- matrix(MASS::mvrnorm(1,as.vector(A.bar),Sigma.posterior[,,s]%x%V.bar),ncol=N)      \n    \n    # STEP 2: draw H from SVcommon.Gibbs.iteration\n    if (s == 1){ # initializing input arguments \n        aux = list( \n              Y             = Y,\n              X             = X,\n              H             = matrix(1,T,1),\n              h0            = 0,\n              sigma.v2      = 1,\n              s             = matrix(1,T,1),\n              Sigma         = Sigma.posterior[,,s],\n              A             = A.posterior[,,s],\n              sigma2        = matrix(1,T,1)\n              )\n    }else{ # updating input arguments  \n      aux = list(\n                Y           = Y,\n                X           = X,\n                H           = tmp$H,\n                h0          = tmp$h0,\n                sigma.v2    = tmp$sigma.v2,\n                s           = tmp$s,\n                Sigma       = Sigma.posterior[,,s],\n                A           = A.posterior[,,s],\n                sigma2      = tmp$sigma2\n              )\n    }\n\n    # H                     &lt;- diag(T)\n    # sdiag(H,-1)           &lt;- -1     \n    tmp                     &lt;- SVcommon.Gibbs.iteration(aux,priors)\n    H.posterior[,s+1]      &lt;- as.matrix(tmp$sigma2)\n  }\n  \n  return(list(Sigma.posterior = Sigma.posterior[,,(S1+1):(S1+S2)],\n              A.posterior     = A.posterior[,,(S1+1):(S1+S2)],\n              H.sv            = H.posterior[,(S1+2):(S1+S2+1)]))\n  }\n\nThe below posterior mean provides a time series plot of posterior draws mean of \\(\\sigma^2\\) at each time spot. From where we can observe the \\(\\sigma^2\\) oscillating around 2100.\n\n\n\n\n\nFollowing plots the draws of Gibbs Sampler of the Stochastic Volatility model, and it shown to be stationary for A and \\(\\Sigma\\).From where, we can observe the mean of \\(A_{21},A_{32}\\) varying around value of 1, \\(\\Sigma_{21},\\Sigma_{22}\\) oscillating around 0.0018-0.0020.\n\n\n\n\n\nFigureÂ 1: Gibbs Sampler draws of A and Sigma-SV model"
  },
  {
    "objectID": "index.html#combinations-of-extension",
    "href": "index.html#combinations-of-extension",
    "title": "An Evidence-based Forecast: Gold as a Traditional Safe-Haven Investment",
    "section": "4.7 Combinations of Extension",
    "text": "4.7 Combinations of Extension\nDue to complications in the real world, some of common shocks to the market are deep influenced and some are disappeared in a decided time period. For example, the GFC, Covid-19 and Russian-Ukraine war influence for a long time, even right now, we are underlying a quantitative easing scenario and suffering a common inflation risk. That reflects that the effects to serial covariance some are independently and some part are autoregressive on the other hand. Therefore, we could combine the Laplace model and Stochastic Volatility model to present, estimate and forecast under this complicated scenario.\nTherefore, we could have the model present as:\n\\[\\begin{align}\n\nY &= X A +U\n\\\\\n\\\\\nU|X &\\sim \\mathcal{MN}_{T\\times N}(\\textbf{0},\\Sigma,\\Omega)\\\\\nvec(U) \\ &\\sim N(0 \\ , \\ \\Sigma \\ \\otimes \\  \\Omega)\\\\\n\\Omega \\ &= diag\\left[\\lambda_{1} \\sigma^2_1, \\lambda_{2}\\sigma^2_2..., \\lambda_{t}\\sigma^2_T\\right]\\\\\n&=\\ diag\\left[ \\lambda_{1}exp\\left\\{ h_1 \\right\\},\\lambda_{2}exp\\left\\{ h_2 \\right\\},...,\\lambda_{T}exp\\left\\{ h_T \\right\\}  \\right]\\\\\n\\lambda \\ &\\sim \\ Exp \\ (\\alpha)\\\\\nh_t &= h_{t-1}+\\sigma_{v}v_t\\\\\nv_t&\\sim N(0,1)\\\\\n\n\\end{align}\\]\nStill have that:\n\\[\\begin{align}\n\n\\epsilon_t &\\sim N(0,1)\\\\\n\n\\end{align}\\]\nNow we could rewrite the equation into:\n\\[\\begin{align}\n\ny_t &= y_{t-1}A_1 \\ \\ + ...+y_{t-p}A_p+\\ \\ \\sqrt{\\lambda_t}exp\\left\\{ \\frac{1}{2} h_t\\right\\}\\epsilon_t\\\\\n\\frac{y_t - y_{t-1}A_1 \\ \\ + ...+y_{t-p}A_p}{\\sqrt{\\lambda_t}} &= exp\\left\\{ \\frac{1}{2} h_t\\right\\}\\epsilon_t\\\\\ny_{\\lambda.t}&=exp\\left\\{ \\frac{1}{2} h_t\\right\\}\\epsilon_t\\\\\n\\\\\ny_{\\lambda.t}&=exp\\left\\{ \\frac{1}{2} h_t\\right\\}\\epsilon_t\\\\\nlog \\ y_{\\lambda.t}^2&=h_t + log \\ \\epsilon_t^2\\\\\n\\tilde{y}_{\\lambda.t}&=h_t+\\tilde{\\epsilon}_t\n\n\\end{align}\\]\nSame to the standard Stochastic Volatility model, and the Hierarchical prior structure are same:\n\\[\\begin{align}\n\n\\tilde{y}_{\\lambda}=h+\\tilde{\\varepsilon}\\\\\nHh=h_0e_{1.T}+\\sigma_vv\\\\\n\\tilde{\\epsilon}\\sim \\log \\chi^2_1\\\\\nv\\sim\\mathcal{N}_T(0_T,I_T)\\\\\n\nP(h,s,h_0,\\sigma^2_v)=P(h|h_0,\\sigma^2_v)P(h_0)P(\\sigma^2_v)P(s)\n\n\\end{align}\\]\n\n4.7.1 Likelihood Function\nThe general likelihood function remain unchanged due to continues of serial covariance symbol \\(\\Omega\\) is used:\n\\[\\begin{align}\n\nL(A,\\Sigma,\\Omega |Y,X) &\\propto \\det(\\Sigma)^{-\\frac{T}{2}} \\det(\\Omega)^{-\\frac{N}{2}} exp\\{-\\frac{1}{2}\\textbf{tr}[\\Sigma^{-1} (Y-XA)' \\Omega^{-1} (Y-XA) ]\\}\\\\\n\n\\end{align}\\]\nAfter simplify we could have:\n\\[\\begin{align}\n\nL(A,\\Sigma,\\Omega |Y,X) &\\propto \\det(\\Sigma)^{-\\frac{T}{2}} \\det(\\Omega)^{-\\frac{N}{2}} exp\\{-\\frac{1}{2}\\textbf{tr}[\\Sigma^{-1} (Y-XA)' \\Omega^{-1} (Y-XA) ]\\}\\\\\n&=\\det(\\Sigma)^{-\\frac{T}{2}}\\left( \\prod^{T}_{t = 1} \\lambda_t\\prod^{T}_{t = 1} exp\\left\\{ h_t \\right\\} \\right)^{-\\frac{N}{2}} exp\\left\\{{-\\frac{1}{2}}\\sum^{T}_{t =1}\\lambda_texp\\left\\{ h_t \\right\\}^{-1} \\textbf{tr}[\\epsilon_t' \\Sigma^{-1}\\epsilon_t)]\\right\\}\\\\\n&=\\det(\\Sigma)^{-\\frac{T}{2}}\\prod^{T}_{t = 1} \\lambda_t^{-\\frac{N}{2}} exp\\left\\{{-\\frac{N}{2}}\\sum^{T}_{t =1}h_t{-\\frac{1}{2}}\\sum^{T}_{t =1}\\lambda_texp\\left\\{ h_t \\right\\}^{-1} \\textbf{tr}[\\epsilon_t' \\Sigma^{-1}\\epsilon_t)]\\right\\}\\\\\n\n\\end{align}\\]\n\n\n4.7.2 Bayesian Estimations\nAs the unchanged likelihood function, we still have:\n\\[\\begin{align}\n\nP(A,\\Sigma|Y,X,\\Omega) &\\propto L(A,\\Sigma,\\Omega|Y,X) \\ P(A,\\Sigma) \\\\\n\\\\\n&= \\det(\\Sigma)^{-\\frac{T}{2}} \\det(\\Omega)^{-\\frac{N}{2}} exp\\{-\\frac{1}{2} \\textbf{tr}[\\Sigma^{-1} (Y-XA)' \\Omega^{-1} (Y-XA) ]\\} \\\\\n&\\times \\det(\\Sigma)^{-\\frac{N+k+\\underline{\\nu}+1}{2}} exp\\{-\\frac{1}{2}\\textbf{tr}[\\Sigma^{-1}(A-\\underline{A})'(\\underline{V})^{-1}(A-\\underline{A})]\\} \\\\\n&\\times exp\\{-\\frac{1}{2}\\textbf{tr}[\\Sigma^{-1}\\underline{S}]\\} \\\\\n\n&= \\det(\\Sigma)^{-\\frac{T+N+K+\\underline{\\nu}+1}{2}} \\det(\\Omega)^{-\\frac{N}{2}} \\\\\n&\\times exp\\{-\\frac{1}{2} \\textbf{tr}[\\Sigma^{-1}(Y'\\Omega^{-1}Y - 2A'X'\\Omega^{-1}Y + A'X'\\Omega^{-1}XA \\\\\n&+ A'\\underline{V}^{-1}A -2A'\\underline{V}^{-1}\\underline{A} + \\underline{A}'\\underline{V}^{-1}\\underline{A} + \\underline{S})]\\}\n\n\\end{align}\\]\nWhich is the kernel represent as Normal-inverse Wishart distribution and given by:\n\\[\\begin{align}\n\nP(A,\\Sigma|Y,X,\\Omega) &\\sim \\mathcal{NIW}(\\bar{A},\\bar{V},\\bar{S},\\bar{\\nu}) \\\\\n&\\\\\n\\bar{V} &= (X'\\Omega^{-1}X + \\underline{V}^{-1})^{-1} \\\\\n\\bar{A} &= \\bar{V}(X'\\Omega^{-1}Y + \\underline{V}^{-1}\\underline{A}) \\\\\n\\bar{\\nu} &= T + \\underline{\\nu}\\\\\n\\bar{S} &= \\underline{S} + Y'\\Omega^{-1}Y + \\underline{A}'\\underline{V}^{-1}\\underline{A} - \\bar{A}'\\bar{V}^{-1}\\bar{A}\n\n\\end{align}\\]\nThe posterior of \\(\\lambda_t\\) could be precent in the form of \\(\\sigma^2\\) which is:\n\\[\\begin{align}\n\nP(\\lambda_t|Y,X,A,\\Sigma,h,h_0,\\sigma^2_v) &\\propto L(A,\\Sigma,\\lambda_t,h,h_0,\\sigma^2_v|Y,X)P(\\lambda_t) \\\\\n\\\\\n&\\propto \\lambda_t^{-\\frac{N}{2}}exp({-\\frac{1}{2}}{\\frac{1}{\\lambda_t}}\\textbf{tr}[\\Sigma^{-1}\\epsilon_t' diag(\\sigma^2)\\epsilon_t]) \\\\\n&\\times \\frac{1}{\\alpha}exp\\{ -\\frac{1}{\\alpha}\\lambda_t \\}\\\\\n&\\propto \\lambda_t^{-\\frac{N}{2}+1-1} exp\\{-\\frac{1}{2}[\\frac{\\textbf{tr}[\\Sigma^{-1}\n\\epsilon_t'diag(\\sigma^2)\\epsilon_t]} {\\lambda_t} +\\frac{2}{\\alpha}\\lambda_t]\\}\n\n\\end{align}\\]\nThe above expression can be rearranged in the form of a Generalized inverse Gaussian distribution kernel as follows:\n\\[\\begin{align}\n\n\\lambda_t|Y,A,\\Sigma &\\sim \\mathcal{GIG}(a,b,p) \\\\\n\\\\\na &=\\frac{2}{\\alpha} \\\\\nb &= \\textbf{tr}[\\Sigma^{-1}\\epsilon_t'diag(\\sigma^2)\\epsilon_t] \\\\\np &= -\\frac{N}{2}+1\n\n\\end{align}\\]\nThe posterior for Hierarchical structure \\(h,h_0,s,\\sigma^2_v\\) remain unchanged:\n\\[\\begin{align}\n\nP(h|h_0,\\sigma^2_v,s,\\tilde{y}) &\\sim \\mathcal{N}(\\overline{h},\\overline{V}_h)\\\\\n\\overline{V}_h&=\\left[ diag\\left(\\sigma^2_s \\right)^{-1}+ \\sigma^{-2}_vH'H\\right]^{-1}\\\\\n\\overline{h}&=\\overline{V}_h\\left[ diag\\left(\\sigma^2_s \\right)^{-1}(\\tilde{y}-\\mu_s)+ \\sigma^{-2}_vh_0e_{1.T}\\right]\\\\\n\\\\\nP(h_0|h,\\sigma^2_v,s,\\tilde{y}) &\\sim \\mathcal{N}(\\overline{h}_0,\\overline{\\sigma}^2_h)\\\\\n\\overline{\\sigma}^2_h&=\\left(\\underline{\\sigma}^{-2}_h+ \\sigma^{-2}_v\\right)^{-1}\\\\\n\\overline{h}_0&=\\overline{\\sigma}^2_h\\left(\\sigma^{-2}_ve_{1.T}'Hh\\right)^{-1}\\\\\n\\\\\nP(\\sigma^2_v|h,h_0,s,\\tilde{y}) &\\sim \\mathcal{IG2}(\\overline{s},\\overline{\\nu})\\\\\n\\overline{\\nu}&=\\underline{\\nu}+ T\\\\\n\\overline{s}&=\\underline{s}+\\left( Hh-h_0e_{1.T} \\right)'\\left( Hh-h_0e_{1.T} \\right)\\\\\n\\\\\nPr[s_t=m|\\tilde{y},h_t]&=\\frac{\\omega_{m.t}}{\\sum_{i=1}^{10}\\omega_{m.t}}\n\n\\end{align}\\]\n\n\n4.7.3 Gibbs Sampler\nGiven the hyper-parameters \\(h_0,s,\\sigma^{2}_v\\), we could use same Hierarchical draws based on function SVcommon.Gibbs.iteration and then follow 3 steps:\n\nDraw \\((h^{(s)}, s^{(s)}, \\sigma^{2(s)}_v,h^{(s)}\\)\nDraw \\(\\lambda^{(s)}\\)\nDraw \\((A,\\Sigma)\\sim\\mathcal{MNIW}(\\bar{A},\\bar{V},\\bar{S},\\bar{\\nu})\\)\n\nTherefore, we could have the function posterior.draws.exten.hetero to draw \\(A\\) and \\(\\Sigma\\):\n\nposterior.draws.exten.hetero = function (total_S, Y, X){\n  \n  ## Pre-setup \n     \n  N             = ncol(Y) \n  t             = N + 1\n  p             = frequency(Y)                      \n  A.hat         = solve(t(X)%*%X)%*%t(X)%*%Y\n  Sigma.hat     = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)\n  T             = dim(Y)[1] \n  K             = dim(X)[2] \n  H             = diag(T)\n  sdiag(H,-1)   = -1\n  HH            = 2*diag(T)\n  sdiag(HH,-1)  = -1\n  sdiag(HH,1)   = -1\n  \n  # Prior distribution (with Minnesota prior)\n\n  A.prior           = matrix(0,nrow(A.hat),ncol(A.hat))\n  A.prior[2:t,]     = diag(N)\n  V.prior           = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))\n  S.prior           = diag(diag(Sigma.hat))\n  nu.prior          = N + 2\n  lambda.priors     = list(alpha=2)\n  # \n  HH                = HH\n  h0.m              = 0\n  h0.v              = 1\n  sigmav.s          = 1\n  sigmav.nu         = 1\n\n\n  # Define posterior\n  posteriors    = list(                        \n    H           = matrix(NA, T, total_S),\n    sigma2      = matrix(NA, T, total_S),\n    s           = matrix(NA, T, total_S),\n    h0          = rep(NA, total_S),\n    sigma.v2    = rep(NA, total_S),\n    A           = array(NA, c((1 + N * p), N, total_S)),\n    Sigma       = array(NA, c(N, N, total_S)),\n    lambda      = rep(NA, total_S)  \n)\n  \n  # Define auxiliary\n  aux        = list(\n    Y        = Y,\n    X        = X,\n    H        = matrix(1, T, 1),\n    h0       = 0,\n    sigma.v2 = 1,\n    s        = matrix(1, T, 1),\n    A        = matrix(0, K, N),\n    Sigma    = matrix(0, N, N),             \n    sigma2   = matrix(1, T, 1),\n    lambda   = 2\n  )\n\n   # ----------------------posterior.draws.exten.hetero------------------------------\n\n for (s in 1:total_S){ \n\n  # normal-inverse Wishard posterior \n      V.bar.inv              = t(aux$X)%*%(aux$lambda*diag(1/as.vector(aux$sigma2)))%*%aux$X + solve(V.prior)\n      V.bar.inv              = invfunc(V.bar.inv)\n      V.bar                  = solve(V.bar.inv)\n      A.bar                  = V.bar%*%(t(aux$X)%*%(aux$lambda*diag(1/as.vector(aux$sigma2)))%*%aux$Y + solve(V.prior)%*%A.prior) \n      nu.bar                 = T + nu.prior\n      S.bar                  = S.prior + t(aux$Y)%*%(aux$lambda*diag(1/as.vector(aux$sigma2)))%*%aux$Y + t(A.prior)%*%solve(V.prior)%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar \n      S.bar                  = invfunc(S.bar)\n      S.bar.inv              = solve(S.bar)\n      \n    # posterior draws for A and Sigma\n      Sigma.posterior.IW     = rWishart(1, df=nu.bar, Sigma=S.bar.inv)\n      Sigma.posterior.draw   = apply(Sigma.posterior.IW, 3 ,solve)\n      aux$Sigma              = array(Sigma.posterior.draw,c(N,N,1))\n      \n      for (i in 1:dim(aux$Sigma)[3]) {\n          eigen_decomp &lt;- eigen(aux$Sigma[, , i], symmetric = TRUE)\n          eigen_decomp$values[eigen_decomp$values &lt;= 0] &lt;- 1\n          aux$Sigma[, , i] &lt;- eigen_decomp$vectors %*% diag(eigen_decomp$values) %*% t(eigen_decomp$vectors)\n      }\n      A.norm                 = array(rnorm(prod(c(K,N,1))),c(K,N,1))\n      L                      = t(chol(V.bar))      \n      aux$A                  = A.bar + L%*%A.norm[,,1]%*%chol(aux$Sigma[,,1])\n      \n      u.t = Y-X%*%aux$A\n      \n      matrix_2x2 &lt;- matrix(aux$Sigma, nrow = 2, ncol = 2)\n    # posterior draws for lambda\n      lambda.p = -N/2 + 1 \n      chi.b    = sum(diag(solve(matrix_2x2)%*%t(u.t)%*%diag(1/as.vector(aux$sigma2))%*%u.t))\n      psi.a    =  2 / lambda.priors$alpha\n      \n      lambda.draw = GIGrvg::rgig(n = 1, lambda = lambda.p, chi = chi.b, psi = psi.a)\n      aux$lambda = lambda.draw\n  \n      # posterior draw for sigma2   \n      N             = dim(aux$Y)[2] \n      alpha.st      = c(1.92677,1.34744,0.73504,0.02266,0-0.85173,-1.97278,-3.46788,-5.55246,-8.68384,-14.65000)\n      sigma.st      = c(0.11265,0.17788,0.26768,0.40611,0.62699,0.98583,1.57469,2.54498,4.16591,7.33342)\n      pi.st         = c(0.00609,0.04775,0.13057,0.20674,0.22715,0.18842,0.12047,0.05591,0.01575,0.00115)\n      \n      Lambda        = solve(chol(aux$Sigma[,,1]))\n      Z             = rowSums( ( aux$Y - aux$X %*% aux$A ) %*% Lambda ) / sqrt(N)\n      Y.tilde       = as.vector(log((Z + 0.0000001)^2))\n      Ytilde.alpha  = as.matrix(Y.tilde - alpha.st[as.vector(aux$s)])\n        \n      # sampling initial condition\n      V.h0.bar      = 1/((1 / h0.v) + (1 / aux$sigma.v2))\n      m.h0.bar      = V.h0.bar*((h0.m / h0.v) + (aux$H[1] / aux$sigma.v2))\n      h0.draw       = rnorm(1, mean = m.h0.bar, sd = sqrt(V.h0.bar))\n      aux$h0        = h0.draw\n      \n      # sampling sigma.v2\n      sigma.v2.s    = sigmav.s + sum(c(aux$H[1] - aux$h0, diff(aux$H))^2)\n      sigma.v2.draw = sigma.v2.s / rchisq(1, sigmav.nu + T)\n      aux$sigma.v2  = sigma.v2.draw\n      \n      # sampling auxiliary states\n      Pr.tmp        = simplify2array(lapply(1:10,function(x){\n        dnorm(Y.tilde, mean = as.vector(aux$H + alpha.st[x]), sd = sqrt(sigma.st[x]), log = TRUE) + log(pi.st[x])\n      }))\n      Pr            = t(apply(Pr.tmp, 1, function(x){exp(x - max(x)) / sum(exp(x - max(x)))}))\n      s.cum         = t(apply(Pr, 1, cumsum))\n      r             = matrix(rep(runif(T), 10), ncol = 10)\n      ss            = apply(s.cum &lt; r, 1, sum) + 1\n      aux$s         = as.matrix(ss)\n      \n      # sampling log-volatilities using functions for tridiagonal precision matrix\n      Sigma.s.inv   = diag(1 / sigma.st[as.vector(aux$s)])\n      D.inv         = Sigma.s.inv + (1 / aux$sigma.v2) * HH\n      b             = as.matrix(Ytilde.alpha / sigma.st[as.vector(aux$s)] + (aux$h0/aux$sigma.v2)*diag(T)[,1])\n      lead.diag     = diag(D.inv)\n      sub.diag      = mgcv::sdiag(D.inv, -1)\n      D.chol        = mgcv::trichol(ld = lead.diag, sd = sub.diag)\n      D.L           = diag(D.chol$ld)\n      mgcv::sdiag(D.L,-1) = D.chol$sd\n      x             = as.matrix(rnorm(T))\n      a             = forwardsolve(D.L, b)\n      draw          = backsolve(t(D.L), a + x)\n      aux$H         = as.matrix(draw)\n      aux$sigma2    = as.matrix(exp(draw))\n        \n      # output list\n      posteriors$H[,s]             = aux$H\n      posteriors$sigma2[,s]        = aux$sigma2\n      posteriors$s[,s]             = aux$s\n      posteriors$h0[s]             = aux$h0\n      posteriors$sigma.v2[s]       = aux$sigma.v2\n      posteriors$A[,,s]            = aux$A\n      posteriors$Sigma[,,s]        = aux$Sigma\n      posteriors$lambda[s]         = aux$lambda\n      \n  }\n        \n  return(posteriors)\n  \n}\n\nThen we also could have A and \\(\\Sigma\\). From where, we can observe the mean of \\(A_{21},A_{32}\\) varying around value of 1."
  },
  {
    "objectID": "index.html#basic-model",
    "href": "index.html#basic-model",
    "title": "An Evidence-based Forecast: Gold as a Traditional Safe-Haven Investment",
    "section": "5.1 Basic Model",
    "text": "5.1 Basic Model\nFirst, set up and read our data:\n\n## Create Y and X\ny             = ts(all_data[,1:ncol(all_data)])     # 10col, 135row\nY             = ts(y[13:nrow(y),], frequency=12)      # 10col, 131row\nX             = matrix(1,nrow(Y),1)\nfor (i in 1:frequency(Y)){\n  X           = cbind(X,y[13:nrow(y)-i,])            # 10*4+1=41col, 131row\n}\n \n## Pre-setup \nN             = ncol(Y)                             # N=10\np             = frequency(Y)                        # p=4\nA.hat         = solve(t(X)%*%X)%*%t(X)%*%Y\nSigma.hat     = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)\n\n# Prior distribution (with Minnesota prior)\nkappa.1       = 0.02^2                                # shrinkage for A1 to Ap\nkappa.2       = 200                                   # shrinkage for constant \nA.prior       = matrix(0,nrow(A.hat),ncol(A.hat))\n# A.prior[2,1]  = 1\n# A.prior[7,6]  = 1  \nA.prior[2:13,] = diag(12)\nV.prior       = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))\nS.prior       = diag(diag(Sigma.hat))\nnu.prior      = N+2 \nI.matrix            = diag(1,nrow(Y),nrow(Y))\n\nThen, we could have the mean of A:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.004298\n-0.023149\n0.009321\n-0.001012\n0.007642\n0.098861\n0.028941\n0.169618\n-0.005717\n-0.043994\n-0.001769\n-0.001993\n\n\n0.999792\n0.000939\n0.000665\n0.000641\n0.000222\n-0.000385\n0.000004\n-0.000886\n-0.000155\n0.001099\n0.000135\n0.000051\n\n\n0.000403\n1.000012\n-0.000220\n0.000560\n0.000127\n0.001208\n-0.002601\n-0.002799\n0.000251\n0.002998\n-0.000163\n0.000475\n\n\n0.000471\n0.001538\n0.997579\n-0.001917\n-0.000680\n-0.002079\n-0.001250\n-0.001013\n0.000229\n0.002503\n-0.000188\n0.000955\n\n\n0.000407\n0.000160\n-0.002930\n0.997198\n-0.000671\n-0.000973\n-0.001437\n0.000079\n0.000044\n0.000674\n-0.000206\n0.000659\n\n\n-0.000093\n0.001903\n0.001711\n0.001530\n1.000143\n-0.001062\n0.002730\n-0.003802\n0.000206\n0.002266\n0.000121\n0.000506\n\n\n0.000272\n-0.002239\n0.000352\n0.000327\n0.000451\n0.909861\n-0.001616\n0.026858\n0.001181\n-0.003858\n-0.000039\n-0.001430\n\n\n-0.000358\n0.006131\n0.002884\n0.001598\n-0.001214\n-0.001994\n0.994742\n-0.003491\n-0.001531\n0.005625\n0.000498\n0.001547\n\n\n0.000398\n-0.001878\n-0.001815\n-0.000834\n0.000638\n-0.007863\n0.001783\n0.983310\n0.003099\n-0.001487\n-0.000178\n-0.001369\n\n\n-0.000224\n0.000380\n0.000093\n0.000022\n-0.000154\n0.002932\n0.001291\n-0.004438\n0.999020\n0.001005\n0.000168\n0.001034\n\n\n0.000425\n-0.002590\n-0.001208\n-0.000128\n0.000119\n0.001178\n-0.002902\n0.000402\n0.000061\n0.998902\n-0.000156\n0.000472\n\n\n0.000036\n0.000250\n0.000086\n0.000125\n0.000007\n-0.000054\n-0.000391\n-0.000017\n0.000011\n0.000256\n0.999962\n-0.000148\n\n\n\n\n\n\n\n\nAnd, we could have the mean of \\(\\Sigma\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.002077\n-0.001460\n-0.004480\n-0.003864\n-0.000069\n0.002707\n0.000642\n-0.000320\n0.000595\n-0.000090\n-0.000401\n-0.000514\n\n\n-0.001460\n0.042012\n0.022136\n0.015871\n0.004942\n-0.053876\n0.002678\n0.009787\n0.009400\n0.024394\n0.000227\n-0.011441\n\n\n-0.004480\n0.022136\n0.072509\n0.064367\n0.004745\n-0.037687\n0.025309\n0.004077\n0.006626\n0.010169\n0.001697\n-0.011639\n\n\n-0.003864\n0.015871\n0.064367\n0.064672\n0.003739\n-0.024884\n0.032114\n0.002651\n0.006129\n0.007076\n0.001377\n-0.008689\n\n\n-0.000069\n0.004942\n0.004745\n0.003739\n0.003449\n-0.011498\n-0.000407\n0.006679\n0.002660\n0.001892\n-0.000346\n-0.009340\n\n\n0.002707\n-0.053876\n-0.037687\n-0.024884\n-0.011498\n1.763861\n0.028616\n-0.098711\n-0.042208\n-0.011745\n-0.001098\n0.020573\n\n\n0.000642\n0.002678\n0.025309\n0.032114\n-0.000407\n0.028616\n0.255045\n-0.093297\n0.008922\n0.016572\n-0.000345\n0.006370\n\n\n-0.000320\n0.009787\n0.004077\n0.002651\n0.006679\n-0.098711\n-0.093297\n0.411516\n0.005838\n-0.029506\n0.000315\n-0.030378\n\n\n0.000595\n0.009400\n0.006626\n0.006129\n0.002660\n-0.042208\n0.008922\n0.005838\n0.015545\n0.004678\n-0.000684\n-0.008457\n\n\n-0.000090\n0.024394\n0.010169\n0.007076\n0.001892\n-0.011745\n0.016572\n-0.029506\n0.004678\n0.031831\n-0.000133\n-0.003685\n\n\n-0.000401\n0.000227\n0.001697\n0.001377\n-0.000346\n-0.001098\n-0.000345\n0.000315\n-0.000684\n-0.000133\n0.000464\n0.001354\n\n\n-0.000514\n-0.011441\n-0.011639\n-0.008689\n-0.009340\n0.020573\n0.006370\n-0.030378\n-0.008457\n-0.003685\n0.001354\n0.048256\n\n\n\n\n\n\n\n\nTherefore, we could have histogram and time series of A for Gibbs sampler:\n\n\n\n\n\nFigure 6 Histograms and trace plots on key variables using basic model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwe could clearly see that underling our Minnesota prior assumption, a high level of shrinkage of random walk process, Gold have its first lag coefficient really close to 1, and other variables close to 0. This support our basic understanding of Gold nature, a safe assets.\nFollowing presents a 3D visualization of the density intervals for the log Gold Future Prices and Log Nasdaq Index points. From past trends we could clearly see that negative correlations between those two. When the market suffered from dot-com and global financial crisis during 2000 to 2012, Gold futures increase sharply as expectation of risks, market returns fluctuations a lot. For next 24 months forecasting based on benchmark model, we could see Log gold future price more sharply increase then Log Nasdaq Index points. The varying heights of the intervals reflect the level of prediction certainty; as we project further into the future, the intervals become wider and more dispersed due to increased uncertainty.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbove graphs we conducted a time series forecast analysis on the gold price (lgold) and the Nasdaq index (LNasIndex) and plotted a comparison chart between the actual data and the forecast results.\nGold price (lgold) From 2000 to 2024, the gold price showed an overall upward trend. The blue shaded area represents the 90% of confidence interval of the forecast value, which gradually widens over time, indicating an increase in forecast uncertainty. The forecast trend line (solid blue line) indicates that the gold price may continue to rise, but the dispersion of the forecast values indicates that future price fluctuations are large.\nNasdaq index (LNasIndex) Same as lgold, the Nasdaq index also showed an upward trend, although it fluctuated greatly during the period. The red shaded area represents the 90% of confidence interval of the forecast value, which indicating an increase in forecast uncertainty. The forecast trend line (solid red line) shows that the Nasdaq index may continue to rise in long run, but there is also great uncertainty.\nThe width of the confidence interval shows that the range of future changes will increase as the forecast period increases. Investors should take these uncertainties into account when making decisions and conduct corresponding risk management."
  },
  {
    "objectID": "index.html#extension-model",
    "href": "index.html#extension-model",
    "title": "An Evidence-based Forecast: Gold as a Traditional Safe-Haven Investment",
    "section": "5.2 Extension Model",
    "text": "5.2 Extension Model\nDue to the tediousness of computer calculations, we simplified the number of iterations to extend the predictions of the basic model. Although the results are not satisfactory, they also assist and strengthen the views we have obtained in the basic model.\n\n5.2.1 Laplace Ditribution of errors\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.004429\n0.016218\n-0.000813\n-0.016809\n0.011316\n0.053302\n0.036399\n-0.011717\n0.011843\n0.015110\n-0.002494\n-0.011072\n\n\n0.999734\n0.000346\n0.001073\n0.001216\n0.000342\n0.001801\n0.000317\n-0.000544\n0.000036\n0.000137\n0.000103\n-0.000938\n\n\n0.000408\n1.000709\n-0.000028\n0.000288\n-0.000038\n0.001630\n-0.000685\n0.001445\n0.000741\n0.001770\n-0.000115\n0.000821\n\n\n0.000404\n0.001476\n0.998543\n-0.001402\n-0.000619\n-0.001453\n-0.000351\n0.002754\n0.000448\n0.002183\n-0.000137\n0.001172\n\n\n0.000425\n0.000687\n-0.001911\n0.997775\n-0.000477\n-0.003386\n0.000545\n0.003739\n0.000166\n0.000805\n-0.000166\n0.000973\n\n\n-0.000198\n0.000863\n0.001217\n0.001299\n1.000083\n0.000632\n0.001077\n-0.002254\n-0.000097\n0.000722\n0.000163\n-0.000117\n\n\n0.000259\n-0.001301\n-0.000534\n-0.000417\n-0.000044\n0.942566\n0.001652\n-0.000418\n0.000383\n-0.000589\n-0.000174\n0.000306\n\n\n-0.000157\n0.003557\n0.001984\n0.001064\n-0.000695\n0.001466\n0.996376\n-0.003802\n-0.000339\n0.003451\n0.000284\n0.000706\n\n\n0.000230\n-0.001800\n-0.001241\n-0.000493\n0.000430\n-0.004680\n0.001354\n0.998588\n0.000451\n-0.001672\n-0.000145\n-0.000937\n\n\n-0.000167\n0.000167\n0.000355\n0.000531\n-0.000002\n0.002449\n-0.001092\n-0.000583\n0.999293\n0.000182\n0.000138\n0.000604\n\n\n0.000243\n-0.000132\n0.000079\n0.000505\n-0.000018\n-0.004482\n-0.000714\n0.000983\n0.000454\n1.000265\n-0.000033\n0.000807\n\n\n0.000213\n-0.000120\n-0.000872\n-0.000810\n-0.000033\n0.001899\n0.000491\n0.000477\n0.000262\n-0.000028\n0.999925\n0.000431\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.001556\n-0.000247\n-0.002559\n-0.002315\n0.000046\n-0.000164\n0.000635\n0.000061\n0.000373\n-0.000026\n-0.000324\n0.000090\n\n\n-0.000247\n0.012832\n0.005426\n0.003904\n0.000238\n-0.002567\n-0.000848\n-0.001356\n0.001395\n0.006807\n0.000156\n-0.000688\n\n\n-0.002559\n0.005426\n0.046399\n0.042608\n0.001715\n0.001395\n0.011173\n-0.001295\n0.003922\n0.001354\n0.000786\n-0.004668\n\n\n-0.002315\n0.003904\n0.042608\n0.043830\n0.001711\n-0.001361\n0.013296\n-0.001635\n0.004236\n0.000666\n0.000498\n-0.004248\n\n\n0.000046\n0.000238\n0.001715\n0.001711\n0.001759\n-0.001009\n-0.000532\n0.000040\n0.000430\n-0.000273\n-0.000196\n-0.004581\n\n\n-0.000164\n-0.002567\n0.001395\n-0.001361\n-0.001009\n1.053716\n0.003187\n-0.001863\n-0.002719\n-0.003805\n0.000753\n-0.000002\n\n\n0.000635\n-0.000848\n0.011173\n0.013296\n-0.000532\n0.003187\n0.119232\n-0.004267\n0.007433\n0.000634\n-0.000490\n0.004980\n\n\n0.000061\n-0.001356\n-0.001295\n-0.001635\n0.000040\n-0.001863\n-0.004267\n0.039166\n-0.001043\n-0.001839\n0.000037\n0.000625\n\n\n0.000373\n0.001395\n0.003922\n0.004236\n0.000430\n-0.002719\n0.007433\n-0.001043\n0.007201\n0.000787\n-0.000365\n-0.001868\n\n\n-0.000026\n0.006807\n0.001354\n0.000666\n-0.000273\n-0.003805\n0.000634\n-0.001839\n0.000787\n0.009794\n-0.000068\n-0.000045\n\n\n-0.000324\n0.000156\n0.000786\n0.000498\n-0.000196\n0.000753\n-0.000490\n0.000037\n-0.000365\n-0.000068\n0.000333\n0.000651\n\n\n0.000090\n-0.000688\n-0.004668\n-0.004248\n-0.004581\n-0.000002\n0.004980\n0.000625\n-0.001868\n-0.000045\n0.000651\n0.030938\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.2.2 Stochastic Volatility\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.006014\n0.026495\n0.047680\n0.038882\n0.010155\n-0.025184\n0.070029\n-0.030703\n0.020465\n0.024434\n0.000347\n0.009250\n\n\n0.999942\n-0.000074\n-0.000052\n-0.000008\n0.000010\n-0.001523\n0.000604\n0.000143\n0.000068\n0.000054\n0.000005\n-0.000065\n\n\n-0.000006\n0.999952\n0.000139\n0.000229\n-0.000015\n0.000369\n-0.000618\n-0.000982\n0.000045\n0.000443\n0.000006\n0.000268\n\n\n0.000096\n0.000427\n0.999564\n-0.000456\n-0.000015\n-0.002134\n-0.000230\n-0.000599\n-0.000071\n0.000412\n-0.000031\n-0.000367\n\n\n0.000031\n0.000085\n-0.000273\n0.999757\n-0.000074\n-0.000314\n-0.000464\n0.000596\n0.000078\n0.000240\n-0.000033\n0.000075\n\n\n-0.000057\n0.000140\n0.000221\n0.000252\n0.999987\n-0.000937\n0.000179\n-0.000493\n0.000056\n0.000231\n0.000022\n-0.000031\n\n\n-0.000045\n-0.000167\n0.000312\n0.000332\n-0.000017\n0.987669\n0.000060\n0.002553\n0.000016\n-0.000298\n0.000012\n0.000112\n\n\n0.000002\n0.000597\n0.000371\n0.000090\n-0.000146\n0.000668\n0.999410\n-0.000193\n-0.000305\n0.000542\n0.000059\n0.000107\n\n\n0.000067\n-0.000361\n-0.000565\n-0.000421\n0.000071\n-0.001161\n0.000105\n0.998272\n0.000277\n-0.000375\n-0.000045\n-0.000088\n\n\n-0.000011\n0.000271\n-0.000080\n-0.000056\n-0.000011\n-0.000677\n-0.000295\n-0.000481\n0.999802\n0.000280\n-0.000003\n0.000124\n\n\n0.000061\n-0.000417\n-0.000191\n-0.000117\n-0.000009\n-0.000577\n-0.000290\n0.000689\n0.000073\n0.999736\n-0.000017\n0.000063\n\n\n0.000032\n0.000082\n-0.000103\n-0.000004\n0.000036\n-0.000476\n-0.000083\n-0.000037\n0.000204\n0.000107\n0.999983\n-0.000119\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.000359\n-0.000021\n-0.000589\n-0.000499\n0.000023\n-0.000789\n0.000075\n0.000049\n0.000095\n0.000049\n-0.000077\n-0.000026\n\n\n-0.000021\n0.004932\n0.002117\n0.001666\n0.000277\n-0.005084\n0.000878\n0.000289\n0.000994\n0.002407\n0.000013\n-0.000587\n\n\n-0.000589\n0.002117\n0.010360\n0.009322\n0.000318\n-0.001336\n0.002861\n0.000867\n0.000940\n0.000694\n0.000182\n-0.001085\n\n\n-0.000499\n0.001666\n0.009322\n0.009788\n0.000288\n-0.001384\n0.003300\n0.000206\n0.001037\n0.000587\n0.000107\n-0.001053\n\n\n0.000023\n0.000277\n0.000318\n0.000288\n0.000440\n-0.000675\n0.000018\n0.000520\n0.000245\n0.000062\n-0.000064\n-0.001263\n\n\n-0.000789\n-0.005084\n-0.001336\n-0.001384\n-0.000675\n0.274415\n0.000137\n-0.007584\n-0.004730\n-0.002688\n0.000284\n0.001381\n\n\n0.000075\n0.000878\n0.002861\n0.003300\n0.000018\n0.000137\n0.030915\n-0.007810\n0.002234\n0.001118\n-0.000216\n0.000730\n\n\n0.000049\n0.000289\n0.000867\n0.000206\n0.000520\n-0.007584\n-0.007810\n0.056988\n-0.000550\n-0.003409\n0.000062\n-0.001911\n\n\n0.000095\n0.000994\n0.000940\n0.001037\n0.000245\n-0.004730\n0.002234\n-0.000550\n0.002432\n0.000579\n-0.000115\n-0.001177\n\n\n0.000049\n0.002407\n0.000694\n0.000587\n0.000062\n-0.002688\n0.001118\n-0.003409\n0.000579\n0.003219\n-0.000044\n-0.000223\n\n\n-0.000077\n0.000013\n0.000182\n0.000107\n-0.000064\n0.000284\n-0.000216\n0.000062\n-0.000115\n-0.000044\n0.000084\n0.000219\n\n\n-0.000026\n-0.000587\n-0.001085\n-0.001053\n-0.001263\n0.001381\n0.000730\n-0.001911\n-0.001177\n-0.000223\n0.000219\n0.008304\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoth models percent not enough smooth in A and \\(\\Sigma\\), but support our results in base models. Under extension models, the serial variance become variate over time and then the lNasdaq performance is much worse than the lgold. This provide a basic understanding of our current society, a high level of uncertainty and worse economies environment."
  }
]