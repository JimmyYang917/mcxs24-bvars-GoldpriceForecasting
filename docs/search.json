[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An Evidence-based Forecast: Gold as a Traditional Safe-Haven Investment",
    "section": "",
    "text": "Abstract. This research aims to explore future trends in Gold prices as a traditonal safe-haven investment using a Bayesian VARs model. In the wake of the 2008 financial crisis and especially the 2019 global Covid-19 pandemic, the world economy appears to be on the brink of a looming risk: a world-wide economic recession. The concern over the risk of investment returns has become a primary focus for global investors and financial institutions. This unease has been further exacerbated by geopolitical conflicts such as the Russia-Ukraine war (2022) and the Israeli-Palestinian conflict (2023). Consequently, this research aims to provide a briefly discussion and data-driven forecast of traditional safe-haven assets: Gold, under current circumstances. Factors considered include emerging safe-haven investments, risk-free investment assets, comparable investments, market returns, inflation on both demand and supply sides, broad money supply (M2), interest rates, unemployment rates and market volatility.\nKeywords. Bayesian VARs, Gold price, Inflation, Interset rate, Unemployments, US Bond Yeild, Safe-haven Assets, Forecasting, Volatility, R, Quarto."
  },
  {
    "objectID": "index.html#first-benchmark-assumption-basic-model-with-minnesota-prior",
    "href": "index.html#first-benchmark-assumption-basic-model-with-minnesota-prior",
    "title": "An Evidence-based Forecast: Gold as a Traditional Safe-Haven Investment",
    "section": "First Benchmark Assumption: Basic model with Minnesota prior",
    "text": "First Benchmark Assumption: Basic model with Minnesota prior\nIn real life, macroeconomic variables are more likely being unit-root non stationary and are well-characterized by a multivariate random walk process\n\\[\\begin{aligned}\n\ny_{t} = y_{t-1} + \\epsilon_t\n\n\\end{aligned}\\]\n-Therefore, we first use Minnesota prior(1984) for our Bayesian forecasting:\nSet the prior mean \\(A\\) to:\n\\[\\begin{aligned}\n\n\\underline{A}=\\left[ 0_{N\\times1} \\ \\ \\ \\  I_{N} \\ \\ \\ \\ \\ 0_{N\\times(p-1)N} \\right]'\n\n\\end{aligned}\\]\nwhere the mean of first lag equal to 1 and mean of constant term and other lags are 0.\nSet the column specific prior covariance of \\(A\\) (prior shrinkage)to:\n\\[\\begin{aligned}\n\n\\underline{V} = diag\\left[ \\kappa_{2} \\quad \\kappa_{1}(\\textbf{p} ^{-2}\\otimes I^{'}_{N}) \\right]\n\n\\end{aligned}\\]\nwhere:\n\nFunction Proofing\nConsider Bi-variate Gaussian random walk process:\n\\[\ny_t =\n\\begin{bmatrix}\ny_{t,1} \\\\\ny_{t,2}\n\\end{bmatrix} =\n\\begin{bmatrix}\ny_{t-1,1} \\\\\ny_{t-1,2}\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\epsilon_{t,1} \\\\\n\\epsilon_{t,2}\n\\end{bmatrix}\n, where   \\  \\\n\\epsilon_{t,1} \\sim \\mathcal{N}(0,1)  \\ and \\\n\\epsilon_{t,2} \\sim \\mathcal{N}(0,1)\n\\]\n\\[\nY = \\begin{bmatrix}\ny_2' \\\\\ny_3' \\\\\n\\vdots \\\\\ny_n'\n\\end{bmatrix},\n\\quad\nX = \\begin{bmatrix}\n1 \\quad y_1' \\\\\n1 \\quad y_2' \\\\\n\\vdots \\quad \\vdots \\\\\n1 \\quad y_{n-1}'\n\\end{bmatrix}\n\\]\nFunction below is posterior.draws:\n\n## Posterior sample draw function for basic model(posterior.draws)\nposterior.draws       = function (S, Y, X){\n  \n    # normal-inverse Wishard posterior parameters\n    V.bar.inv         = t(X)%*%X + diag(1/diag(V.prior))\n    V.bar             = solve(V.bar.inv)\n    A.bar             = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)\n    nu.bar            = nrow(Y) + nu.prior\n    S.bar             = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar\n    S.bar.inv         = solve(S.bar)\n    \n    # posterior draws \n    Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)\n    Sigma.posterior   = apply(Sigma.posterior,3,solve)\n    Sigma.posterior   = array(Sigma.posterior,c(N,N,S))\n    A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))\n    L                 = t(chol(V.bar))\n    \n    for (s in 1:S){\n      A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])\n    }\n\n    output            = list(A.posterior=A.posterior, Sigma.posterior=Sigma.posterior)\n    return(output)\n}\n\n\nThe posterior mean of the \\(A\\) is:\n\n\n\n\n\n\n\nA\nSimulation Parameter Y1\nSimulation Parameter Y2\n\n\n\n\nConstant term\n0.0613\n0.0817\n\n\nY1 lag\n0.9887\n0.0052\n\n\nY2 lag\n0.0026\n0.9955\n\n\n\n\n\n\n\n\nTable 2 Basic Model Proofing Simulation for \\(A\\)\n\nThe posterior mean of the \\(\\Sigma\\) is:\n\n\n\n\n\n\n\nSigma\nSimulation Parameter Y1\nSimulation Parameter Y2\n\n\n\n\nY1 lag\n0.9783\n0.0879\n\n\nY2 lag\n0.0879\n1.0165\n\n\n\n\n\n\n\n\nTable 3 Basic Model Proofing Simulation for \\(\\Sigma\\)\nExtension Model:"
  },
  {
    "objectID": "index.html#the-extended-model-laplace-distribution-of-error-term",
    "href": "index.html#the-extended-model-laplace-distribution-of-error-term",
    "title": "An Evidence-based Forecast: Gold as a Traditional Safe-Haven Investment",
    "section": "The extended model: Laplace distribution of error term",
    "text": "The extended model: Laplace distribution of error term\nThe Basic Model is the standard VARs model that assume the error terms \\(U\\) are independent and identically distributed(\\(iid\\)) as \\(N\\sim (0,\\Sigma)\\). In other formation, it could be presented as \\(vec(U)\\sim N(0,\\Sigma\\otimes I_{t})\\). Where \\(\\Sigma\\) is a $n n $ covariance matrix, \\(I_{t}\\) is a $ t t$ identity matrix, \\(\\otimes\\) is the Kronecker product and the operator \\(vec(.)\\) is vectorization that inverts the matrix into the column vector by stacking the columns.\nTherefore, we could consider a more general covariance structure:\n\\[\\begin{aligned}\n\nvec(U)\\sim N(0,\\Sigma\\otimes I_{t})\n\n\\end{align}\\]\nThe extended model will be built based on the the change in distribution of the error to Laplace distribution instead of the normally distributed errors assumption. The Laplace distribution is suitable for describing financial anomalies due to its sharp peaks and thick tails and the use of this distribution improves the robustness of the model to anomalies and is particularly suitable for financial time series. As our variables are most financial time series data, a Laplace distribution is more suitable to apply to our error term.\nFollowing Eltoft,Kim, and Lee 2006b, for covariance with a general Kronecker structure, if each \\({\\lambda_t}\\) has an independent exponential distribution with mean \\({\\alpha}\\), then marginally \\({U_t}\\) has a multivariate Laplace distribution with mean vector 0 and covariance matrix \\({\\alpha\\Sigma}\\).\n\\[\\begin{align}\nU_t &\\sim \\text{Laplace}(0, \\alpha\\Sigma) \\\\\nU_t | \\lambda_t &\\sim \\mathcal{MN}(0, \\Sigma, \\lambda_t I_T) \\\\\n\\lambda_t &\\sim \\text{Exponential}(\\frac{1}{\\alpha})\n\\end{align}\\]\nThe kernel of the likelihood function:\n\\[\\begin{align}\nL(A,\\Sigma,\\lambda_t|Y,X) &\\propto \\det(\\Sigma)^{-\\frac{T}{2}} \\det(\\lambda_t I_T)^{-\\frac{N}{2}} exp\\{-\\frac{1}{2} tr[\\Sigma^{-1} (Y-XA)' (\\lambda_t I_T)^{-1} (Y-XA) ]\\}\n\\end{align}\\]\nFor posteriors distribution, \\(A\\), \\(\\Sigma\\) and \\(\\lambda_t\\) can then be derived using the likelihood and the prior distributions as follows:\n\\[\\begin{align}\np(A,\\Sigma|Y,X) &\\propto L(A,\\Sigma,\\lambda_t|Y,X)p(A,\\Sigma) \\\\\n\\\\\n&= \\det(\\Sigma)^{-\\frac{T}{2}} \\det(\\lambda_t I_T)^{-\\frac{N}{2}} exp\\{-\\frac{1}{2} tr[\\Sigma^{-1} (Y-XA)' (\\lambda_t I_T)^{-1} (Y-XA) ]\\} \\\\\n&\\times \\det(\\Sigma)^{-\\frac{N+k+\\underline{\\nu}+1}{2}} exp\\{-\\frac{1}{2}tr[\\Sigma^{-1}(A-\\underline{A})'(\\underline{V})^{-1}(A-\\underline{A})]\\} \\\\\n&\\times exp\\{-\\frac{1}{2}tr[\\Sigma^{-1}\\underline{S}]\\} \\\\\n&= \\det(\\Sigma)^{-\\frac{T+N+K+\\underline{\\nu}+1}{2}} \\det(\\lambda_t I_T)^{-\\frac{N}{2}} \\\\\n&\\times exp\\{-\\frac{1}{2} tr[\\Sigma^{-1}(Y'(\\lambda_t I_T)^{-1}Y - 2A'X'(\\lambda_t I_T)^{-1}Y + A'X'(\\lambda_t I_T)^{-1}XA \\\\\n&+ A'\\underline{V}^{-1}A -2A'\\underline{V}^{-1}\\underline{A} + \\underline{A}'\\underline{V}^{-1}\\underline{A} + \\underline{S})]\\}\n\n\\end{align}\\]\nThe kernel can be rearranged in the form of the Matrix-variate normal-inverse Wishart distribution.\n\\[\\begin{align}\np(A,\\Sigma|Y,X) &\\sim MNIW(\\bar{A},\\bar{V},\\bar{S},\\bar{\\nu}) \\\\\n&\\\\\n\\bar{V} &= (X'(\\lambda_t I_T)^{-1}X + \\underline{V}^{-1})^{-1} \\\\\n\\bar{A} &= \\bar{V}(X'(\\lambda_t I_T)^{-1}Y + \\underline{V}^{-1}\\underline{A}) \\\\\n\\bar{\\nu} &= T + \\underline{\\nu}\\\\\n\\bar{S} &= Y'(\\lambda_t I_T)^{-1}Y + \\underline{A}'\\underline{V}^{-1}\\underline{A} + \\underline{S} - \\bar{A}'\\bar{V}^{-1}\\bar{A}\n\\end{align}\\]\nThe kernel of the fully conditional posterior distribution of \\(\\lambda_t\\) is then derived as follows:\n\\[\\begin{align}\np(\\lambda_t|Y,X,A,\\Sigma) &\\propto L(A,\\Sigma,\\lambda_t|Y,X)p(\\lambda_t) \\\\\n\\\\\n&\\propto \\det(\\lambda_t I_T)^{-\\frac{N}{2}} exp\\{-\\frac{1}{2} tr[\\Sigma^{-1} (Y-XA)' (\\lambda_t I_T)^{-1} (Y-XA) ]\\} \\\\\n&\\times \\frac{1}{\\alpha}exp\\{ -\\frac{1}{\\alpha}\\lambda_t \\}\\\\\n\n&= \\lambda_t^{-\\frac{TN}{2}} exp\\{-\\frac{1}{2}\\frac{1}{\\lambda_t} tr[\\Sigma^{-1}(Y-XA)'(Y-XA)]\\}\\\\\n&\\times exp\\{-\\frac{1}{\\alpha}\\lambda_t \\}\\\\\n\n&= \\lambda_t^{-\\frac{TN}{2}+1-1} exp\\{-\\frac{1}{2}[\\frac{[tr[\\Sigma^{-1}(Y-XA)'(Y-XA)]}{\\lambda_t} +\\frac{2}{\\alpha}\\lambda_t]\\}\n\\end{align}\\]\nThe above expression can be rearranged in the form of a Generalized inverse Gaussian distribution kernel as follows:\n\\[\\begin{align}\n\\lambda_t|Y,A,\\Sigma &\\sim GIG(a,b,p) \\\\\n\\\\\na &=\\frac{2}{\\alpha} \\\\\nb &= tr[\\Sigma^{-1}(Y-XA)'(Y-XA)] \\\\\np &= -\\frac{TN}{2}+1\n\\end{align}\\]\n\nProof of extended model\nThe Gibbs sampler method will be applied to generate random draws from the full conditional posterior distribution:\n\nDraw \\(\\Sigma^{(s)}\\) from the \\(IW(\\bar{S},\\bar{\\nu})\\) distribution.\nDraw \\(A^{(s)}\\) from the \\(MN(\\bar{A},\\Sigma^{(s)}, \\bar{V})\\) distribution.\nDraw \\(\\lambda_t^{(s)}\\) from \\(GIG(a,b,p)\\).\n\nRepeat steps 1, step 2 and 3 for \\(S_1\\)+\\(S_2\\)times.\nDiscard the first draws that allowed the algorithm to converge to the stationary posterior distribution.\nOutput is \\(\\left\\{ {A^{(s)}, \\Sigma^{(s)}}, \\lambda_t^{(s)}\\right\\}^{S_1+S_2}_{s=S_1+1}\\).\nFunction below is posterior.draws.exten:\n\n## Posterior sample draw function for extended model(posterior.draws.exten)\nposterior.draws.exten = function (total_S, Y, X){\nfor (s in 1:total_S){\n    # NIW posterior parameters\n    V.bar.inv              = t(X)%*%X + diag(1/diag(lambda.posterior[s]* V.prior)) \n    V.bar                  = solve(V.bar.inv)\n    A.bar                  = V.bar%*%(t(X)%*%Y + diag(1/diag(lambda.posterior[s]* V.prior))%*%A.prior)\n    nu.bar                 = nrow(Y) + nu.prior\n    S.bar                  = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(lambda.posterior[s]* V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar\n    S.bar.inv              = solve(S.bar)\n  \n    # posterior draws for A and Sigma\n    Sigma.posterior.IW     = rWishart(1, df=nu.bar, Sigma=S.bar.inv)\n    Sigma.posterior.draw   = apply(Sigma.posterior.IW,3,solve)\n    Sigma.posterior[,,s]   = Sigma.posterior.draw\n    A.posterior[,,s]       = array(rnorm(prod(c(dim(A.bar),1))),c(dim(A.bar),1))\n    L                      = t(chol(V.bar))\n    A.posterior[,,s]       = A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])\n    \n    \n    # Update parameters for lambda posterior\n    p                      = lambda.priors$k - (N)/2              # N=10\n    diff_A                 = A.posterior[,,s] - A.prior\n    product                = t(diff_A) %*% solve(V.prior) %*% diff_A\n    b                      = sum(diag(solve(Sigma.posterior[,,s] %*% product)))\n    a                      = 2 / lambda.priors$theta\n    \n    # Draw next period value for lambda from GIG distribution\n    if (s!=total_S){\n       lambda.posterior[s+1] = GIGrvg::rgig(n=1, lambda = p, chi = b, psi = a)\n    }\n  }\n  \n    output                 = list(A.posterior.exten = A.posterior[,,(S1+1):S2], \n                                  Sigma.posterior.exten = Sigma.posterior[,,(S1+1):S2], \n                                  lambda.posterior.exten = lambda.posterior[(S1+1):S2,])\n\n    return(output)\n}\n\n\nFunction Proofing\nAfter fitting a model that includes a constant term and one lag with artificial data, just like the basic model, the extend model also shows that the posterior mean of both the autoregressive and covariance matrices closely identity matrix, and the posterior mean of the constant term is almost a vector of zeros.\n\nThe posterior mean of the \\(A\\) is:\n\n\n\n\n\n\n\nA\nSimulation Parameter Y1\nSimulation Parameter Y2\n\n\n\n\nConstant term\n0.0620\n0.0816\n\n\nY1 lag\n0.9886\n0.0052\n\n\nY2 lag\n0.0026\n0.9955\n\n\n\n\n\n\n\n\nTable 4 Extend Model Proofing Simulation for \\(A\\)\n\nThe posterior mean of the \\(\\Sigma\\) is:\n\n\n\n\n\n\n\nSigma\nSimulation Parameter Y1\nSimulation Parameter Y2\n\n\n\n\nY1 lag\n0.9780\n0.0876\n\n\nY2 lag\n0.0876\n1.0163\n\n\n\n\n\n\n\n\nTable 5 Extend Model Proofing Simulation for \\(\\Sigma\\)"
  }
]