[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An Evidence-based Forecast: Gold as a Traditional Safe-Haven Investment",
    "section": "",
    "text": "Abstract. This research aims to explore future trends in Gold prices as a traditional safe-haven investment using a Bayesian VARs model. In the wake of the 2008 financial crisis and especially the 2019 global Covid-19 pandemic, the world economy appears to be on the brink of a looming risk: a world-wide economic recession. The concern over the risk of investment returns has become a primary focus for global investors and financial institutions. This unease has been further exacerbated by geopolitical conflicts such as the Russia-Ukraine war (2022) and the Israeli-Palestinian conflict (2023). Consequently, this research aims to provide a briefly discussion and data-driven forecast of traditional safe-haven assets: Gold, under current circumstances. Factors considered include emerging safe-haven investments, risk-free investment assets, comparable investments, market returns, inflation on both demand and supply sides, broad money supply (M2), interest rates, unemployment rates and market volatility.\nKeywords. Bayesian VARs, Gold price, Inflation, Interest rate, Unemployment, US Bond Yield, Safe-haven Assets, Forecasting, Volatility, R, Quarto."
  },
  {
    "objectID": "index.html#matrix-notation-for-the-model",
    "href": "index.html#matrix-notation-for-the-model",
    "title": "An Evidence-based Forecast: Gold as a Traditional Safe-Haven Investment",
    "section": "Matrix Notation for the model:",
    "text": "Matrix Notation for the model:\nMatrix form are used to simplify the notation and the derivations. Let \\(T\\) be the available sample size for the variable \\(y\\) and \\(K\\) be the sum of lags and constant term (\\(K = 1 + pN\\)). Define a identity matrix of order \\(T\\), \\(I_T\\), as well as following matrix:\n\\[\\begin{aligned}\n\nA=\n\\begin{bmatrix}\n\\mu_{0}' \\\\ A_{1}' \\\\\nA_{2} '\\\\.\\\\.\\\\.\\\\A_{p}'\n\\end{bmatrix}_{K \\times N}\n\nY=\n\\begin{bmatrix}\ny_{1}' \\\\ y_{2}' \\\\\ny_{3} '\\\\.\\\\.\\\\.\\\\y_{T}'\n\\end{bmatrix}_{T \\times N}\n\nx_t=\n\\begin{bmatrix}\n1 \\\\ y_{t-1}' \\\\\ny_{t-2} '\\\\.\\\\.\\\\.\\\\y_{t-p}'\n\\end{bmatrix}_{K \\times 1}\n\nX=\n\\begin{bmatrix}\nx_{1}' \\\\ x_{2}' \\\\\nx_{3} '\\\\.\\\\.\\\\.\\\\x_{T}'\n\\end{bmatrix}_{T \\times K}\n\nE=\n\\begin{bmatrix}\n\\epsilon_1' \\\\ \\epsilon_2' \\\\\n\\epsilon_3 '\\\\.\\\\.\\\\.\\\\\\epsilon_T'\n\\end{bmatrix}_{T \\times N}\n\n\\end{aligned}\\]\nThen the model can be written in a concise notation as:\n\\[\\begin{aligned}\n\nY &= X A +E\n\\\\\n\\\\\nE|X &\\sim \\mathcal{MN}_{T\\times N}(\\textbf{0},\\Sigma,I_T)\n\n\\end{aligned}\\]\nGiven that the density function of Matrix-variate Normal distribution \\(Z\\sim \\mathcal{MN}_{T\\times N}(M,Q,P)\\) is:\n\\[\\begin{aligned}\n\n\\mathcal{MN}_{T\\times N}(M,Q,P) & =c^{-1}_{mn}exp\\left\\{ -\\frac{1}{2}\\textbf{tr}\\left[ Q^{-1}\\left( Z-M \\right)'P^{-1}\\left( Z-M \\right) \\right] \\right\\}\\\\\nc_{mn} & = \\left( 2\\pi \\right)^{\\frac{TN}{2}}det\\left( Q \\right)^{\\frac{T}{2}}det\\left( P \\right)^{\\frac{N}{2}}\n\n\\end{aligned}\\]\nBase on the model above, we could first turn B Vars(p) model into B Vars(1) model and easily regress to have the parameter matrix. Then we could have a \\(t+h\\) period forward forecasting with increase of variance, in this case: \\(h\\) = 24.\nThe main focus of estimate output is the conditional mean of Gold price, which base on current information set \\(Y_{t-1}\\). It provide the average mean prediction of Gold price which investors and financial institutions interested in. Moreover, 1 standard deviation and 2 standard deviation will also produced in forecasting process to provide a 68% and 95% of confidence intervals of future Gold price movements in \\(h\\) periods base on current information set.\nFurthermore, different prior distribution might be used to provide different level of uncertainty of current environment(information set). Compare the difference of Gold price under different priors could help to prove the Gold as a high quality safe-haven investment and increase investors and financial institutions confidence and further expectations. (Competitors for golds might also be used under different priors, such as Nasdaq Index and short to mid-term treasury bills.)"
  },
  {
    "objectID": "index.html#likelihood-function",
    "href": "index.html#likelihood-function",
    "title": "An Evidence-based Forecast: Gold as a Traditional Safe-Haven Investment",
    "section": "Likelihood Function",
    "text": "Likelihood Function\nThe model equation imply the predictive density of the data vector \\(Y\\). We could consider the model equation is the linear transformation of the matrix-variate normal distribution \\(E\\). Therefore, the data vector also follows a matrix-variate normal distribution given by:\n\\[\\begin{aligned}\n\nY|X,A,\\Sigma &\\sim \\mathcal{MN}_{T\\times N}(XA,\\Sigma,I_T)\n\n\\end{aligned}\\]\nThis distribution determines the shape of the likelihood function that is defined as the sampling data density:\n\\[\\begin{aligned}\n\nL(A,\\Sigma | Y,X)\\equiv P(Y|X,A,\\Sigma)\n\n\\end{aligned}\\]\nThe likelihood function for the parameters estimation (\\(A,\\Sigma\\)), and after plugging in data in place of \\(Y,X\\), is considered a function of parameters \\(A\\) and \\(\\Sigma\\) is given by:\n\\[\\begin{aligned}\n\nL(A,\\Sigma | Y,X) &= \\left( 2\\pi \\right)^{-\\frac{TN}{2}}det\\left( \\Sigma \\right)^{-\\frac{T}{2}}exp\\left\\{ -\\frac{1}{2}\\sum_{t=1}^{T}\\epsilon_{t}'\\Sigma^{-1}\\epsilon_{t} \\right\\}\n\\\\\n&= \\left( 2\\pi \\right)^{-\\frac{TN}{2}}det\\left( \\Sigma \\right)^{-\\frac{T}{2}}exp\\left\\{ -\\frac{1}{2}\\sum_{t=1}^{T} \\left( y_{t}-A'x_{t} \\right)'\\Sigma^{-1}\\left( y_{t}-A'x_{t} \\right)\\right\\}\n\\\\\n&= \\left( 2\\pi \\right)^{-\\frac{TN}{2}}det\\left( \\Sigma \\right)^{-\\frac{T}{2}}exp\\left\\{ -\\frac{1}{2}vec\\left(\\left( Y-XA \\right)'\\right)' \\left( I_T\\otimes \\Sigma^{-1} vec\\left(\\left( Y-XA \\right)'\\right)\\right)\\right\\}\n\\\\\n&= \\left( 2\\pi \\right)^{-\\frac{TN}{2}}det\\left( \\Sigma \\right)^{-\\frac{T}{2}}exp\\left\\{ -\\frac{1}{2}\\textbf{tr}\\left[\\Sigma^{-1}\\left( Y-XA \\right)'I^{-1}_T\\left(Y-XA \\right) \\right] \\right\\}\n\\\\\n&= \\left( 2\\pi \\right)^{-\\frac{TN}{2}}det\\left( \\Sigma \\right)^{-\\frac{T}{2}}exp\\left\\{ -\\frac{1}{2}\\textbf{tr}\\left[\\Sigma^{-1}\\left(Y-XA\\right)'\\left(Y-XA\\right) \\right] \\right\\}\n\n\\end{aligned}\\]\nGiven that the trace:\n\n\\(\\textbf{tr}\\left( X \\right) = \\sum_{n=1}^{N}X_{nn} \\ \\ \\ \\ \\text{for a }N \\times N\\text{ matrix }X\\)\n\\(\\textbf{tr}\\left( ABCD \\right) = vec(D')'\\left( C'\\otimes A \\right)vec(B)\\)"
  },
  {
    "objectID": "index.html#prior-distribution",
    "href": "index.html#prior-distribution",
    "title": "An Evidence-based Forecast: Gold as a Traditional Safe-Haven Investment",
    "section": "Prior Distribution",
    "text": "Prior Distribution\nFor the given model, we assume that unknown parameters have following distributions:\n\\[\\begin{aligned}\n\nA|\\Sigma &\\sim \\mathcal{MN}_{K\\times N}(M,\\Sigma,P)\n\\\\\n\\Sigma &\\sim \\mathcal{IW}_{N}(S,\\nu)\n\n\\end{aligned}\\]\nwhere \\(A_{K \\times N}|\\Sigma\\) follow a matrix-variate normal distribution:\n- \\(M\\) is the mean of matrix normal distribution\n- \\(\\Sigma_{N \\times N}\\)is the row specific covariance matrix\n- \\(P_{K \\times K}\\) is the column specific covariance matrix with the density given by:\n\\[\\begin{aligned}\n\n\\mathcal{MN}_{K\\times N}(M,\\Sigma,P) & =c^{-1}_{mn}exp\\left\\{ -\\frac{1}{2}\\textbf{tr}\\left[ \\Sigma^{-1}\\left( A-M \\right)'P^{-1}\\left( A-M \\right) \\right] \\right\\}\\\\\nc_{mn} & = \\left( 2\\pi \\right)^{\\frac{KN}{2}}det\\left( \\Sigma \\right)^{\\frac{K}{2}}det\\left( P \\right)^{\\frac{N}{2}}\n\n\\end{aligned}\\]\nAnd \\(\\Sigma\\) follow a Inverse Wishart distribution:\n- \\(S\\) is N × N positive definite symmetric matrix called the scale matrix.\n- \\(\\nu\\) &gt; N + 2 denotes degrees of freedom. with the density given by:\n\\[\\begin{aligned}\n\n\\mathcal{IW}_{N}(S,\\nu) &= c^{-1}_{iw}det(\\Sigma)^{-\\frac{\\nu + N+1}{2}}exp\\left\\{ -\\frac{1}{2} \\textbf{tr}\\left[ \\Sigma^{-1} S\\right]\\right\\}\\\\\nc_{iw} &= 2^{\\frac{\\nu N}{2}}\\pi^{\\frac{N(N-1)}{4}}\\prod_{n=1}^{N}\\Gamma\\left( \\frac{\\nu + n + 1}{2} \\right)det(S)^{-\\frac{\\nu}{2}}\n\n\\end{aligned}\\]\nThen the joint distribution of (\\(A,\\Sigma\\)) is Normal-Inverse Wishart:\n\\[\\begin{aligned}\n\nP(A,\\Sigma) & \\sim \\mathcal{NIW}_{K \\times N}(M,P,S,\\nu)\n\n\\end{aligned}\\]\nwith the density given by:\n\\[\\begin{aligned}\n\nP(A,\\Sigma) &= c^{-1}_{nw}det(\\Sigma)^{-\\frac{\\nu + N + K + 1}{2}}\\\\\n&\\times exp\\left\\{ -\\frac{1}{2}\\textbf{tr}\\left[ \\Sigma^{-1}\\left( A-M \\right)'P^{-1}\\left( A-M \\right) \\right] \\right\\}\\\\\n&\\times exp\\left\\{ -\\frac{1}{2} \\textbf{tr}\\left[ \\Sigma^{-1} S\\right]\\right\\}\n\\\\\n\\\\\nc_{nw} &= 2^{\\frac{N(K + \\nu)}{2}}\\pi^{\\frac{N(N + 2K -1)}{4}}[\\prod_{n=1}^{N}\\Gamma\\left( \\frac{\\nu + 1 - n}{2} \\right)]det\\left( P \\right)^{\\frac{N}{2}}det(S)^{-\\frac{\\nu}{2}}\n\n\\end{aligned}\\]\n\nNatural-conjugate prior distribution\nLeads to joint posterior distribution for (\\(A,\\Sigma\\)) has the same form as prior:\n\\[\\begin{aligned}\n\nP(A,\\Sigma) &= P(A|\\Sigma)P(\\Sigma)\n\\\\\nA|\\Sigma &\\sim \\mathcal{MN}_{K\\times N}(\\underline{A},\\Sigma,\\underline{V})\n\\\\\n\\Sigma &\\sim \\mathcal{IW}_{N}(\\underline{S}, \\underline{\\nu})\n\n\\end{aligned}\\]\nwith the kernel given by:\n\\[\\begin{aligned}\n\nP(A,\\Sigma) &\\propto \\det(\\Sigma)^{-\\frac{N+K+\\underline{\\nu}+1}{2}} \\\\\n&\\times exp\\{-\\frac{1}{2}tr[\\Sigma^{-1}(A-\\underline{A}) \\underline{V}^{-1}(A-\\underline{A})]\\} \\\\\n&\\times exp\\{-\\frac{1}{2}tr[\\Sigma^{-1}\\underline{S}]\\}\n\n\\end{aligned}\\]"
  },
  {
    "objectID": "index.html#benchmark-model-with-minnesota-prior",
    "href": "index.html#benchmark-model-with-minnesota-prior",
    "title": "An Evidence-based Forecast: Gold as a Traditional Safe-Haven Investment",
    "section": "Benchmark model with Minnesota prior",
    "text": "Benchmark model with Minnesota prior\nIn real life, macroeconomic variables are more likely being unit-root non stationary and are well-characterized by a multivariate random walk process：\n\\[\\begin{aligned}\n\ny_{t} = y_{t-1} + \\epsilon_t\n\n\\end{aligned}\\]\nTherefore, our benchmark model uses Minnesota prior(1984) based on random walk process for our Bayesian forecasting.\nSet the prior mean \\(A\\) to:\n\\[\\begin{aligned}\n\n\\underline{A}=\\left[ 0_{N\\times1} \\ \\ \\ \\  I_{N} \\ \\ \\ \\ \\ 0_{N\\times(p-1)N} \\right]'\n\n\\end{aligned}\\]\nwhich means the mean of first lag equal to 1 and mean of constant term and other lags are 0.\nSet the column specific prior covariance of \\(A\\) (prior shrinkage)to:\n\\[\\begin{aligned}\n\n\\underline{V} = diag\\left[ \\kappa_{2} \\quad \\kappa_{1}(\\textbf{P} ^{-2}\\otimes \\textbf{i}^{'}_{N}) \\right]\n\n\\end{aligned}\\]\nwhere:\n\n\\(\\textbf{P}\\) is the list of legs, \\(\\textbf{P} \\ \\ \\ =\\left[ 1 \\quad 2 \\quad 3\\quad ... \\quad p \\right]\\)\n\\(\\textbf{i}^{'}_{N}\\) is a \\(N \\times 1\\) vector of ones\n\\(\\kappa_{1}\\): overall shrinkage level for autoregressive slopes\n\\(\\kappa_{2}\\): overall shrinkage for the constant term\n\ntherefore, we could have the variance of \\(A\\):\n\\[\nVAR\\left[ vec\\left( A \\right) \\right]\\quad = \\quad \\Sigma \\ \\ \\otimes \\ \\ \\underline{V}\n\\]\n\nBayesian Estimations\nBased on Bayes’ Theorem:\n\\[\\begin{aligned}\n\nP(A|B) & =\\frac{P(B|A)P(A)}{P(B)}\n\n\\end{aligned}\\]\nWe could have the kernel of conditional joint posterior distribution \\(P(A,\\Sigma|Y,X)\\), which is the proportion of the product between conditional distribution of data \\(P(Y|X,A,\\Sigma)\\) and joint prior distribution \\(P(A,\\Sigma)\\):\n\\[\\begin{aligned}\n\nP(A,\\Sigma|Y,X) &= \\frac{P(Y|X,A,\\Sigma) \\ P(A,\\Sigma)}{P(Y)}\\\\\n&\\propto P(Y|X,A,\\Sigma) \\ P(A,\\Sigma)\\\\\n&\\propto L(A,\\Sigma | Y,X) \\ P(A|\\Sigma) \\ P(\\Sigma) \\\\\\\\\n\n\\end{aligned}\\]\nAnd the kernel of conditional joint posterior distribution is given by:\n\\[\\begin{aligned}\n\nP(A,\\Sigma|Y,X) &\\propto L(A,\\Sigma | Y,X) \\ P(A,\\Sigma)\\\\\n&\\propto det(\\Sigma)^{-\\frac{T}{2}} \\\\\n&\\times exp\\{-\\frac{1}{2}tr[\\Sigma^{-1}(Y-XA)'(Y-XA)]\\} \\\\\n&\\times \\det(\\Sigma)^{-\\frac{N+K+\\underline{\\nu}+1}{2}} \\\\\n&\\times exp\\{-\\frac{1}{2}tr[\\Sigma^{-1}(A-\\underline{A}) \\underline{V}^{-1}(A-\\underline{A})]\\} \\\\\n&\\times exp\\{-\\frac{1}{2}tr[\\Sigma^{-1}\\underline{S}]\\}\n\n\\end{aligned}\\]\nThen, the kernel could be represent as the normal-inverse Wishart distribution:\n\\[\\begin{aligned}\n\nP(A,\\Sigma|Y,X) &\\sim \\mathcal{NIW}_{K\\times N}(\\bar{A},\\bar{V},\\bar{S},\\bar{\\nu})\n\\\\\n\\\\\n\\bar{V} &= (X'X + \\underline{V}^{-1})^{-1} \\\\\n\\bar{A} &= \\bar{V}(X'Y + \\underline{V}^{-1}\\underline{A}) \\\\\n\\bar{\\nu} &= T + \\underline{\\nu}\\\\\n\\bar{S} &= \\underline{S} + Y'Y + \\underline{A}'\\underline{V}^{-1}\\underline{A} - \\bar{A}'\\bar{V}^{-1}\\bar{A}\n\n\\end{aligned}\\]\n\n\nGibbs sampler: Function Proofing\nConsider Bi-variate Gaussian random walk process:\n\\[\n\\begin{align}\ny_t &=\n\\begin{bmatrix}\ny_{t,1} \\\\\ny_{t,2}\n\\end{bmatrix} =\n\\begin{bmatrix}\ny_{t-1,1} \\\\\ny_{t-1,2}\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\epsilon_{t,1} \\\\\n\\epsilon_{t,2}\n\\end{bmatrix} \\\\\n\\epsilon_{t,1} &\\sim \\mathcal{N}(0,1)  \\\\\n\\epsilon_{t,2} &\\sim \\mathcal{N}(0,1)\n\\end{align}\n\\]\nVariables in Matrix Notation:\n\\[\nY = \\begin{bmatrix}\ny_2' \\\\\ny_3' \\\\\n\\vdots \\\\\ny_n'\n\\end{bmatrix},\n\\quad\nX = \\begin{bmatrix}\n1 \\quad y_1' \\\\\n1 \\quad y_2' \\\\\n\\vdots \\quad \\vdots \\\\\n1 \\quad y_{n-1}'\n\\end{bmatrix}\n\\]\nTherefore, we could basic set up for this Bi-variate Gaussian random walk process with Minnesota prior:\n\ne1 = cumsum(rnorm(1000, 0, sd=1))\ne2 = cumsum(rnorm(1000, 0, sd=1))\ne  = cbind(e1,e2)\n\n## Define data X, Y \nY = ts(e[2:nrow(e),], frequency=1)\nX = matrix(1,nrow(Y),1)\nX = cbind(X,e[2:nrow(e)-1,])\n\n\n## Test on basic model\nN           = ncol(Y)                          # N=2\np           = frequency(Y)\nA.hat       = solve(t(X)%*%X)%*%t(X)%*%Y\nSigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)\n\n# Prior distribution (with Minnesota prior)\nkappa.1             = 0.02^2                              # shrinkage for A1 to Ap\nkappa.2             = 200                                  # shrinkage for constant \nA.prior             = matrix(0,nrow(A.hat),ncol(A.hat))\nA.prior[2:(N + 1),] = diag(N)\nV.prior             = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))\nS.prior             = diag(diag(Sigma.hat))\nnu.prior            = N+2\nI.matrix            = diag(1,nrow(Y),nrow(Y))\n\n\n\n\n\n\nFunction below is posterior.draws:\n\n## Posterior sample draw function for basic model(posterior.draws)\nposterior.draws       = function (S, Y, X){\n  \n    # normal-inverse Wishard posterior parameters\n    V.bar.inv         = t(X)%*%X + diag(1/diag(V.prior))\n    V.bar             = solve(V.bar.inv)\n    A.bar             = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)\n    nu.bar            = nrow(Y) + nu.prior\n    S.bar             = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar\n    S.bar.inv         = solve(S.bar)\n    \n    # posterior draws \n    Sigma.posterior   = rWishart(1, df=nu.bar, Sigma=S.bar.inv)\n    Sigma.posterior   = apply(Sigma.posterior,3,solve)\n    Sigma.posterior   = array(Sigma.posterior,c(N,N,S))\n    A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))\n    L                 = t(chol(V.bar))\n    \n    for (s in 1:S){\n      A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])\n    }\n\n    output            = list(A.posterior=A.posterior, Sigma.posterior=Sigma.posterior)\n    return(output)\n}\n\n\nThe posterior mean of the \\(A\\) is:\n\n\n\n\n\n\n\nA\nSimulation Parameter Y1\nSimulation Parameter Y2\n\n\n\n\nConstant term\n0.0610\n0.0807\n\n\nY1 lag\n0.9893\n0.0048\n\n\nY2 lag\n0.0024\n0.9957\n\n\n\n\n\n\n\n\n\nThe posterior mean of the \\(\\Sigma\\) is:\n\n\n\n\n\n\n\nSigma\nSimulation Parameter Y1\nSimulation Parameter Y2\n\n\n\n\nY1 lag\n1.0052\n0.1171\n\n\nY2 lag\n0.1171\n0.9771"
  },
  {
    "objectID": "index.html#the-extended-model-laplace-distribution-of-error-term",
    "href": "index.html#the-extended-model-laplace-distribution-of-error-term",
    "title": "An Evidence-based Forecast: Gold as a Traditional Safe-Haven Investment",
    "section": "The extended model: Laplace distribution of error term",
    "text": "The extended model: Laplace distribution of error term\nThe Basic Model is the standard VARs model that assume the error terms \\(U\\) are independent and identically distributed(\\(iid\\)) as \\(U \\ \\sim N_{TN}(0 \\ , \\ \\Sigma)\\). In other formation, it could be presented as \\(vec(U) \\ \\sim N(0 \\ , \\ \\Sigma \\ \\otimes \\  I_{T})\\). Where \\(\\Sigma\\) is a \\(N\\times N\\) cross sectional covariance matrix, \\(I_{t}\\) is a \\(T\\times T\\) identity matrix present serial covariance, \\(\\otimes\\) is the Kronecker product and the operator \\(vec(.)\\) is vectorization that inverts the matrix into the column vector by stacking the columns.\nTherefore, we could consider a more general serial covariance structure:\n\\[\\begin{align}\n\nvec(U) \\ \\sim N(0 \\ , \\ \\Sigma \\ \\otimes \\  \\Omega)\\\\\n\n\\end{align}\\]\nWhere:\n\\[\\begin{align}\n\n\\Omega \\ &= \\ \\ diag\\left[ \\lambda_{1}, \\lambda_{2},...,\\lambda_{t}  \\right]\\\\\n\\lambda \\ &\\sim \\ Exp \\ (\\alpha)\\\\\n\n\\end{align}\\]\nAnd then, the distribution of error terms in extension model will be Laplace distribution instead of the normally distributed errors assumption. The Laplace distribution is well-suited for describing financial anomalies due to its sharp peaks and heavy tails. Utilizing this distribution enhances the model’s robustness against anomalies, making it particularly appropriate for financial time series analysis. Given that most of our variables are financial time series data, applying a Laplace distribution to the error term is more appropriate.\nFollowing Eltoft,Kim, and Lee 2006b, for covariance with a general Kronecker structure, if each \\({\\lambda}\\) has an independent exponential distribution with mean \\({\\alpha}\\), then marginally \\({U_t}\\) has a multivariate Laplace distribution with mean vector 0 and covariance matrix \\({\\alpha\\Sigma}\\).\n\\[\\begin{align}\n\nU &\\sim \\ Laplace(0 \\ , \\  \\alpha\\Sigma) \\\\\nU_t|\\lambda_t &\\sim \\mathcal{MN}(0 \\ , \\  \\Sigma \\ , \\ \\Omega) \\\\\n\\Omega \\ &= \\ \\ diag\\left[ \\lambda_{1}, \\lambda_{2},...,\\lambda_{t}  \\right] \\ = \\ \\lambda_t \\times \\textbf{I}_T\\\\\n\\lambda_t &\\sim \\ Exp(\\alpha)\n\n\\end{align}\\]\nTherefore, the prior distribution of lambda which is following exponential distribution defined as:\n\\[\\begin{align}\n\nP(\\lambda_t|\\alpha) &= \\frac{1}{\\alpha}exp\\{ -\\frac{1}{\\alpha}\\lambda_t \\}\\\\\n\n\\end{align} \\]\n\n\nFollowing graphs describe the difference between alpha values, we could see that as the mean, alpha, increase. The rates of exponetial function decrease.\n\n\n\n\n\n\n\n\nBayesian Estimations\nThen, the kernel of the likelihood function could be rewritten as to:\n\\[\\begin{align}\n\nL(A,\\Sigma,\\Omega |Y,X) &\\propto \\det(\\Sigma)^{-\\frac{T}{2}} \\det(\\Omega)^{-\\frac{N}{2}} exp\\{-\\frac{1}{2}\\textbf{tr}[\\Sigma^{-1} (Y-XA)' \\Omega^{-1} (Y-XA) ]\\}\\\\\n&= \\det(\\Sigma)^{-\\frac{T}{2}} \\det(\\left[ \\lambda_{1}, \\lambda_{2},...,\\lambda_{t} \\right])^{-\\frac{N}{2}} exp\\{-\\frac{1}{2} \\textbf{tr}[\\Sigma^{-1} (Y-XA)' (\\left[ \\lambda_{1}, \\lambda_{2},...,\\lambda_{t} \\right])^{-1} (Y-XA) ]\\}\\\\\n&=\\det(\\Sigma)^{-\\frac{T}{2}}(\\prod^{T}_{t = 1} \\lambda_t)^{-\\frac{N}{2}} exp\\left\\{{-\\frac{1}{2}}\\sum^{T}_{t =1}{\\frac{1}{\\lambda_t}} \\textbf{tr}[(Y_t-X_tA)' \\Sigma^{-1}(Y_t-X_tA)]\\right\\}\\\\\n&=\\det(\\Sigma)^{-\\frac{T}{2}}\\prod^{T}_{t = 1} \\lambda_t^{-\\frac{N}{2}} exp\\left\\{{-\\frac{1}{2}}\\sum^{T}_{t =1}{\\frac{1}{\\lambda_t}} \\textbf{tr}[\\epsilon_t' \\Sigma^{-1}\\epsilon_t)]\\right\\}\\\\\n&=\\det(\\Sigma)^{-\\frac{T}{2}}\\prod^{T}_{t = 1}(\\lambda_t^{-\\frac{N}{2}} exp\\left\\{{-\\frac{1}{2}}{\\frac{1}{\\lambda_t}} \\textbf{tr}[\\epsilon_t' \\Sigma^{-1}\\epsilon_t])\\right\\})\n\n\\end{align}\\]\nTherefore at each time t, we have the likelihood function be a proportion of lambda:\n\\[\\begin{align}\n\n\\propto\\det(\\Sigma)^{-\\frac{T}{2}}\\lambda_t^{-\\frac{N}{2}} exp\\left\\{{-\\frac{1}{2}}{\\frac{1}{\\lambda_t}} \\textbf{tr}[\\epsilon_t' \\Sigma^{-1}\\epsilon_t])\\right\\}\n\n\\end{align}\\]\nFor joint posteriors distribution, \\(A\\), \\(\\Sigma\\) can then be derived using the likelihood and the prior distributions as follows:\n\\[\\begin{align}\n\nP(A,\\Sigma|Y,X) &\\propto L(A,\\Sigma,\\Omega|Y,X) \\ P(A,\\Sigma) \\\\\n\\\\\n&= \\det(\\Sigma)^{-\\frac{T}{2}} \\det(\\Omega)^{-\\frac{N}{2}} exp\\{-\\frac{1}{2} \\textbf{tr}[\\Sigma^{-1} (Y-XA)' \\Omega^{-1} (Y-XA) ]\\} \\\\\n&\\times \\det(\\Sigma)^{-\\frac{N+k+\\underline{\\nu}+1}{2}} exp\\{-\\frac{1}{2}\\textbf{tr}[\\Sigma^{-1}(A-\\underline{A})'(\\underline{V})^{-1}(A-\\underline{A})]\\} \\\\\n&\\times exp\\{-\\frac{1}{2}\\textbf{tr}[\\Sigma^{-1}\\underline{S}]\\} \\\\\n\n&= \\det(\\Sigma)^{-\\frac{T+N+K+\\underline{\\nu}+1}{2}} \\det(\\Omega)^{-\\frac{N}{2}} \\\\\n&\\times exp\\{-\\frac{1}{2} \\textbf{tr}[\\Sigma^{-1}(Y'\\Omega^{-1}Y - 2A'X'\\Omega^{-1}Y + A'X'\\Omega^{-1}XA \\\\\n&+ A'\\underline{V}^{-1}A -2A'\\underline{V}^{-1}\\underline{A} + \\underline{A}'\\underline{V}^{-1}\\underline{A} + \\underline{S})]\\}\n\n\\end{align}\\]\nThe kernel also could be rearranged in the form of the Normal-inverse Wishart distribution and given by:\n\\[\\begin{align}\n\nP(A,\\Sigma|Y,X,\\Omega) &\\sim \\mathcal{NIW}(\\bar{A},\\bar{V},\\bar{S},\\bar{\\nu}) \\\\\n&\\\\\n\\bar{V} &= (X'\\Omega^{-1}X + \\underline{V}^{-1})^{-1} \\\\\n\\bar{A} &= \\bar{V}(X'\\Omega^{-1}Y + \\underline{V}^{-1}\\underline{A}) \\\\\n\\bar{\\nu} &= T + \\underline{\\nu}\\\\\n\\bar{S} &= \\underline{S} + Y'\\Omega^{-1}Y + \\underline{A}'\\underline{V}^{-1}\\underline{A} - \\bar{A}'\\bar{V}^{-1}\\bar{A}\n\n\\end{align}\\]\nThe kernel of the fully conditional posterior distribution of \\(\\lambda_t\\) is then derived as follows:\n\\[\\begin{align}\n\nP(\\lambda_t|Y,X,A,\\Sigma) &\\propto L(A,\\Sigma,\\lambda_t|Y,X)P(\\lambda_t) \\\\\n\\\\\n&\\propto \\lambda_t^{-\\frac{N}{2}}exp({-\\frac{1}{2}}{\\frac{1}{\\lambda_t}}\\textbf{tr}[\\epsilon_t' \\Sigma^{-1}\\epsilon_t]) \\\\\n&\\times \\frac{1}{\\alpha}exp\\{ -\\frac{1}{\\alpha}\\lambda_t \\}\\\\\n&\\propto \\lambda_t^{-\\frac{N}{2}+1-1} exp\\{-\\frac{1}{2}[\\frac{\\textbf{tr}[\n\\epsilon_t' \\Sigma^{-1}\\epsilon_t]} {\\lambda_t} +\\frac{2}{\\alpha}\\lambda_t]\\}\n\n\\end{align}\\]\nThe above expression can be rearranged in the form of a Generalized inverse Gaussian distribution kernel as follows:\n\\[\\begin{align}\n\n\\lambda_t|Y,A,\\Sigma &\\sim \\mathcal{GIG}(a,b,p) \\\\\n\\\\\na &=\\frac{2}{\\alpha} \\\\\nb &= \\textbf{tr}[\\epsilon_t' \\Sigma^{-1}\\epsilon_t] \\\\\np &= -\\frac{N}{2}+1\n\n\\end{align}\\]\n\n\nGibbs Sampler: Function proving\nThe Gibbs sampler method will be applied to generate random draws from the full conditional posterior distribution:\n\nDraw \\(\\Sigma^{(s)}\\) from the \\(IW(\\bar{S},\\bar{\\nu})\\) distribution.\nDraw \\(A^{(s)}\\) from the \\(MN(\\bar{A},\\Sigma^{(s)}, \\bar{V})\\) distribution.\nDraw \\(\\lambda_t^{(s)}\\) from \\(GIG(a,b,p)\\).\n\nRepeat steps 1, step 2 and 3 for \\(S_1\\)+\\(S_2\\)times.\nDiscard the first draws that allowed the algorithm to converge to the stationary posterior distribution.\nOutput is \\(\\left\\{ {A^{(s)}, \\Sigma^{(s)}}, \\lambda_t^{(s)}\\right\\}^{S_1+S_2}_{s=S_1+1}\\).\nFunction below is posterior.draws.extended:\n\n\n\nPosterior mean of the autoregressive coefficient matrix A\n\n\n\nSimulation_Y1\nSimulation_Y2\n\n\n\n\nConstant\n0.0536249\n0.1065899\n\n\nY1-Lag\n0.9909827\n0.0086799\n\n\nY2-Lag\n0.0025720\n0.9939169\n\n\n\n\n\n\n\n\nPosterior mean of the covariance matrix Sigma\n\n\n\nSimulation_Y1\nSimulation_Y2\n\n\n\n\nY1-Lag\n0.6165036\n0.0538747\n\n\nY2-Lag\n0.0538747\n0.6587449"
  },
  {
    "objectID": "index.html#basic-model",
    "href": "index.html#basic-model",
    "title": "An Evidence-based Forecast: Gold as a Traditional Safe-Haven Investment",
    "section": "Basic Model",
    "text": "Basic Model\nFigure 6 presents a 3D visualization of the density intervals for the log Gold Future Prices and Log Nasdaq Index points. From past trends we could clearly see that negative correlations between those two. When the market suffered from dot-com and global financial crisis during 2000 to 2012, Gold futures increase sharply as expectation of risks, market returns fluctuations a lot. For next 24 months forecasting based on benchmark model, we could see Log gold future price more sharply increase then Log Nasdaq Index points. The varying heights of the intervals reflect the level of prediction certainty; as we project further into the future, the intervals become wider and more dispersed due to increased uncertainty.\n\n\n\n\n\nFigure 6 3D forecasting graph on basic model\n\n\n\n\n\n\n\n\nFigure 7 Basic Model Key Data Plot\n\n\n\n\n\n\nExtension Model"
  },
  {
    "objectID": "index.html#stochastic-volatility-heteroskedasticity",
    "href": "index.html#stochastic-volatility-heteroskedasticity",
    "title": "An Evidence-based Forecast: Gold as a Traditional Safe-Haven Investment",
    "section": "Stochastic Volatility Heteroskedasticity",
    "text": "Stochastic Volatility Heteroskedasticity\nTherefore, for the same general serial covariance structure:\n\\[\\begin{align}\n\nvec(U) \\ \\sim N(0 \\ , \\ \\Sigma \\ \\otimes \\  \\Omega)\\\\\n\n\\end{align}\\]\nHowever, this time we have \\(h_t\\) follows a simple stochastic volatility process:\n\\[\\begin{align}\n\n\\Omega \\ &= diag\\left[ \\sigma^2_1,..., \\sigma^2_T\\right]\\\\\n&=\\ diag\\left[ exp\\left\\{ h_1 \\right\\},exp\\left\\{ h_2 \\right\\},...,exp\\left\\{ h_T \\right\\}  \\right]\\\\\nh_t &= h_{t-1}+v_t\\\\\n\n\\end{align}\\]\nThen, the kernel of the likelihood function could be rewritten as to:\n\\[\\begin{align}\n\nL(A,\\Sigma,\\Omega |Y,X) &\\propto \\det(\\Sigma)^{-\\frac{T}{2}} \\det(\\Omega)^{-\\frac{N}{2}} exp\\{-\\frac{1}{2}\\textbf{tr}[\\Sigma^{-1} (Y-XA)' \\Omega^{-1} (Y-XA) ]\\}\\\\\n&=\\det(\\Sigma)^{-\\frac{T}{2}}\\left( \\prod^{T}_{t = 1} exp\\left\\{ h_t \\right\\} \\right)^{-\\frac{N}{2}} exp\\left\\{{-\\frac{1}{2}}\\sum^{T}_{t =1}exp\\left\\{ h_t \\right\\}^{-1} \\textbf{tr}[\\epsilon_t' \\Sigma^{-1}\\epsilon_t)]\\right\\}\\\\\n&=\\det(\\Sigma)^{-\\frac{T}{2}}exp\\left\\{{-\\frac{N}{2}}\\sum^{T}_{t =1}h_t{-\\frac{1}{2}}\\sum^{T}_{t =1}exp\\left\\{ h_t \\right\\}^{-1} \\textbf{tr}[\\epsilon_t' \\Sigma^{-1}\\epsilon_t)]\\right\\}\\\\\n\n\\end{align}\\]\n\nMatrix Notation for Stochastic Volatility model\nRecall the matrix notation:\n\\[\\begin{align}\n\nY&=XA \\  +  \\ E\\\\\nE|X &\\sim MN(0,\\Sigma,\\Omega) \\\\\n\\Omega \\ &=\\ diag\\left[ exp\\left\\{ h_1 \\right\\},exp\\left\\{ h_2 \\right\\},...,exp\\left\\{h_T \\right\\}  \\right]\\\\\nh_t &= h_{t-1}+v_t\\\\\n\n\\end{align}\\]\nGiven that:\n\\[\\begin{align}\n\n\\epsilon_t &\\sim N(0,1)\\\\\nv_t&\\sim N(0,1)\\\\\n\n\\end{align}\\]\nWe could rewrite the equation as:\n\\[\\begin{align}\n\ny_t &= y_{t-1}A_1 \\ \\ + ...+y_{t-p}A_p+\\ \\ exp\\left\\{ \\frac{1}{2} h_t\\right\\}\\epsilon_t\\\\\ny_t - y_{t-1}A_1 \\ \\ + ...+y_{t-p}A_p &= exp\\left\\{ \\frac{1}{2} h_t\\right\\}\\epsilon_t\\\\\ny_{u.t}&=exp\\left\\{ \\frac{1}{2} h_t\\right\\}\\epsilon_t\\\\\n\n\\end{align}\\]\nThen, taking the square and the logarithm of both sides of the equation, we could have:\n\\[\\begin{align}\n\ny_{u.t}&=exp\\left\\{ \\frac{1}{2} h_t\\right\\}\\epsilon_t\\\\\nlog \\ y_{u.t}^2&=h_t + log \\ \\epsilon_t^2\\\\\n\\tilde{y}_t&=h_t+\\tilde{\\epsilon}_t\n\n\\end{align}\\]\nAnd then:\n\\[\\begin{align}\n\n\\tilde{\\epsilon}_t&\\sim \\log \\chi^2_1\n\n\\end{align}\\]\nDefine the following \\(T × 1\\) matrices:\n\\[\\begin{aligned}\n\n\\tilde{y}=\n\\begin{bmatrix}\n\\tilde{y}_1'\\\\ \\tilde{y}_2'\n\\\\.\\\\.\\\\.\\\\\\tilde{y}'_T\n\\end{bmatrix}_{T \\times 1}\n\nh=\n\\begin{bmatrix}\nh_{1} \\\\ h_{2} \\\\\n.\\\\.\\\\.\\\\h_{T}\n\\end{bmatrix}_{T \\times 1}\n\n\\tilde{\\epsilon}=\n\\begin{bmatrix}\n\\tilde{\\epsilon}_1'\\\\ \\tilde{\\epsilon}_2' \\\\\n.\\\\.\\\\.\\\\\\tilde{\\epsilon}_T'\n\\end{bmatrix}_{T \\times 1}\n\nv=\n\\begin{bmatrix}\nv_{1} \\\\ v_{2} \\\\\n.\\\\.\\\\.\\\\v_{T}\n\\end{bmatrix}_{T \\times 1}\n\ne_{1.T}=\n\\begin{bmatrix}\n1 \\\\ 0 \\\\\n.\\\\.\\\\.\\\\0\n\\end{bmatrix}_{T \\times 1}\n\n\\end{aligned}\\]\nAnd a \\(T × T\\) matrix, H:\n\\[\\begin{align}\n\nH = \\begin{pmatrix}\n1 &  &  &  &  & \\\\\n-1 & 1 &  &  &  & \\\\\n0 & -1 & 1 &  &  & \\\\\n\\vdots & \\ddots & \\ddots & \\ddots &  & \\\\\n0 & \\cdots & 0 & -1 & 1 \\\\\n\\end{pmatrix}_{T \\times T}\n\n\\end{align}\\]\nTherefore, we could have the simple matrix notation for Stochastic Volatility model:\n\\[\\begin{align}\n\n\\tilde{y}=h+\\tilde{\\varepsilon}\\\\\nHh=h_0e_{1.T}+\\sigma_vv\\\\\n\\tilde{\\epsilon}\\sim \\log \\chi^2_1\\\\\nv\\sim\\mathcal{N}_T(0_T,I_T)\\\\\n\n\\end{align}\\]\nAnd approximate the \\(\\log \\chi^2_1\\) distribution by a mixture of ten normal distributions given by:\n\\[\\begin{align}\n\n\\log \\chi^2_1\\approx \\sum_{m=1}^{10}Pr(s_t = m)\\mathcal{N}(\\mu_m,\\sigma^2_m)\n\n\\end{align}\\]\nwhere:\n\ns_t ∈ {1, . . . , 10} is a discrete-valued random indicator of the mixture component\n\\(\\mu_m,\\sigma^2_m\\) ,Pr(st = m) are predetermined\n\nTherefore, we could rewrite \\(\\tilde{\\epsilon}\\) in a Normal distribution:\n\\[\\begin{align}\n\n\\tilde{\\epsilon}|s \\sim\\mathcal{N}(\\mu_s,diag(\\sigma^2_s))\n\n\\end{align}\\]\n\n\nPriors distribution\nHierarchical prior structure is given by:\n\\[\\begin{align}\n\nP(h,s,h_0,\\sigma^2_v)=P(h|h_0,\\sigma^2_v)P(h_0)P(\\sigma^2_v)P(s)\n\n\\end{align}\\]\nTherefore, conditional prior distribution for \\(h\\) is:\n\\[\\begin{align}\n\nP(h|h_0,\\sigma^2_v)\\sim\\mathcal{N}(h_0H^{-1}e_{1.T},\\sigma^2_v(H'H)^{-1})\n\n\\end{align}\\]\nwith kernel given by:\n\\[\\begin{align}\n\n\\propto det(\\sigma^2_v\\textbf{I}_T)^{-\\frac{1}{2}}exp\\left( -\\frac{1}{2}\\sigma^{-2}_v\\left( Hh-h_0e_{1.T} \\right)'\\left( Hh-h_0e_{1.T} \\right) \\right)\n\n\\end{align}\\]\nPrior distribution for \\(h_0\\) is:\n\\[\\begin{align}\n\nP(h_0)\\sim\\mathcal{N}(0,\\underline{\\sigma}^2_h)\n\n\\end{align}\\]\nwith kernel given by:\n\\[\\begin{align}\n\n\\propto \\left( \\underline{\\sigma}^2_h \\right)^{-\\frac{1}{2}}exp\\left( -\\frac{1}{2}\\underline{\\sigma}^{-2}_hh_0h_0 \\right)\n\n\\end{align}\\]\nPrior distribution for \\(\\sigma^2_v\\) is:\n\\[\\begin{align}\n\nP(\\sigma^2_v)\\sim\\mathcal{IG2}(\\underline{s},\\underline{\\nu})\n\n\\end{align}\\]\nwith kernel given by:\n\\[\\begin{align}\n\n\\propto \\left( \\underline{\\sigma}^2_v \\right)^{-\\frac{\\underline{\\nu}+2}{2}}exp\\left( -\\frac{1}{2}\\frac{\\underline{s}}{\\underline{\\sigma}^{2}_v} \\right)\n\n\\end{align}\\]\nPrior distribution for \\(s\\) is:\n\\[\\begin{align}\n\nP(s_t)\\sim\\mathcal{Multinomial}(\\mathrm{\\left\\{ m \\right\\}}_{m=1}^{10},\\mathrm{\\left\\{Pr(s_t=m) \\right\\}}_{m=1}^{10})\n\n\\end{align}\\]\n\n\nBayesian Estimations\nFor joint posteriors distribution, \\(A\\), \\(\\Sigma\\) can then be derived using the likelihood and the prior distributions as follows:\n\\[\\begin{align}\n\nP(A,\\Sigma|Y,X,\\Omega) &\\propto L(A,\\Sigma,\\Omega|Y,X) \\ P(A,\\Sigma) \\\\\n\\\\\n&= \\det(\\Sigma)^{-\\frac{T}{2}} \\det(\\Omega)^{-\\frac{N}{2}} exp\\{-\\frac{1}{2} \\textbf{tr}[\\Sigma^{-1} (Y-XA)' \\Omega^{-1} (Y-XA) ]\\} \\\\\n&\\times \\det(\\Sigma)^{-\\frac{N+k+\\underline{\\nu}+1}{2}} exp\\{-\\frac{1}{2}\\textbf{tr}[\\Sigma^{-1}(A-\\underline{A})'(\\underline{V})^{-1}(A-\\underline{A})]\\} \\\\\n&\\times exp\\{-\\frac{1}{2}\\textbf{tr}[\\Sigma^{-1}\\underline{S}]\\} \\\\\n\n&= \\det(\\Sigma)^{-\\frac{T+N+K+\\underline{\\nu}+1}{2}} \\det(\\Omega)^{-\\frac{N}{2}} \\\\\n&\\times exp\\{-\\frac{1}{2} \\textbf{tr}[\\Sigma^{-1}(Y'\\Omega^{-1}Y - 2A'X'\\Omega^{-1}Y + A'X'\\Omega^{-1}XA \\\\\n&+ A'\\underline{V}^{-1}A -2A'\\underline{V}^{-1}\\underline{A} + \\underline{A}'\\underline{V}^{-1}\\underline{A} + \\underline{S})]\\}\n\n\\end{align}\\]\nThen, we have same kernel presentation as Laplace Distribution and could be rearranged in the form of the Normal-inverse Wishart distribution and given by:\n\\[\\begin{align}\n\nP(A,\\Sigma|Y,X,\\Omega) &\\sim \\mathcal{NIW}(\\bar{A},\\bar{V},\\bar{S},\\bar{\\nu}) \\\\\n&\\\\\n\\bar{V} &= (X'\\Omega^{-1}X + \\underline{V}^{-1})^{-1} \\\\\n\\bar{A} &= \\bar{V}(X'\\Omega^{-1}Y + \\underline{V}^{-1}\\underline{A}) \\\\\n\\bar{\\nu} &= T + \\underline{\\nu}\\\\\n\\bar{S} &= \\underline{S} + Y'\\Omega^{-1}Y + \\underline{A}'\\underline{V}^{-1}\\underline{A} - \\bar{A}'\\bar{V}^{-1}\\bar{A}\n\n\\end{align}\\]\nThe kernel of the fully conditional posterior distribution of \\(h\\) is then derived as follows:\n\\[\\begin{align}\n\nP(h|h_0,\\sigma^2_v,s,\\tilde{y}) &\\propto L(h,h_0,\\sigma^2_v,s|\\tilde{y})P(h) \\\\\n&\\propto exp\\left( -\\frac{1}{2}\\left( h-(\\tilde{y}-\\mu_s) \\right)'diag\\left(\\sigma^2_s  \\right)^{-1}\\left( h-(\\tilde{y}-\\mu_s) \\right) \\right)\\\\\n&\\times exp\\left( -\\frac{1}{2}\\sigma^{-2}_v\\left( Hh-h_0e_{1.T} \\right)'\\left( Hh-h_0e_{1.T} \\right) \\right)\\\\\n\n\\end{align}\\]\nThe above expression can be rearranged in the form of a Normal distribution kernel as follows:\n\\[\\begin{align}\n\nP(h|h_0,\\sigma^2_v,s,\\tilde{y}) &\\sim \\mathcal{N}(\\overline{h},\\overline{V}_h)\\\\\n\\overline{V}_h&=\\left[ diag\\left(\\sigma^2_s \\right)^{-1}+ \\sigma^{-2}_vH'H\\right]^{-1}\\\\\n\\overline{h}&=\\overline{V}_h\\left[ diag\\left(\\sigma^2_s \\right)^{-1}(\\tilde{y}-\\mu_s)+ \\sigma^{-2}_vh_0e_{1.T}\\right]\\\\\n\n\\end{align}\\]\nThe kernel of the fully conditional posterior distribution of \\(h_0\\) is then derived as follows:\n\\[\\begin{align}\n\nP(h_0|h,\\sigma^2_v,s,\\tilde{y}) &\\propto L(h_0,\\sigma^2_v|h)P(h_0) \\\\\n&\\propto exp\\left( -\\frac{1}{2}\\sigma^{-2}_v\\left( Hh-h_0e_{1.T} \\right)'\\left( Hh-h_0e_{1.T} \\right) \\right)\\\\\n&\\times exp\\left( -\\frac{1}{2}\\underline{\\sigma}^{-2}_hh_0h_0 \\right)\n\n\\end{align}\\]\nThe above expression can be rearranged in the form of a Normal distribution kernel as follows:\n\\[\\begin{align}\n\nP(h_0|h,\\sigma^2_v,s,\\tilde{y}) &\\sim \\mathcal{N}(\\overline{h}_0,\\overline{\\sigma}^2_h)\\\\\n\\overline{\\sigma}^2_h&=\\left(\\underline{\\sigma}^{-2}_h+ \\sigma^{-2}_v\\right)^{-1}\\\\\n\\overline{h}_0&=\\overline{\\sigma}^2_h\\left(\\sigma^{-2}_ve_{1.T}'Hh\\right)^{-1}\\\\\n\n\\end{align}\\]\nThe kernel of the fully conditional posterior distribution of \\(\\sigma^2_v\\) is then derived as follows:\n\\[\\begin{align}\n\nP(\\sigma^2_v|h,h_0,s,\\tilde{y}) &\\propto L(h_0,\\sigma^2_v|h)P(\\sigma^2_v) \\\\\n&\\propto exp\\left( -\\frac{1}{2}\\sigma^{-2}_v\\left( Hh-h_0e_{1.T} \\right)'\\left( Hh-h_0e_{1.T} \\right) \\right)\\\\\n&\\times \\left( \\underline{\\sigma}^2_v \\right)^{-\\frac{\\underline{\\nu}+2}{2}}exp\\left( -\\frac{1}{2}\\frac{\\underline{s}}{\\underline{\\sigma}^{2}_v} \\right)\n\n\\end{align}\\]\nThe above expression can be rearranged in the form of a Inverse-Gamma 2 distribution kernel as follows:\n\\[\\begin{align}\n\nP(\\sigma^2_v|h,h_0,s,\\tilde{y}) &\\sim \\mathcal{IG2}(\\overline{s},\\overline{\\nu})\\\\\n\\overline{\\nu}&=\\underline{\\nu}+ T\\\\\n\\overline{s}&=\\underline{s}+\\left( Hh-h_0e_{1.T} \\right)'\\left( Hh-h_0e_{1.T} \\right)\\\\\n\n\\end{align}\\]\nThe fully conditional posterior distribution of \\(s\\) is a multinomial distribution with the probabilities proportional to:\n\\[\\begin{align}\n\n\\omega_{m.t} = Pr[s_t=m]p(\\tilde{y}|h_t,s_t=m),\\text{for} \\ m=1,...,10\n\n\\end{align}\\]\nFor each \\(t\\) and \\(m\\) obtain \\(\\omega_{m.t}\\) using parallel computations and compute the probabilities of the multinomial full conditional posterior distribution by:\n\\[\\begin{align}\n\nPr[s_t=m|\\tilde{y},h_t]=\\frac{\\omega_{m.t}}{\\sum_{i=1}^{10}\\omega_{m.t}}\n\n\\end{align}\\]\n\n\nGibbs Sampler\nConsidering draws from this posterior involves a Gibbs sampler, which follows the following algorithm:\n\nInitialize \\(h^{(0)}\\), \\(s^{(0)}\\), and \\(\\sigma^{2(0)}_v\\)\nDraw \\(h_0^{(s)}\\sim\\mathcal{N}(\\bar{h}_0,\\bar{\\sigma}^2_h)\\)\nDraw \\(\\sigma_v^{2(s)}\\sim\\mathcal{IG2}(\\bar{s},\\bar{\\nu})\\)\nDraw \\(s_t^{(s)}\\sim\\mathcal{Multiomial}(\\{m\\}^{10}_{m=1},\\{Pr[s_t=m|\\tilde{y},h_t^{(s)}]\\}^{10}_{m=1})\\) for all \\(s_t\\) in \\(s\\).\nDraw \\(h^{(s)}\\sim\\mathcal{N}_T(\\bar{h},\\bar{V}_h)\\)\nDraw \\((A,\\Sigma)\\sim\\mathcal{MNIW}(\\bar{A},\\bar{V},\\bar{S},\\bar{\\nu})\\)\n\nSteps 2 to 7 are repeated for \\(S=S_1+S_2\\) draws, where \\(S_1\\) draws are discarded as burn-in and the latter \\(S_2\\) draws are kept as posterior draws."
  },
  {
    "objectID": "index.html#combinations-of-extension",
    "href": "index.html#combinations-of-extension",
    "title": "An Evidence-based Forecast: Gold as a Traditional Safe-Haven Investment",
    "section": "Combinations of Extension",
    "text": "Combinations of Extension"
  },
  {
    "objectID": "未命名.html",
    "href": "未命名.html",
    "title": "Untitled",
    "section": "",
    "text": "library(readabs)\n\nEnvironment variable 'R_READABS_PATH' is unset. Downloaded files will be saved in a temporary directory.\nYou can set 'R_READABS_PATH' at any time. To set it for the rest of this session, use\n    Sys.setenv(R_READABS_PATH = &lt;path&gt;)\n\nlibrary(fredr)\nlibrary(blscrapeR)\nlibrary(readrba)\nlibrary(xts)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nlibrary(fUnitRoots)   # ADF test - adfTest\nlibrary(tidyverse)    # for table\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::first()  masks xts::first()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::last()   masks xts::last()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(kableExtra)   # for print table\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(dplyr)\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\nlibrary(tseries) # for adf test\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\nlibrary(GIGrvg)\nlibrary(zoo)\nlibrary(ggplot2)      # plot in same graph\nlibrary(GIGrvg)       # GIG distribution\nlibrary(mvtnorm)      # Bvars forecast\nlibrary(plot3D)\nlibrary(HDInterval)   # hdi plot\nlibrary(grDevices)\nset.seed(123)#set random seed for test\n\n\ne1 = cumsum(rnorm(1000, 0, sd=1))\ne2 = cumsum(rnorm(1000, 0, sd=1))\ne  = cbind(e1,e2)\n\n## Define data X, Y \nY = ts(e[2:nrow(e),], frequency=1)\nX = matrix(1,nrow(Y),1)\nX = cbind(X,e[2:nrow(e)-1,])\n\n\n## Test on basic model\nN           = ncol(Y)                          # N=2\np           = frequency(Y)\nA.hat       = solve(t(X)%*%X)%*%t(X)%*%Y\nSigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)\n\nT &lt;- nrow(Y)\nK = 1 + (p*N)\n\n# Prior distribution (with Minnesota prior)\nkappa.1             = 1                                    # shrinkage for A1 to Ap\nkappa.2             = 100                                  # shrinkage for constant \nA.prior             = matrix(0,nrow(A.hat),ncol(A.hat))\nA.prior[2:(N + 1),] = diag(N)\nV.prior             = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))\nS.prior             = diag(diag(Sigma.hat))\nnu.prior            = N+2\nI.matrix            = diag(1,nrow(Y),nrow(Y))\n\nalpha &lt;- 2\nlambda.0 &lt;- rexp(T, rate = 1/alpha)\nlambda.priors = list(alpha = 2)\n\n\n# setup \nS1                = 500                            # determine the burn-in draws\nS2                = 9500                           # number of draws from the final simulation\ntotal_S           = S1+S2\nA.posterior       = array(NA, dim = c((1+N*p),N,S1+S2))\nSigma.posterior   = array(NA, dim = c(N,N,S1+S2))\nlambda.posterior  = matrix(NA, S1+S2, 1)\n\n\nposterior.draws.extended &lt;- function(total_S,Y, X){\n  S=total_S\n  # Initialize arrays to store posterior draws\n  Sigma.posterior.draws = array(NA, c(N,N,S))\n  A.posterior.draws = array(NA, c((1+p*N),N,S))\n  \n  lambda.posterior.draws = array(NA,c(T,S+1))\n  b = array(NA,c(T,S))\n  \n  #lambda.posterior.draws &lt;- array(NA,c(T,S+1))\n   for (s in 1:S){\n    \n    if (s == 1) {\n      lambda.s = lambda.0\n    } else {\n      lambda.s    = lambda.posterior.draws[,s]\n    }\n    \n    Omega = (diag(lambda.s))\n    Omega.inv = diag(1/lambda.s)\n    # Omega.inv = sqrt(diag(1/lambda.s))\n    # Omega.inv = sqrt(Omega.inv)\n    \n    V.bar.inv.ext   = t(X)%*%Omega.inv%*%X + solve(V.prior)\n    V.bar.ext       = solve(V.bar.inv.ext)\n    A.bar.ext       = V.bar.ext%*%(t(X)%*%Omega.inv%*%Y + diag(1/diag(V.prior))%*%A.prior)\n    nu.bar.ext      = T + nu.prior\n    S.bar.ext       = S.prior + t(Y)%*%Omega.inv%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar.ext)%*%V.bar.inv.ext%*%A.bar.ext\n    S.bar.ext.inv   = solve(S.bar.ext)\n    \n    \n    Sigma.inv.draw = rWishart(1, df = nu.bar.ext, Sigma = S.bar.ext.inv)[,,1]\n    Sigma.posterior.draws[,,s] = solve(Sigma.inv.draw)\n    A.posterior.draws[,,s] = matrix(mvtnorm::rmvnorm(1, mean=as.vector(A.bar.ext), sigma = Sigma.posterior.draws[,,s] %x% V.bar.ext), ncol=N)\n    \n    \n    u.t = Y-X%*%A.posterior.draws[,,s]\n    #    ---- loop lambda posterior ----   #\n    c                      = -N/2 + 1          # N=13\n    a                      = 2 / lambda.priors$alpha\n    for (x in 1:T){\n      b                  = sum(diag(t((u.t)[x,])%*%Sigma.posterior.draws[,,s]%*%(u.t)[x,]))\n      lambda.posterior.draws[x,s+1] = GIGrvg::rgig(1, lambda = c, chi = b, psi = a)\n    } # END x loop\n  } # END s loop\n  #}\n  \n  output                 = (list(A.posterior.exten = A.posterior.draws[,,S1:total_S], \n                                Sigma.posterior.exten = Sigma.posterior.draws[,,S1:total_S], \n                                lambda.posterior.exten = lambda.posterior.draws[,(S1+1):(S+1)]))\n  \n}\n\n# conduct simulation\nposterior.extended = posterior.draws.extended(total_S  = total_S , Y=Y, X=X)\n\n\nSigma_posterior_mean &lt;- apply(posterior.extended$Sigma.posterior.exten, 1:2, mean)\n\nSigma_df &lt;- as.data.frame(Sigma_posterior_mean)\ncolnames(Sigma_df) &lt;- c(\"Simulation_Y1\", \"Simulation_Y2\")\nrownames(Sigma_df) &lt;- c(\"Y1-Lag\", \"Y2-Lag\")\nknitr::kable(Sigma_df, caption = \"Posterior mean of the covariance matrix Sigma\")\n\n\nPosterior mean of the covariance matrix Sigma\n\n\n\nSimulation_Y1\nSimulation_Y2\n\n\n\n\nY1-Lag\n0.8796520\n0.0405029\n\n\nY2-Lag\n0.0405029\n0.9140267\n\n\n\n\n\n\nA_posterior_means &lt;- apply(posterior.extended$A.posterior.exten, 1:2, mean)\n\nA_df &lt;- as.data.frame(A_posterior_means)\ncolnames(A_df) &lt;- c(\"Simulation_Y1\", \"Simulation_Y2\")\nrownames(A_df) &lt;- c(\"Constant\", \"Y1-Lag\", \"Y2-Lag\")\nknitr::kable(A_df, caption = \"Posterior mean of the autoregressive coefficient matrix A\")\n\n\nPosterior mean of the autoregressive coefficient matrix A\n\n\n\nSimulation_Y1\nSimulation_Y2\n\n\n\n\nConstant\n0.0509733\n0.1120435\n\n\nY1-Lag\n0.9901219\n0.0103928\n\n\nY2-Lag\n0.0032246\n0.9928226"
  }
]