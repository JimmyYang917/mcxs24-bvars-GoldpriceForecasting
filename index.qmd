---
title: "An Evidence-based Forecast: Gold as a Traditional Safe-Haven Investment"
author: "Yuqiao Yang"

execute:
  echo: false
  
bibliography: references.bib
---

> **Abstract.** This research aims to explore future trends in Gold prices as a traditonal safe-haven investment using a Bayesian VARs model. In the wake of the 2008 financial crisis and especially the 2019 global Covid-19 pandemic, the world economy appears to be on the brink of a looming risk: a world-wide economic recession. The concern over the risk of investment returns has become a primary focus for global investors and financial institutions. This unease has been further exacerbated by geopolitical conflicts such as the Russia-Ukraine war (2022) and the Israeli-Palestinian conflict (2023). Consequently, this research aims to provide a briefly discussion and data-driven forecast of traditional safe-haven assets: Gold, under current circumstances. Factors considered include emerging safe-haven investments, risk-free investment assets, comparable investments, market returns, inflation on both demand and supply sides, broad money supply (M2), interest rates, unemployment rates and market volatility.
>
> **Keywords.** Bayesian VARs, Gold price, Inflation, Interset rate, Unemployments, US Bond Yeild, Safe-haven Assets, Forecasting, Volatility, R, Quarto.

# Introduction

**Objective:**

This research project aims to provide a monthly based, data-driven forecast of Gold price(USD) in two years, utilizing a Bayesian VARs model.

**Question:**

Gold as a traditional safe-haven asset, what are the anticipated price movements for the next year or beyond within the current environment?

**Introduction:**

Ulrich Beck introduced the concept of the risk society in the late 20th century, highlighting how humans confront entirely different systemic risks and face challenges in risk allocation under industrial society. Concurrently, globalization has reshaped the world and transformed human perceptions and experiences. Increasingly, evidence suggests that humanity is entering the risk society as described by Beck. Globalization encompasses not only economic, finance, technologe, and culture, but also risks. A China's financial exchange restriction might influence Australia's housing prices, while decisions made by the U.S. Central Bank regarding interest rates can prompt Western central banks to follow suit simultaneously. Following the 2008 financial crisis, major economies mitigated its aftermath significantly through quantitative easing monetary policies. This injection of substantial liquidity propelled economic growth steadily, leading to unprecedented prosperity in specific industries. However, the limitations of quantitative easing became apparent as the Covid-19 pandemic drew to a close. The United States and Western countries experienced unprecedented hyperinflation, coupled with indicators such as rising unemployment rates inversely correlated with inflation, conflicting long-term government bond yields with short-term treasuries, and record-breaking composite indices, signaling an impending global recession.

Geopolitical conflicts, such as the Russia-Ukraine war and the Israeli-Palestinian conflict, have intensified various risks. Disruptions in oil supply, blockages in key waterways, and logistical challenges in transporting goods and agricultural products have further exacerbated commodity price hikes and inflationary pressures. Consequently, mitigating or hedging investment risks has become the primary focus for global investors and financial institutions.

Among various safe-haven investments, gold has regained popularity as a hedge against uncertainty. This research aims to provide investors with a data-supported prediction of gold price trends over the next two years using Bayesian VAR models. The study incorporates information on comparable and emerging hedging products, market risks, returns, unemployment rates, inflation, interest rates, and other relevant parameters to construct a robust Bayesian VAR model. Ultimately, this research aids investors in identifying gold price trends and confidence intervals under different uncertainties within the current environment, thereby mitigating investment risks effectively.

# Data and Data Properties

To enhance the accuracy of gold price predictions, a total selection of 12 variables has been chosen, encompassing gold competitors, risk-free assets, and the Nasdaq index. From a broader macroeconomic standpoint, the variables also include inflation, the Producer Price Index (PPI), unemployment rates, crude oil prices, volatility indices, the dollar index, changes in the M2 money supply level, and federal fund effective rates.

-   $GoldFutures_{t}$ : Gold future price in USD per ounce, as considering the expectations in further gold price movements and high liquidity safe haven currency than real product.

-   Competitors and Substitutes for Gold ：

    -   Risk-free assets: treasury bonds
        -   $13WeekNotes_{t}$ : Considering as short-term risk free assets return. More time using in short term risk hedging in portfolio.
        -   $Tbill(5Year)_{t}$ : Considering as mid-term risk free assets return.
        -   $Tbill(10Year)_{t}$ : Considering as long-term risk free assets return.

-   Market returns as opposed to safe-haven investments：

    -   $NasdaqIndex_{t}$ ：Index that include all stocks available in NASDAQ to present the market returns.

-   Macro environment：

    -   $M2_{t}$ : the Board money supply of United States cause price index goes up.
    -   $Infla_{t}$ : CPI Index that present whole price level changes of all goods and services in US. Gold price are highly correlated with inflation.
    -   $Unemp_{t}$ : unemployment rate provide by Bureau of Statistic of US to present a general environment of Labor market.
    -   $CrudeOil_{t}$ : to present as basic cost of production.
    -   $FFERs_{t}$ : As the Federal Funds Effective Rates aim to lower the inflation, decrease the M2 level.
    -   $USDIndex_{t}$ : to present as the purchasing power of USD cross world-wide. That increase the total amount of investors and financial institutions come into US market to earn higher returns.
    -   $VolatilityIndex_{t}$ : to present the risk cross whole market and expectations of further coming events.

```{r}
rm(list=ls())
```

```{r global options}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```

```{r Library}
library(readabs)
library(fredr)
library(blscrapeR)
library(readrba)
library(xts)
library(fUnitRoots)   # ADF test - adfTest
library(tidyverse)    # for table
library(kableExtra)   # for print table
library(dplyr)
library(corrplot)
library(tseries) # for adf test
library(GIGrvg)
library(zoo)
library(ggplot2)      # plot in same graph
library(GIGrvg)       # GIG distribution
library(mvtnorm)      # Bvars forecast
library(plot3D)
library(HDInterval)   # hdi plot
library(grDevices)
set.seed(123)#set random seed for test
```

```{r}

Data_collection <- function(data_download) {
  data           = data.frame(data_download[,1], data_download[,6])
  colnames(data) = c('date','price')
  
  data$date      = as.Date(as.character(data$date),format="%Y-%m-%d") 
  data <- data %>%
    filter(date >= as.Date("2000-08-01"))
  data <- data[data$price != "null", ]
  #data$price[data$price == "null"] <- NA
  #data           = na.omit(data)
  data           = xts(data$price, data$date)
  data_finally           = to.monthly(data, OHLC = FALSE)
  return(data_finally)
}

```

```{r}

Data_collection_US <- function(data_download) {
  data           = data.frame(data_download[,1], data_download[,2])
  colnames(data) = c('date','price')
  
  data$date      = as.Date(as.character(data$date),format="%Y-%m-%d") 
  data <- data %>%
    filter(date >= as.Date("2000-08-01"))
  data <- data[data$price != "null", ]
  #data$price[data$price == "null"] <- NA
  #data           = na.omit(data)
  data           = xts(data$price, data$date)
  data_finally           = to.monthly(data, OHLC = FALSE)
  return(data_finally)
}

```

```{r}

Date_collection <- function(data) {
  correct_dates       = seq(as.Date("2000-09-01"), by="month", length.out=length(data))
  correct_dates       = floor_date(correct_dates, "month") - days(1)
  return(correct_dates)
}

```

```{r}
# 1. Gold futures price in USD per ounce (data periods: 2000-8 to recent)
#daily data download

gold_link = "https://query1.finance.yahoo.com/v7/finance/download/GC%3DF?period1=967608000&period2=1715586098&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true" 
gold_download       = read.csv(gold_link)
gold_data           = Data_collection(gold_download)
index(gold_data)    = Date_collection(gold_data)


# 2. 13 Weeks treasury bond yield (data periods: 1960-1 to recent)
bond_yield_13weeks = "https://query1.finance.yahoo.com/v7/finance/download/%5EIRX?period1=-315312000&period2=1715610633&interval=1d&events=history&includeAdjustedClose=true"
bond_yield_13weeks_file = read.csv(bond_yield_13weeks)
bond_yield_13weeks_data           = Data_collection(bond_yield_13weeks_file)
index(bond_yield_13weeks_data)    = Date_collection(bond_yield_13weeks_data)

# 3. 5-Year treasury bond yield (data periods: 1962-2 to recent)
bond_yield_5years = "https://query1.finance.yahoo.com/v7/finance/download/%5EFVX?period1=-252326400&period2=1715610588&interval=1d&events=history&includeAdjustedClose=true"
bond_yield_5years_file = read.csv(bond_yield_5years)
bond_yield_5years_data           = Data_collection(bond_yield_5years_file)
index(bond_yield_5years_data)    = Date_collection(bond_yield_5years_data)

# 4. 10-Year treasury bond yield (data periods: 1962-2 to recent) TNX
bond_yield_10years = "https://query1.finance.yahoo.com/v7/finance/download/%5ETNX?period1=-252326400&period2=1715610541&interval=1d&events=history&includeAdjustedClose=true"
bond_yield_10_years = read.csv(bond_yield_10years)
bond_yield_10_years_data           = Data_collection(bond_yield_10_years)
index(bond_yield_10_years_data)    = Date_collection(bond_yield_10_years_data)


# 5. Nasdaq Index (data periods: 1971-2 to recent) ^IXIC
Nasdaq_IXIC = "https://query1.finance.yahoo.com/v7/finance/download/%5EIXIC?period1=34612200&period2=1715610493&interval=1d&events=history&includeAdjustedClose=true"
Nasdaq_IXIC_file = read.csv(Nasdaq_IXIC)
Nasdaq_IXIC_data           = Data_collection(Nasdaq_IXIC_file)
index(Nasdaq_IXIC_data)    = Date_collection(Nasdaq_IXIC_data)

# 6. M2 level
M2_rate = "https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23e1e9f0&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=on&txtcolor=%23444444&ts=12&tts=12&width=1318&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=WM2NS&scale=left&cosd=1980-11-03&coed=2024-05-04&line_color=%234572a7&link_values=false&line_style=solid&mark_type=none&mw=3&lw=2&ost=-99999&oet=99999&mma=0&fml=a&fq=Monthly&fam=eop&fgst=lin&fgsnd=2020-02-01&line_index=1&transformation=pch&vintage_date=2024-05-13&revision_date=2024-05-13&nd=1980-11-03"
M2_rate_file = read.csv(M2_rate)
M2_rate_data           = Data_collection_US(M2_rate_file)
index(M2_rate_data)    = Date_collection(M2_rate_data)

# 7. Inflation rate （******Questions******）
Inflation_rate = "https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23e1e9f0&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=on&txtcolor=%23444444&ts=12&tts=12&width=1318&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=CPIAUCSL&scale=left&cosd=1947-01-01&coed=2024-05-01&line_color=%234572a7&link_values=false&line_style=solid&mark_type=none&mw=3&lw=2&ost=-99999&oet=99999&mma=0&fml=a&fq=Monthly&fam=avg&fgst=lin&fgsnd=2020-02-01&line_index=1&transformation=pc1&vintage_date=2024-05-13&revision_date=2024-05-13&nd=1947-01-01"

Inflation_rate_file = read.csv(Inflation_rate)
Inflation_rate_data           = Data_collection_US(Inflation_rate_file)
index(Inflation_rate_data)    = Date_collection(Inflation_rate_data)

# 8. Unemployment rate
unemployment_rate = "https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23e1e9f0&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=on&txtcolor=%23444444&ts=12&tts=12&width=1318&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=UNRATE&scale=left&cosd=1948-01-01&coed=2024-05-01&line_color=%234572a7&link_values=false&line_style=solid&mark_type=none&mw=3&lw=2&ost=-99999&oet=99999&mma=0&fml=a&fq=Monthly&fam=avg&fgst=lin&fgsnd=2020-02-01&line_index=1&transformation=lin&vintage_date=2024-05-13&revision_date=2024-05-13&nd=1948-01-01"
unemployment_rate_file = read.csv(unemployment_rate)
unemployment_rate_data           = Data_collection_US(unemployment_rate_file)
index(unemployment_rate_data)    = Date_collection(unemployment_rate_data)


# 9. Crude oil price
Crude_oil_price = "https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23e1e9f0&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=off&txtcolor=%23444444&ts=12&tts=12&width=1318&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=DCOILBRENTEU&scale=left&cosd=1990-01-01&coed=2024-05-13&line_color=%234572a7&link_values=false&line_style=solid&mark_type=none&mw=3&lw=2&ost=-99999&oet=99999&mma=0&fml=a&fq=Monthly&fam=eop&fgst=lin&fgsnd=2020-02-01&line_index=1&transformation=lin&vintage_date=2024-05-13&revision_date=2024-05-13&nd=1987-05-20"
Crude_oil_price_file = read.csv(Crude_oil_price)
Crude_oil_price_data           = Data_collection_US(Crude_oil_price_file)
index(Crude_oil_price_data)    = Date_collection(Crude_oil_price_data)


# 10. Federal Funds Effective Rates
Federal_funds_rate = "https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23e1e9f0&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=on&txtcolor=%23444444&ts=12&tts=12&width=1318&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=FEDFUNDS&scale=left&cosd=1990-01-01&coed=2024-05-13&line_color=%234572a7&link_values=false&line_style=solid&mark_type=none&mw=3&lw=2&ost=-99999&oet=99999&mma=0&fml=a&fq=Monthly&fam=avg&fgst=lin&fgsnd=2020-02-01&line_index=1&transformation=lin&vintage_date=2024-05-13&revision_date=2024-05-13&nd=1954-07-01"
Federal_funds_rate_file = read.csv(Federal_funds_rate)
Federal_funds_rate_data           = Data_collection_US(Federal_funds_rate_file)
index(Federal_funds_rate_data)    = Date_collection(Federal_funds_rate_data)

# 11. US dollars Index DX-Y.NYB
US_dollars_Index = "https://query1.finance.yahoo.com/v7/finance/download/DX-Y.NYB?period1=31813200&period2=1715611000&interval=1d&events=history&includeAdjustedClose=true"
US_dollars_Index_file = read.csv(US_dollars_Index)
US_dollars_Index_data           = Data_collection(US_dollars_Index_file)
index(US_dollars_Index_data)    = Date_collection(US_dollars_Index_data)

# 12. Volatility Index
Volatility_Index = "https://query1.finance.yahoo.com/v7/finance/download/%5EVIX?period1=631267200&period2=1715610192&interval=1d&events=history&includeAdjustedClose=true"
Volatility_Index_file = read.csv(Volatility_Index)
Volatility_Index_data          = Data_collection(Volatility_Index_file)
index(Volatility_Index_data)   = Date_collection(Volatility_Index_data)

```

```{r Data Combine, fig.pos="H"}

# All Variables
all_data             = merge( gold_data,               bond_yield_13weeks_data, 
                                      bond_yield_5years_data,  bond_yield_10_years_data,  
                                      Nasdaq_IXIC_data,        M2_rate_data,            
                                      Inflation_rate_data,     unemployment_rate_data,  
                                      Crude_oil_price_data,    Federal_funds_rate_data, 
                                      US_dollars_Index_data,   Volatility_Index_data)

colnames(all_data)   = c("GFutures_data",    "TBill13W_data", 
                         "TBill5Y_data",     "TBill10Y_data", 
                         "Nasdaq_data",      "M2_data", 
                         "InflaR_data",      "UnempR_data",
                         "CrudeOil_data",    "InterestR_data",
                         "USDIndex_data",    "Volatility_data")
all_data <- all_data[complete.cases(all_data), ]
```

After download all variables as designed, merge all data sets into a new frame and change the column names. The date of data used in this research will start at 2004-08-31 as at 2024-03-31.

# Plots of all variables

```{r}
data1 <- subset(all_data)

par(mfrow = c(4, 4), mar=c(2,2,2,2))
for (i in 1:12) { 
  ts.plot(data1[, i], main = colnames(data1)[i], 
          ylab = "", xlab = "", col="red")
}

```

```{r}

# Choose the columns
selected_columns <- c("GFutures_data", "Nasdaq_data")#, "CrudeOil_data", "USDIndex_data", "Volatility_data")
selected_columns2 <- c("TBill13W_data", "TBill5Y_data", "TBill10Y_data", "M2_data", "InflaR_data", "UnempR_data", "InterestR_data")

selected_data <- data1[, selected_columns]
selected_data2 <- data1[, selected_columns2]

colors1 <- rainbow(length(selected_columns))
colors2 <- rainbow(length(selected_columns2))

# plot first variables
plot.xts(selected_data, main="Selected Time Series Data", col=colors1)
legend("topright", legend=names(selected_data), col=colors1, lty=1, cex=0.8, bty="n")

# plot second variables
plot.xts(selected_data2, main="Selected Rates Series Data", col=colors2, lwd=2)
legend("topright", legend=names(selected_data2), col=colors2, lty=1, cex=0.8, bty="n")
```

As above, we could have all 12 variables in visualization format individually. As a quick overlook, 13 weeks bond has a similar trend with federal funds effective rates; other 2 treasury bonds move quite same; gold future, Nasdaq, crude oil price and US dollars Index have upper ward trends; Unemployment rates seems like opposite to federal rates; M2 and other 2 time series seems like stationary on mean but variance change over time.

**Correlation Table**

```{r}
correlation_matrix <- cor(data1)

corrplot(correlation_matrix, method = "color", type = "upper", tl.srt = 45,tl.col = "black",tl.cex = 0.5)
```

A simple correlation provide us a basic understanding of those 12 variables. Gold future as safe-haven is highly positive correlated with Nasdaq Index.

Therefore, we could use ACF and PACF test to indicate whether there is autocorrelations.

```{r}
par(mfrow = c(4, 4), mar=c(2,2,2,2))
for (i in 1:12){
acf = acf(data1[,i], plot = FALSE)[1:12]
plot(acf, main = "")
title(main = paste(colnames(data1)[i]), line = 0.5)
}

par(mfrow = c(4, 4), mar=c(2,2,2,2))
for (i in 1:12){
pacf = pacf(data1[,i], plot = FALSE)[1:12]
plot(pacf, main = "")
title(main = paste(colnames(data1)[i]), line = 0.5)
}
```

Both ACF and PACF suggest that variables are highly autocorrelated except M2: Board money change rate. That might because the policy has changed in earlier 2020 and 2022. Therefore, we further need ADF test(unit root test) to feed our time series are stationary or not.

```{r}

adf_test <- list()
for (i in 1:12) {
  adf_result = adf.test(data1[,i], k = 4)
  adf_test[[i]] <- adf_result
}
adf_table <- data.frame(p_value = numeric(length(adf_test)))
adf_table <- data.frame(p_value = numeric(length(adf_test)))
for (i in 1:length(adf_test)) {adf_table[i, "p_value"] = round(adf_test[[i]]$p.value,3)
}

rownames(adf_table)<- c("GFutures_data",    "TBill13W_data", 
                         "TBill5Y_data",     "TBill10Y_data", 
                         "Nasdaq_data",      "M2_data", 
                         "InflaR_data",      "UnempR_data",
                         "CrudeOil_data",    "InterestR_data",
                         "USDIndex_data",    "Volatility_data")
colnames(adf_table)<- c("P-value")
knitr::kable(adf_table)
```

```{r}

columns_to_log <- c("GFutures_data", "Nasdaq_data", "CrudeOil_data", "USDIndex_data", "Volatility_data")

all_data[, columns_to_log] <- log(all_data[, columns_to_log] + 1e-6)

names(all_data)[1] <- "Lgold"
names(all_data)[5] <- "LNasIndex"
```

```{r}

selected_columns <- c("Lgold", "LNasIndex", "CrudeOil_data", "USDIndex_data", "Volatility_data")

selected_data <- all_data[, selected_columns]

colors1 <- rainbow(length(selected_columns))

plot.xts(selected_data, main="Log INDEX Data", col=colors1, type="l", lty=1, lwd=2, grid=T, major.ticks="years", minor.ticks="quarters", xlab="Date", ylab="Log Value")
legend("bottom", legend=colnames(selected_data), col=colors1, lty=1, lwd=2, cex=0.8, bty="n")

```

# Modeling and Hypothesis

This research project based on **Bayesian VARs(p) model** to forecast the Gold price in next two years. For time $t$ = {1,2,3,...,$T-1$,$T$} :

```{=tex}
\begin{aligned}
y_t &= \mu_0 + A_1y_{t-1} + A_2y_{t-2}...+A_py_{t-p} +\epsilon_t\\

\epsilon_t|Y_{t-1} &\sim iid \mathcal{N}_{12}(0_{12}, \Sigma)
\end{aligned}
```
Where N = 12 and $y_{t}$ is a vector of 12 variables at time $t$:

```{=tex}
\begin{aligned}

y_{t}=\begin{pmatrix}
GoldFutures_{t}\\
13WeekNotes_{t}\\
Treasurybill(5Year)_{t} \\
Treasurybill(10Year)_{t} \\
NasdaqIndex_{t} \\
M2_{t} \\
Inflation_{t} \\
Unemployment_{t}\\
CrudeOil_{t}\\
FFERs_{t} \\
USDollarIndex_{t}\\
VolatilityIndex_{t}\\
\end{pmatrix}

\end{aligned}
```
For time $t$ = 1,2,.....,$T$：

-   $y_t$ is a $N(12)\times 1$ vector of observations at time $t$
-   $\mu_0$ is a $N(12)\times 1$ vector of constant terms
-   $A_i$ is a $N(12)\times N(12)$ matrix of autoregressive slope parameters
-   $\epsilon_t$ is a $N(12)\times 1$ vector of error terms which is a multivariate white nose process(time invariant)
-   $Y_{t-1}$ is the information set collecting observations on y up to time $t-1$
-   $\Sigma$ is a $N(12)\times N(12)$ covariance matrix of the error term

In matrix notation:

$$
\begin{align}
Y &= X A +E 
\\
E|X &\sim \mathcal{MN}_{T\times N}(\textbf{0},\Sigma,I_T)
\\
Y|X &\sim \mathcal{MN}_{T\times N}(XA,\Sigma,I_T)
\end{align} 
$$

```{=tex}
\begin{aligned}

A=
\begin{bmatrix}
\mu_{0}' \\ A_{1}' \\
A_{2} '\\.\\.\\.\\A_{p}'
\end{bmatrix}_{K \times N}

Y=
\begin{bmatrix}
y_{1}' \\ y_{2}' \\
y_{3} '\\.\\.\\.\\y_{T}'
\end{bmatrix}_{T \times N}

x_t=
\begin{bmatrix}
1 \\ y_{t-1}' \\
y_{t-2} '\\.\\.\\.\\y_{t-p}'
\end{bmatrix}_{K \times 1}

X=
\begin{bmatrix}
x_{1}' \\ x_{2}' \\
x_{3} '\\.\\.\\.\\x_{T}'
\end{bmatrix}_{T \times K}

E=
\begin{bmatrix}
\epsilon_1' \\ \epsilon_2' \\
\epsilon_3 '\\.\\.\\.\\\epsilon_T'
\end{bmatrix}_{T \times N}

\end{aligned}
```
where K = 1 + pN

Base on the model above, we could first turn B Vars(p) model into B Vars(1) model and easily regress to have the parameter matrix. Then we could have a $t+h$ period forward forecasting with increase of variance, in this case: $h$ = 24.

The main focus of estimate output is the conditional mean of Gold price, which base on current information set $Y_{t-1}$. It provide the average mean prediction of Gold price which investors and financial institutions interested in. Moreover, 1 standard deviation and 2 standard deviation will also produced in forecasting process to provide a 68% and 95% of confidence intervals of future Gold price movements in $h$ periods base on current information set.

Furthermore, different prior distribution might be used to provide different level of uncertainty of current environment(information set). Compare the difference of Gold price under different priors could help to prove the Gold as a high quality safe-haven investment and increase investors and financial institutions confidence and further expectations. ( Competitors for golds might also be used under different priors, such as BIT-USD, Nasdaq Index and short to mid-term treasury bills. )

Based on Bayes' Theorem:

```{=tex}
\begin{aligned}

P(A|B) & =\frac{P(B|A)P(A)}{P(B)} \\
P(A|B) & \propto P(B|A)P(A)

\end{aligned}
```
Therefore we have:

```{=tex}
\begin{aligned}

P(A,\Sigma|Y,X) &= \frac{P(Y|X,A,\Sigma) \ P(A,\Sigma)}{P(Y)}\\
&\propto P(Y|X,A,\Sigma) \ P(A,\Sigma)\\
&= L(A,\Sigma | Y,X) \ P(A|\Sigma) \ P(\Sigma) \\\\

A|\Sigma &\sim \mathcal{MN}_{K\times N}(\underline{A}, \Sigma,\underline{V}) 
\\ 
\Sigma &\sim \mathcal{IW}_{N}(\underline{S}, \underline{v})

\end{aligned}
```
Which is the posterior distribution is the proportion of likelihood function and prior.

where $A_{M \times N}$ follow a matrix normal distribution:

-    $\underline{A}$ is the mean of matrix normal distribution

-    $\Sigma_{N \times N}$is the row specific covariance matrix

-   $\underline{V}_{M \times M}$ is the column specific covariance matrix

therefore, we have:

```{=tex}
\begin{aligned}

vec(A)\sim N_{MN}(vec(\underline{A}),\Sigma\otimes \underline{V})

\end{aligned}
```
where $\Sigma$ follow a Inverse Wishart distribution:

-    $\underline{S}$ is N × N positive definite symmetric matrix called the scale matrix. -

-    $\underline{v}$ \> N + 2 denotes degrees of freedom.

## First Benchmark Assumption: Basic model with Minnesota prior

In real life, macroeconomic variables are more likely being unit-root non stationary and are well-characterized by a multivariate random walk process：

```{=tex}
\begin{aligned}

y_{t} = y_{t-1} + \epsilon_t

\end{aligned}
```
Therefore, our benchmark model uses Minnesota prior(1984) based on random walk process for our Bayesian forecasting:

Set the prior mean $A$ to:

```{=tex}
\begin{aligned}

\underline{A}=\left[ 0_{N\times1} \ \ \ \  I_{N} \ \ \ \ \ 0_{N\times(p-1)N} \right]'

\end{aligned}
```
where the mean of first lag equal to 1 and mean of constant term and other lags are 0.

Set the column specific prior covariance of $A$ (prior shrinkage)to:

```{=tex}
\begin{aligned}

\underline{V} = diag\left[ \kappa_{2} \quad \kappa_{1}(\textbf{P} ^{-2}\otimes \textbf{i}^{'}_{N}) \right]

\end{aligned}
```
where:

-   \-$\textbf{P}$ is the list of legs, $\textbf{P} \ \ \ =\left[ 1 \quad 2 \quad 3\quad ... \quad p \right]$

-   $\textbf{i}^{'}_{N}$ is a $N \times 1$ vector of ones

-   $\kappa_{1}$: overall shrinkage level for autoregressive slopes

-   $\kappa_{2}$: overall shrinkage for the constant term

therefore, we could have the variance of $A$: $$
VAR\left[ vec\left( A \right) \right]\quad = \quad \Sigma \ \ \otimes \ \ \underline{V}
$$

### Function Proofing

Consider Bi-variate Gaussian random walk process:

$$
\begin{align}
y_t &= 
\begin{bmatrix}
y_{t,1} \\
y_{t,2}
\end{bmatrix} = 
\begin{bmatrix}
y_{t-1,1} \\
y_{t-1,2}
\end{bmatrix} + 
\begin{bmatrix}
\epsilon_{t,1} \\
\epsilon_{t,2}
\end{bmatrix} \\
where \quad  
\epsilon_{t,1} &\sim \mathcal{N}(0,1)  \\ 
\epsilon_{t,2} &\sim \mathcal{N}(0,1)
\end{align}
$$ Variables in Matrix Notation:
 
<!-- please note the def below works only for $p=1$ -->
$$ 
Y = \begin{bmatrix}
y_2' \\
y_3' \\
\vdots \\
y_n'
\end{bmatrix},
\quad
X = \begin{bmatrix}
1 \quad y_1' \\
1 \quad y_2' \\
\vdots \quad \vdots \\
1 \quad y_{n-1}'
\end{bmatrix}
$$

<!-- Please present the posterior -->

Function below is `posterior.draws`:

```{r Basic Model, fig.align='center',fig.pos='H'}
#| echo: true

## Posterior sample draw function for basic model(posterior.draws)
posterior.draws       = function (S, Y, X, A.prior, V.prior, S.prior, nu.prior){
  
    # normal-inverse Wishard posterior parameters
    V.bar.inv         = t(X)%*%X + diag(1/diag(V.prior))
    V.bar             = solve(V.bar.inv)
    A.bar             = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)
    nu.bar            = nrow(Y) + nu.prior
    S.bar             = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
    S.bar.inv         = solve(S.bar)
    
    # posterior draws 
    Sigma.posterior   = rWishart(1, df=nu.bar, Sigma=S.bar.inv)
    Sigma.posterior   = apply(Sigma.posterior,3,solve)
    Sigma.posterior   = array(Sigma.posterior,c(N,N,S))
    A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))
    L                 = t(chol(V.bar))
    
    for (s in 1:S){
      A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])
    }

    output            = list(A.posterior=A.posterior, Sigma.posterior=Sigma.posterior)
    return(output)
}
```

```{r Basic Model Function Proof, fig.align='center',fig.pos='H'}

e1 = cumsum(rnorm(1000, 0, sd=1))
e2 = cumsum(rnorm(1000, 0, sd=1))
e  = cbind(e1,e2)

## Define data X, Y 
Y = ts(e[2:nrow(e),], frequency=1)
X = matrix(1,nrow(Y),1)
X = cbind(X,e[2:nrow(e)-1,])


## Test on basic model
N           = ncol(Y)                          # N=2
p           = frequency(Y)
A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)

# Prior distribution (with Minnesota prior)
kappa.1             = 1                                    # shrinkage for A1 to Ap
kappa.2             = 100                                  # shrinkage for constant 
A.prior             = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2:(N + 1),] = diag(N)
V.prior             = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior             = diag(diag(Sigma.hat))
nu.prior            = N+2
I.matrix            = diag(1,nrow(Y),nrow(Y))
# Applying function 
posterior.sample.draws.p = posterior.draws(S=10000, Y=Y, X=X, A.prior, V.prior, S.prior, nu.prior)

```

-   The posterior mean of the $A$ is:

```{r Basic Model Proof A, fig.align='center',fig.pos='H'}


basic.model.proof.A <- 
  tibble( "A" = c("Constant term", "Y1 lag","Y2 lag"),
          "Simulation Parameter Y1" 
          = c(round(mean(posterior.sample.draws.p[["A.posterior"]][1,1,]),4),
              round(mean(posterior.sample.draws.p[["A.posterior"]][2,1,]),4),
              round(mean(posterior.sample.draws.p[["A.posterior"]][3,1,]),4)),
           "Simulation Parameter Y2" 
          = c(round(mean(posterior.sample.draws.p[["A.posterior"]][1,2,]),4),
              round(mean(posterior.sample.draws.p[["A.posterior"]][2,2,]),4),
              round(mean(posterior.sample.draws.p[["A.posterior"]][3,2,]),4))

  )

kable(basic.model.proof.A, align = "c") %>% 
  kable_styling(font_size = 8, 
                fixed_thead = TRUE, 
                full_width = FALSE, 
                position = "center",
                latex_options = c("HOLD_position"),
                bootstrap_options = c("striped", "hover", "bordered", "responsive", "dark"))

```

Table 2 Basic Model Proofing Simulation for $A$

-   The posterior mean of the $\Sigma$ is:

```{r}

basic.model.proof.Sigma <-
  tibble( "Sigma" = c("Y1 lag","Y2 lag"),
          "Simulation Parameter Y1" 
          = c(round(mean(posterior.sample.draws.p[["Sigma.posterior"]][1,1,]),4),
              round(mean(posterior.sample.draws.p[["Sigma.posterior"]][2,1,]),4)),
           "Simulation Parameter Y2" 
          = c(round(mean(posterior.sample.draws.p[["Sigma.posterior"]][1,2,]),4),
              round(mean(posterior.sample.draws.p[["Sigma.posterior"]][2,2,]),4))

  )

kable(basic.model.proof.Sigma, align = "c") %>% 
  kable_styling(font_size = 8, 
                fixed_thead = TRUE, 
                full_width = FALSE, 
                position = "center",
                latex_options = c("HOLD_position"),
                bootstrap_options = c("striped", "hover", "bordered", "responsive", "dark"))

```

Table 3 Basic Model Proofing Simulation for $\Sigma$

Extension Model:

## The extended model: Laplace distribution of error term

The **Basic Model** is the standard VARs model that assume the error terms $U$ are independent and identically distributed($iid$) as $U \ \sim N_{TN}(0 \ , \ \Sigma)$. In other formation, it could be presented as $vec(U) \ \sim N(0 \ , \ \Sigma \ \otimes \  I_{T})$. Where $\Sigma$ is a $N\times N$ cross sectional covariance matrix, $I_{t}$ is a $T\times T$ identity matrix present serial covariance, $\otimes$ is the Kronecker product and the operator $vec(.)$ is vectorization that inverts the matrix into the column vector by stacking the columns.

Therefore, we could consider a more general covariance structure:

```{=tex}
\begin{align}

vec(U) \ \sim N(0 \ , \ \Sigma \ \otimes \  \Omega)\\
where \quad \quad \Omega \ &= \ \ diag\left[ \lambda_{1}, \lambda_{2},...,\lambda_{t}  \right]\\
and \quad \quad \lambda \ &\sim \ Exp \ (\alpha)\\

\end{align}
```
And then, the distribution of error terms in extension model will be **Laplace distribution** instead of the normally distributed errors assumption. The Laplace distribution is well-suited for describing financial anomalies due to its sharp peaks and heavy tails. Utilizing this distribution enhances the model's robustness against anomalies, making it particularly appropriate for financial time series analysis. Given that most of our variables are financial time series data, applying a Laplace distribution to the error term is more appropriate.

Following [Eltoft,Kim, and Lee 2006b](https://ieeexplore.ieee.org/document/1618702), for covariance with a general Kronecker structure, if each ${\lambda}$ has an independent exponential distribution with mean ${\alpha}$, then marginally ${U_t}$ has a multivariate Laplace distribution with mean vector 0 and covariance matrix ${\alpha\Sigma}$.

```{=tex}
\begin{align}

U &\sim \ Laplace(0 \ , \  \alpha\Sigma) \\
U|\lambda &\sim \mathcal{MN}(0 \ , \  \Sigma \ , \ \Omega) \\
\Omega \ &= \ \ diag\left[ \lambda_{1}, \lambda_{2},...,\lambda_{t}  \right]\\
\lambda &\sim \ Exp(\alpha)

\end{align}
```
The kernel of the likelihood function:

```{=tex}
\begin{align}
L(A,\Sigma,\Omega|Y,X) &\propto \det(\Sigma)^{-\frac{T}{2}} \det(\Omega)^{-\frac{N}{2}} exp\{-\frac{1}{2} tr[\Sigma^{-1} (Y-XA)' \Omega^{-1} (Y-XA) ]\}
\end{align}
```
For posteriors distribution, $A$, $\Sigma$ and $\lambda_t$ can then be derived using the likelihood and the prior distributions as follows:

```{=tex}
\begin{align}

p(A,\Sigma|Y,X) &\propto L(A,\Sigma,\Omega|Y,X) \ p(A,\Sigma) \\
\\
&= \det(\Sigma)^{-\frac{T}{2}} \det(\Omega)^{-\frac{N}{2}} exp\{-\frac{1}{2} tr[\Sigma^{-1} (Y-XA)' \Omega^{-1} (Y-XA) ]\} \\
&\times \det(\Sigma)^{-\frac{N+k+\underline{\nu}+1}{2}} exp\{-\frac{1}{2}tr[\Sigma^{-1}(A-\underline{A})'(\underline{V})^{-1}(A-\underline{A})]\} \\
&\times exp\{-\frac{1}{2}tr[\Sigma^{-1}\underline{S}]\} \\

&= \det(\Sigma)^{-\frac{T+N+K+\underline{\nu}+1}{2}} \det(\Omega)^{-\frac{N}{2}} \\
&\times exp\{-\frac{1}{2} tr[\Sigma^{-1}(Y'\Omega^{-1}Y - 2A'X'\Omega^{-1}Y + A'X'\Omega^{-1}XA \\
&+ A'\underline{V}^{-1}A -2A'\underline{V}^{-1}\underline{A} + \underline{A}'\underline{V}^{-1}\underline{A} + \underline{S})]\}

\end{align}
```
The kernel can be rearranged in the form of the **Matrix-variate normal-inverse Wishart distribution**.

```{=tex}
\begin{align}

p(A,\Sigma|Y,X) &\sim MNIW(\bar{A},\bar{V},\bar{S},\bar{\nu}) \\
&\\
\bar{V} &= (X'\Omega^{-1}X + \underline{V}^{-1})^{-1} \\
\bar{A} &= \bar{V}(X'\Omega^{-1}Y + \underline{V}^{-1}\underline{A}) \\
\bar{\nu} &= T + \underline{\nu}\\
\bar{S} &= Y'\Omega^{-1}Y + \underline{A}'\underline{V}^{-1}\underline{A} + \underline{S} - \bar{A}'\bar{V}^{-1}\bar{A}

\end{align}
```
The kernel of the fully conditional posterior distribution of $\lambda_t$ is then derived as follows:


```{=tex}
\begin{align}
p(\lambda_t|Y,X,A,\Sigma) &\propto L(A,\Sigma,\lambda_t|Y,X)p(\lambda_t) \\
\\
&\propto \det(\Omega)^{-\frac{N}{2}} exp\{-\frac{1}{2} tr[\Sigma^{-1} (Y-XA)' (\lambda_t I_T)^{-1} (Y-XA) ]\} \\
&\times (\prod_{t=1}^{T}\frac{1}{\alpha}exp\{ -\frac{1}{\alpha}\lambda_t \})\\

&= (\prod_{t=1}^{T}\lambda_t^{-\frac{N}{2}}) exp\{-\frac{1}{2}\sum_{t=1}^{T}\frac{1}{\lambda_t} \epsilon_t'\Sigma^{-1}\epsilon_t]\}\\
&\times (\prod_{t=1}^{T}\frac{1}{\alpha}exp\{ -\frac{1}{\alpha}\lambda_t \})\\

&= (\prod_{t=1}^{T}\lambda_t^{-\frac{N}{2}+1-1}) exp\{-\frac{1}{2}\sum_{t=1}^{T}[\frac{\epsilon_t'\Sigma^{-1}\epsilon_t}{\lambda_t} +\frac{2}{\alpha}\lambda_t]\} 
\end{align}
```
```{r}
# the following extension equations will be update latter due to the change of lambda,
# the extension will be separate into two parts
# 1: the independent lambda
# 2: estimate lambda's mean, alpha, still use gibbs sampler
```

The above expression can be rearranged in the form of a Generalized inverse Gaussian distribution kernel as follows:

```{=tex}
\begin{align}
\lambda_t|Y,A,\Sigma &\sim GIG(a,b,p) \\
\\
a &=\frac{2}{\alpha} \\
b &= tr[\Sigma^{-1}(Y-XA)'(Y-XA)] \\
p &= -\frac{TN}{2}+1
\end{align}
```
```{r}

par(mfrow = c(1, 2), mar = c(4, 2, 4, 2))  


lambdas <- c(0.5, 1, 2)


colors <- c("red", "blue", "green")


curve(dexp(x, rate = lambdas[1]), from = 0, to = 5, ylim = c(0, 2), lwd = 2,
      xlab = "x", ylab = "Density", col = colors[1], main = "Exponential Distributions")
for (i in 2:length(lambdas)) {
  curve(dexp(x, rate = lambdas[i]), from = 0, to = 5, col = colors[i], add = TRUE, lwd = 2)
}


legend("topright", legend = paste("lambda =", lambdas), col = colors, lty = 1, lwd = 2, cex = 0.6)

library(extraDistr)

parameters <- list(
  list(mean = 0, beta = 2),
  list(mean = 0, beta = 1),
  list(mean = 0, beta = 0.5)
)


colors <- c("red", "blue", "green")


plot(NULL, xlim = c(-10, 10), ylim = c(0, 1), xlab = "x", ylab = "Density",
     main = "Laplace Distributions")
for (i in seq_along(parameters)) {
  curve(dlaplace(x, mu = parameters[[i]]$mean, s = parameters[[i]]$beta),
        col = colors[i], lwd = 2, add = TRUE)
}


legend("topright", legend = sapply(parameters, function(p) paste("mean =", p$mean, ", beta =", p$beta)),
       col = colors, lty = 1, lwd = 2, bty = "n", cex = 0.6)
```

### Proof of extended model

The Gibbs sampler method will be applied to generate random draws from the full conditional posterior distribution:

1.  Draw $\Sigma^{(s)}$ from the $IW(\bar{S},\bar{\nu})$ distribution.
2.  Draw $A^{(s)}$ from the $MN(\bar{A},\Sigma^{(s)}, \bar{V})$ distribution.
3.  Draw $\lambda_t^{(s)}$ from $GIG(a,b,p)$.

Repeat steps 1, step 2 and 3 for $S_1$+$S_2$times.

Discard the first draws that allowed the algorithm to converge to the stationary posterior distribution.

Output is $\left\{ {A^{(s)}, \Sigma^{(s)}}, \lambda_t^{(s)}\right\}^{S_1+S_2}_{s=S_1+1}$.

Function below is `posterior.draws.exten`:

```{r}
# rewrite the function so that the elements A.posterior, Sigma.posterior, and lambda.posterior are defined inside of the function
# setup 
S1                = 500                            # determine the burn-in draws
S2                = 9500                           # number of draws from the final simulation
total_S           = S1+S2
A.posterior       = array(NA, dim = c((1+N*p),N,S1+S2))
Sigma.posterior   = array(NA, dim = c(N,N,S1+S2))
lambda.posterior  = matrix(NA, S1+S2, 1)

# initial value of lambda
lambda.posterior[1] = 2                               # set lambda0 

# Prior distribution: alpha
lambda.priors = list(
  alpha = 2
)

```

```{r}
#| echo: true

## Posterior sample draw function for extended model(posterior.draws.exten)
posterior.draws.exten = function (total_S, Y, X, X=X, A.prior, V.prior, S.prior, nu.prior, lambda.priors){
for (s in 1:total_S){
    # NIW posterior parameters
    
    V.bar.inv              = t(X)%*%solve(lambda.posterior[s]*I.matrix)%*%X + diag(1/diag(V.prior)) 
    V.bar                  = solve(V.bar.inv)
    A.bar                  = V.bar%*%(t(X)%*%solve(lambda.posterior[s]*I.matrix)%*%Y + diag(1/diag(V.prior))%*%A.prior)
    nu.bar                 = nrow(Y) + nu.prior
    S.bar                  = S.prior + t(Y)%*%solve(lambda.posterior[s]*I.matrix)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
    S.bar.inv              = solve(S.bar)
  
    # posterior draws for A and Sigma
    Sigma.posterior.IW     = rWishart(1, df=nu.bar, Sigma=S.bar.inv)
    Sigma.posterior.draw   = apply(Sigma.posterior.IW,3,solve)
    Sigma.posterior[,,s]   = Sigma.posterior.draw
    A.posterior[,,s]       = array(rnorm(prod(c(dim(A.bar),1))),c(dim(A.bar),1))
    L                      = t(chol(V.bar))
    A.posterior[,,s]       = A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])
    
    
    # Update parameters for lambda posterior
    p                      = 1-0.5*nrow(Y)*ncol(Y)             # N=10
    diff_A                 = Y - X%*%A.posterior[,,s]
    product                = t(diff_A) %*% diff_A
    b                      = sum(diag(solve(Sigma.posterior[,,s]) %*% product))
    a                      = 2 / lambda.priors$alhpa
    
    # Draw next period value for lambda from GIG distribution
    if (s!=total_S){
       lambda.posterior[s+1] = GIGrvg::rgig(n=1, lambda = p, chi = b, psi = a)
    }
  }
  
    output                 = list(A.posterior.exten = A.posterior[,,(S1+1):S2], 
                                  Sigma.posterior.exten = Sigma.posterior[,,(S1+1):S2], 
                                  lambda.posterior.exten = lambda.posterior[(S1+1):S2,])

    return(output)
}


```

#### Function Proofing

```{r}

# setup 
kappa.1           = 1                                # shrinkage for A1 to Ap
kappa.2           = 100                              # shrinkage for constant 
S1                = 5                              # determine the burn-in draws
S2                = 95                             # number of draws from the final simulation
total_S           = S1+S2
A.posterior       = array(NA, dim = c((1+N*p),N,S1+S2))
Sigma.posterior   = array(NA, dim = c(N,N,S1+S2))

#lambda.posterior     = diag(5,nrow(Y),nrow(Y))
lambda.posterior  = matrix(NA, S1+S2, 1)

# initial value of lambda
lambda.posterior[1] = 2                               # set lambda0 

# Prior Gamma distribution: k, theta
lambda.priors = list(
    alpha = 2
)

# Applying function 
posterior.extend.draws.p = posterior.draws.exten(total_S = total_S, Y=Y, X=X, A.prior, V.prior, S.prior, nu.prior, lambda.priors)
```

After fitting a model that includes a constant term and one lag with artificial data, just like the basic model, the extend model also shows that the posterior mean of both the autoregressive and covariance matrices closely identity matrix, and the posterior mean of the constant term is almost a vector of zeros.

-   The posterior mean of the $A$ is:

```{r}

extend.model.proof.A <-
  tibble( "A" = c("Constant term", "Y1 lag","Y2 lag"),
          "Simulation Parameter Y1" 
          = c(round(mean(posterior.extend.draws.p[["A.posterior.exten"]][1,1,]),4),
              round(mean(posterior.extend.draws.p[["A.posterior.exten"]][2,1,]),4),
              round(mean(posterior.extend.draws.p[["A.posterior.exten"]][3,1,]),4)),
           "Simulation Parameter Y2" 
          = c(round(mean(posterior.extend.draws.p[["A.posterior.exten"]][1,2,]),4),
              round(mean(posterior.extend.draws.p[["A.posterior.exten"]][2,2,]),4),
              round(mean(posterior.extend.draws.p[["A.posterior.exten"]][3,2,]),4))
  )

kable(extend.model.proof.A, align = "c") %>% 
  kable_styling(font_size = 8, 
                fixed_thead = TRUE, 
                full_width = FALSE, 
                position = "center",
                latex_options = c("HOLD_position"),
                bootstrap_options = c("striped", "hover", "bordered", "responsive", "dark"))

```

Table 4 Extend Model Proofing Simulation for $A$

-   The posterior mean of the $\Sigma$ is:

```{r}

extend.model.proof.Sigma <-
  tibble( "Sigma" = c("Y1 lag","Y2 lag"),
          "Simulation Parameter Y1" 
          = c(round(mean(posterior.extend.draws.p[["Sigma.posterior.exten"]][1,1,]),4),
              round(mean(posterior.extend.draws.p[["Sigma.posterior.exten"]][2,1,]),4)),
           "Simulation Parameter Y2" 
          = c(round(mean(posterior.extend.draws.p[["Sigma.posterior.exten"]][1,2,]),4),
              round(mean(posterior.extend.draws.p[["Sigma.posterior.exten"]][2,2,]),4))
  )

kable(extend.model.proof.Sigma, align = "c") %>% 
  kable_styling(font_size = 8, 
                fixed_thead = TRUE, 
                full_width = FALSE, 
                position = "center",
                latex_options = c("HOLD_position"),
                bootstrap_options = c("striped", "hover", "bordered", "responsive", "dark"))

```

Table 5 Extend Model Proofing Simulation for $\Sigma$

# Empirical Analysis - Model Applying and Forecasing

## Basic Model

```{r}

## Create Y and X
y             = ts(all_data[,1:ncol(all_data)])     # 10col, 135row
Y             = ts(y[13:nrow(y),], frequency=12)      # 10col, 131row
X             = matrix(1,nrow(Y),1)
for (i in 1:frequency(Y)){
  X           = cbind(X,y[13:nrow(y)-i,])            # 10*4+1=41col, 131row
}
 
## Pre-setup 
N             = ncol(Y)                             # N=10
p             = frequency(Y)                        # p=4
A.hat         = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat     = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)

# Prior distribution (with Minnesota prior)
kappa.1       = 0.01                                     # shrinkage for A1 to Ap
kappa.2       = 100                                   # shrinkage for constant 
A.prior       = matrix(0,nrow(A.hat),ncol(A.hat))
# A.prior[2,1]  = 1
# A.prior[7,6]  = 1  
A.prior[2:13,] = diag(12)
V.prior       = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior       = diag(diag(Sigma.hat))
nu.prior      = N+2 
I.matrix            = diag(1,nrow(Y),nrow(Y))
```

```{r}

## Function Applying
posterior.sample.draws = posterior.draws(S=10000, Y=Y, X=X, A.prior, V.prior, S.prior, nu.prior) # sample use S=10000
A.posterior.simu       = posterior.sample.draws$A.posterior
Sigma.posterior.simu   = posterior.sample.draws$Sigma.posterior

```

```{r}

## Two-year ahead forecasting h=24
# set up
h                      = 24
S                      = 10000
Y.h                    = array(NA,c(h,N,S))

# sampling predictive density
for (s in 1:S){
  A.posterior.draw     = A.posterior.simu[,,s]
  Sigma.posterior.draw = Sigma.posterior.simu[,,s]
    x.Ti               = Y[(nrow(Y)-p+1):nrow(Y),]
    x.Ti               = x.Ti[p:1,]
  for (i in 1:h){
    x.T                = c(1,as.vector(t(x.Ti)))
    Y.f                = rmvnorm(1, mean = x.T%*%A.posterior.draw, sigma=Sigma.posterior.draw)
    x.Ti               = rbind(Y.f,x.Ti[1:(p-1),])
    Y.h[i,,s]          = Y.f[1:N]
  }
}

```

Figure 6 presents a 3D visualization of the density intervals for the log Gold Future Prices and Log Nasdaq Index points. From past trends we could clearly see that negative correlations between those two. When the market suffered from dot-com and global financial crisis during 2000 to 2012, Gold futures increase sharply as expectation of risks, market returns fluctuations a lot. For next 24 months forecasting based on benchmark model, we could see Log gold future price more sharply increase then Log Nasdaq Index points. The varying heights of the intervals reflect the level of prediction certainty; as we project further into the future, the intervals become wider and more dispersed due to increased uncertainty.

```{r}

blue1 = "deepskyblue"            # #0000FF
blue2 = "lightblue"       # #ADD8E6
blue3 = "royalblue"       # #4169E1
blue4 = "blue"     # #00BFFF
blue5 = "dodgerblue"      # #1E90FF
blue6 = "steelblue"       # #4682B4

blue1.rgb   = col2rgb(blue1)
blue1.shade1= rgb(blue1.rgb[1],blue1.rgb[2],blue1.rgb[3], alpha=120, maxColorValue=255)


green1  = "#05386B"
green2  = "#379683"
green3  = "#5CDB95"
green4  = "#8EE4AF"
green5  = "#EDF5E1"
green6  = "darkgreen"

green3.rgb   = col2rgb(green3)
green3.shade1= rgb(green3.rgb[1],green3.rgb[2],green3.rgb[3], alpha=120, maxColorValue=255)
```

```{r}
#| echo: false
#| message: false
#| warning: false

par(mfcol = c(1, 2), mar=c(2,2,2,2)-0.1)

# Log gold forecasting 
limits.Lgold      = range(Y.h[,1,])
point.Lgold.f     = apply(Y.h[,1,],1,mean)

interval.Lgold.f  = apply(Y.h[,1,],1,hdi,credMass=0.90)
theta            = 180
phi              = 15.5

x                = seq(from=limits.Lgold[1], to=limits.Lgold[2], length.out=100)
z                = matrix(NA,h,99)
for (i in 1:h){
  z[i,]          = hist(Y.h[i,1,], breaks=x, plot=FALSE)$density
}
x                = hist(Y.h[i,1,], breaks=x, plot=FALSE)$mids
yy               = 1:h
z                = t(z)

f4               = persp3D(x=x, y=yy, z=z, phi=phi, theta=theta, 
                           xlab="\ngold[t+h|t]", ylab="h", zlab="\npredictive densities of cpi",
                           shade=NA, border=NA, ticktype="detailed", nticks=3,cex.lab=1,
                           col=NA,plot=FALSE)
perspbox (x=x, y=yy, z=z, bty="f", col.axis="black", phi=phi, theta=theta, 
          xlab="\nlog.gold[t+h|t]", ylab="h", zlab="\npredictive densities of gold", 
          ticktype="detailed", nticks=3,cex.lab=1, col = NULL, plot = TRUE)

polygon3D(x=c(interval.Lgold.f[1,],interval.Lgold.f[2,h:1]),  y=c(1:h,h:1), 
          z=rep(0,2*h), 
          col = blue2, NAcol = blue5, border = NA, add = TRUE, plot = TRUE)

for (i in 1:h){
  f4.l = trans3d(x=x, y=yy[i], z=z[,i], pmat=f4)
  lines(f4.l, lwd=1, col=blue5)
}
f4.l1 = trans3d(x=point.Lgold.f, y=yy, z=0, pmat=f4)
lines(f4.l1, lwd=1.5, col=blue4)

# Log NASDAQ INDEX forecasting 
limits.LNasIndex      = range(Y.h[,5,])
point.LNasIndex.f     = apply(Y.h[,5,],1,mean)

interval.LNasIndex.f  = apply(Y.h[,5,],1,hdi,credMass=0.90)
theta            = 180
phi              = 15.5

x                = seq(from=limits.LNasIndex[1], to=limits.LNasIndex[2], length.out=100)
z                = matrix(NA,h,99)
for (i in 1:h){
  z[i,]          = hist(Y.h[i,5,], breaks=x, plot=FALSE)$density
}
x                = hist(Y.h[i,5,], breaks=x, plot=FALSE)$mids
yy               = 1:h
z                = t(z)

f4               = persp3D(x=x, y=yy, z=z, phi=phi, theta=theta, 
                           xlab="\nLNasIndex[t+h|t]", ylab="h", zlab="\npredictive densities of LNasIndex",
                           shade=NA, border=NA, ticktype="detailed", nticks=3,cex.lab=1,
                           col=NA,plot=FALSE)
perspbox (x=x, y=yy, z=z, bty="f", col.axis="black", phi=phi, theta=theta, 
          xlab="\nlog.NasIndex[t+h|t]", ylab="h", zlab="\npredictive densities of LNasIndex", 
          ticktype="detailed", nticks=3,cex.lab=1, col = NULL, plot = TRUE)

polygon3D(x=c(interval.LNasIndex.f[1,],interval.LNasIndex.f[2,h:1]),  y=c(1:h,h:1), 
          z=rep(0,2*h), 
          col = blue2, NAcol = blue5, border = NA, add = TRUE, plot = TRUE)

for (i in 1:h){
  f4.l = trans3d(x=x, y=yy[i], z=z[,i], pmat=f4)
  lines(f4.l, lwd=1, col=blue5)
}
f4.l1 = trans3d(x=point.LNasIndex.f, y=yy, z=0, pmat=f4)
lines(f4.l1, lwd=1.5, col=blue4)

```

Figure 6 3D forecasting graph on basic model

```{r}
lgold.point.f    = apply(Y.h[,1,],1,mean)
lgold.interval.f = apply(Y.h[,1,],1,hdi,credMass=0.99)
lgold.range      = range(y[,1],lgold.interval.f)

par(mfrow=c(1,1), mar=rep(3,4),cex.axis=1.5)
plot(1:(length(y[,1])+h),c(y[,1],lgold.point.f), type="l", ylim=lgold.range, axes=FALSE, xlab="", ylab="", lwd=2, col=blue1)
axis(1,c(3,51,111,171,231,267,nrow(y),nrow(y)+h),c("2000","2005","2010","2015","2020","2023","",""), col=blue1)
axis(2,c(lgold.range[1],mean(lgold.range),lgold.range[2]),c("","lgold",""), col=blue1)
abline(v=284, col=blue1)
polygon(c(length(y[,1]):(length(y[,1])+h),(length(y[,1]):(length(y[,1])+h))[21:1]),
        c(y[284,1],lgold.interval.f[1,],lgold.interval.f[2,20:1],y[284,1]),
        col=blue1.shade1, border=blue1.shade1)

LNasIndex.point.f    = apply(Y.h[,5,],1,mean)
LNasIndex.interval.f = apply(Y.h[,5,],1,hdi,credMass=0.99)
LNasIndex.range      = range(y[,5],LNasIndex.interval.f)

par(mfrow=c(1,1), mar=rep(3,4),cex.axis=1.5)
plot(1:(length(y[,5])+h),c(y[,5],LNasIndex.point.f), type="l", ylim=LNasIndex.range, axes=FALSE, xlab="", ylab="", lwd=2, col=blue1)
axis(1,c(3,51,111,171,231,267,nrow(y),nrow(y)+h),c("2000","2005","2010","2015","2020","2023","",""), col=blue1)
axis(2,c(LNasIndex.range[1],mean(LNasIndex.range),LNasIndex.range[2]),c("","LNasIndex",""), col=blue1)
abline(v=284, col=blue1)
polygon(c(length(y[,5]):(length(y[,5])+h),(length(y[,5]):(length(y[,5])+h))[21:1]),
        c(y[284,1],LNasIndex.interval.f[1,],LNasIndex.interval.f[2,20:1],y[284,1]),
        col=blue1.shade1, border=blue1.shade1)

```

Figure 7 Basic Model Key Data Plot

```{r}

combined_range = range(c(y[,1], y[,5], lgold.interval.f, LNasIndex.interval.f))

par(mfrow=c(1,1), mar=rep(3,4), cex.axis=1.5)

plot(1:(length(y[,1])+h), c(y[,1], lgold.point.f), type="l", ylim=combined_range, col="blue", lwd=2, xlab="", ylab="")
polygon(c(length(y[,1]):(length(y[,1])+h), (length(y[,1]):(length(y[,1])+h))[21:1]),
        c(y[284,1], lgold.interval.f[1,], lgold.interval.f[2,20:1], y[284,1]),
        col=rgb(0, 0, 255, 100, max=255), border=rgb(0, 0, 255, 100, max=255))

lines(1:(length(y[,5])+h), c(y[,5], LNasIndex.point.f), type="l", ylim=combined_range, col="red", lwd=2, xlab="", ylab="")
polygon(c(length(y[,5]):(length(y[,5])+h), (length(y[,5]):(length(y[,5])+h))[21:1]),
        c(y[284,5], LNasIndex.interval.f[1,], LNasIndex.interval.f[2,20:1], y[284,5]),
        col=rgb(255, 0, 0, 100, max=255), border=rgb(255, 0, 0, 100, max=255))
abline(v=284, col=green6)

```

### Extension Model

```{r}

# setup 
kappa.1           = 0.01                                # shrinkage for A1 to Ap
kappa.2           = 100                              # shrinkage for constant 
S1                = 500                              # determine the burn-in draws
S2                = 9500                             # number of draws from the final simulation
total_S           = S1+S2
A.posterior       = array(NA, dim = c((1+N*p),N,S1+S2))
Sigma.posterior   = array(NA, dim = c(N,N,S1+S2))

#lambda.posterior     = diag(5,nrow(Y),nrow(Y))
lambda.posterior  = matrix(NA, S1+S2, 1)

lambda.posterior[1] = 2
# Prior Gamma distribution: k, theta
lambda.priors = list(
  alpha = 2
)

```

```{r}
#| echo: false
#| message: false
#| warning: false

## Function Applying
posterior.extend.draws     = posterior.draws.exten(total_S = total_S, Y=Y, X=X, A.prior, V.prior, S.prior, nu.prior, lambda.priors)
A.posterior.ext.simu       = posterior.extend.draws[["A.posterior.exten"]]
Sigma.posterior.ext.simu   = posterior.extend.draws[["Sigma.posterior.exten"]]

```

```{r}
#| echo: false
#| message: false
#| warning: false
## two-year ahead forecasting h=8
# set up
h                 = 24
S                 = 9000
Y.h.ext           = array(NA,c(h,N,S))


# sampling predictive density
for (s in 1:S){
  A.posterior.draw         = A.posterior.ext.simu[,,s]
  Sigma.posterior.draw     = Sigma.posterior.ext.simu[,,s]
    x.Ti                   = Y[(nrow(Y)-p+1):nrow(Y),]
    x.Ti                   = x.Ti[p:1,]
  for (i in 1:h){
    x.T                    = c(1,as.vector(t(x.Ti)))
    Y.f                    = rmvnorm(1, mean = x.T%*%A.posterior.draw, sigma=Sigma.posterior.draw)
      x.Ti                 = rbind(Y.f,x.Ti[1:(p-1),])
    Y.h.ext[i,,s]          = Y.f[1:N]
  }
}
```

Figure 8 presents a 3D visualization of the density intervals for the log gold future price and Log Nasdaq Index points calculated by the extension model. The results are generally similar to those of the basic model, with the only difference being that the intervels are much smaller then basic ones.

```{r}
# Need recheck coding latter
```

```{r}
#| echo: false
#| message: false
#| warning: false

par(mfcol = c(1, 2), mar=c(2,2,2,2)-0.1)

# Log CPI forecasting 
limits.lcpi.ext      = range(Y.h.ext[,1,])
point.lcpi.f.ext     = apply(Y.h.ext[,1,],1,mean)

interval.lcpi.f.ext  = apply(Y.h.ext[,1,],1,hdi,credMass=0.90)

x                = seq(from=limits.lcpi.ext[1], to=limits.lcpi.ext[2], length.out=100)
z                = matrix(NA,h,99)
for (i in 1:h){
  z[i,]          = hist(Y.h.ext[i,1,], breaks=x, plot=FALSE)$density
}
x                = hist(Y.h.ext[i,1,], breaks=x, plot=FALSE)$mids
yy               = 1:h
z                = t(z)

f4               = persp3D(x=x, y=yy, z=z, phi=phi, theta=theta, 
                           xlab="\ncpi[t+h|t]", ylab="h", zlab="\npredictive densities of cpi",
                           shade=NA, border=NA, ticktype="detailed", nticks=3,cex.lab=1,
                           col=NA,plot=FALSE)
perspbox (x=x, y=yy, z=z, bty="f", col.axis="black", phi=phi, theta=theta, 
          xlab="\nlog.cpi[t+h|t]", ylab="h", zlab="\npredictive densities of cpi", 
          ticktype="detailed", nticks=3,cex.lab=1, col = NULL, plot = TRUE)

polygon3D(x=c(interval.lcpi.f.ext[1,],interval.lcpi.f.ext[2,h:1]), 
           y=c(1:h,h:1), 
          z=rep(0,2*h), 
          col = blue2, NAcol = blue5, border = NA, add = TRUE, plot = TRUE)

for (i in 1:h){
  f4.l = trans3d(x=x, y=yy[i], z=z[,i], pmat=f4)
  lines(f4.l, lwd=1, col=blue3)
}
f4.l1 = trans3d(x=point.lcpi.f.ext, y=yy, z=0, pmat=f4)
lines(f4.l1, lwd=1.5, col=green1)

# Expected inflation rate forecasting 
limits.infexp.ext      = range(Y.h.ext[,2,])
point.infexp.f.ext     = apply(Y.h.ext[,2,],1,mean)

interval.infexp.f.ext  = apply(Y.h.ext[,2,],1,hdi,credMass=0.90)

x                = seq(from=limits.infexp.ext[1], to=limits.infexp.ext[2], length.out=100)
z                = matrix(NA,h,99)
for (i in 1:h){
  z[i,]          = hist(Y.h.ext[i,2,], breaks=x, plot=FALSE)$density
}
x                = hist(Y.h.ext[i,2,], breaks=x, plot=FALSE)$mids
yy               = 1:h
z                = t(z)

f4               = persp3D(x=x, y=yy, z=z, phi=phi, theta=theta, 
                           xlab="\ninfexp[t+h|t]", ylab="h", zlab="\npredictive densities of infexp",
                           shade=NA, border=NA, ticktype="detailed", nticks=3,cex.lab=1,
                           col=NA,plot=FALSE)
perspbox (x=x, y=yy, z=z, bty="f", col.axis="black", phi=phi, theta=theta, 
          xlab="\ninfexp[t+h|t]", ylab="h", zlab="\npredictive densities of infexp", 
          ticktype="detailed", nticks=3,cex.lab=1, col = NULL, plot = TRUE)

polygon3D(x=c(interval.infexp.f.ext[1,],interval.infexp.f.ext[2,h:1]), 
          y=c(1:h,h:1), z=rep(0,2*h), 
          col = green5, NAcol = green2, border = NA, add = TRUE, plot = TRUE)

for (i in 1:h){
  f4.l = trans3d(x=x, y=yy[i], z=z[,i], pmat=f4)
  lines(f4.l, lwd=1, col=green6)
}
f4.l1 = trans3d(x=point.infexp.f.ext, y=yy, z=0, pmat=f4)
lines(f4.l1, lwd=1.5, col=green1)
```

Figure 8 3d forecasting graph on extension model

```{r}
lgold.point.f    = apply(Y.h.ext[,1,],1,mean)
lgold.interval.f = apply(Y.h.ext[,1,],1,hdi,credMass=0.90)
lgold.range      = range(y[,1],lgold.interval.f)

par(mfrow=c(1,1), mar=rep(3,4),cex.axis=1.5)
plot(1:(length(y[,1])+h),c(y[,1],lgold.point.f), type="l", ylim=lgold.range, axes=FALSE, xlab="", ylab="", lwd=2, col=blue1)
axis(1,c(3,51,111,171,231,267,nrow(y),nrow(y)+h),c("2000","2005","2010","2015","2020","2023","",""), col=blue1)
axis(2,c(lgold.range[1],mean(lgold.range),lgold.range[2]),c("","lgold",""), col=blue1)
abline(v=284, col=blue1)
polygon(c(length(y[,1]):(length(y[,1])+h),(length(y[,1]):(length(y[,1])+h))[21:1]),
        c(y[284,1],lgold.interval.f[1,],lgold.interval.f[2,20:1],y[284,1]),
        col=blue1.shade1, border=blue1.shade1)

LNasIndex.point.f    = apply(Y.h.ext[,5,],1,mean)
LNasIndex.interval.f = apply(Y.h.ext[,5,],1,hdi,credMass=0.90)
LNasIndex.range      = range(y[,5],LNasIndex.interval.f)

par(mfrow=c(1,1), mar=rep(3,4),cex.axis=1.5)
plot(1:(length(y[,5])+h),c(y[,5],LNasIndex.point.f), type="l", ylim=LNasIndex.range, axes=FALSE, xlab="", ylab="", lwd=2, col=blue1)
axis(1,c(3,51,111,171,231,267,nrow(y),nrow(y)+h),c("2000","2005","2010","2015","2020","2023","",""), col=blue1)
axis(2,c(LNasIndex.range[1],mean(LNasIndex.range),LNasIndex.range[2]),c("","LNasIndex",""), col=blue1)
abline(v=284, col=blue1)
polygon(c(length(y[,5]):(length(y[,5])+h),(length(y[,5]):(length(y[,5])+h))[21:1]),
        c(y[284,1],LNasIndex.interval.f[1,],LNasIndex.interval.f[2,20:1],y[284,1]),
        col=blue1.shade1, border=blue1.shade1)
```

```{r}
combined_range = range(c(y[,1], y[,5], lgold.interval.f, LNasIndex.interval.f))

par(mfrow=c(1,1), mar=rep(3,4), cex.axis=1.5)

plot(1:(length(y[,1])+h), c(y[,1], lgold.point.f), type="l", ylim=combined_range, col="blue", lwd=2, xlab="", ylab="")
polygon(c(length(y[,1]):(length(y[,1])+h), (length(y[,1]):(length(y[,1])+h))[21:1]),
        c(y[284,1], lgold.interval.f[1,], lgold.interval.f[2,20:1], y[284,1]),
        col=rgb(0, 0, 255, 100, max=255), border=rgb(0, 0, 255, 100, max=255))

lines(1:(length(y[,5])+h), c(y[,5], LNasIndex.point.f), type="l", ylim=combined_range, col="red", lwd=2, xlab="", ylab="")
polygon(c(length(y[,5]):(length(y[,5])+h), (length(y[,5]):(length(y[,5])+h))[21:1]),
        c(y[284,5], LNasIndex.interval.f[1,], LNasIndex.interval.f[2,20:1], y[284,5]),
        col=rgb(255, 0, 0, 100, max=255), border=rgb(255, 0, 0, 100, max=255))
abline(v=284, col=green6)

```

# References {.unnumbered}
