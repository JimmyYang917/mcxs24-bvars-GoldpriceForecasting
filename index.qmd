---
title: "An Evidence-based Forecast: Gold as a Traditional Safe-Haven Investment"
author: "Yuqiao Yang"
format:
  html:
    toc: true
    toc-location: left
    number-sections: true
    
execute:
  echo: false
  
bibliography: references.bib
---

> **Abstract.** This research aims to explore future trends in Gold prices as a traditional safe-haven investment using a Bayesian VARs model. In the wake of the 2008 financial crisis and especially the 2019 global Covid-19 pandemic, the world economy appears to be on the brink of a looming risk: a world-wide economic recession. The concern over the risk of investment returns has become a primary focus for global investors and financial institutions. This unease has been further exacerbated by geopolitical conflicts such as the Russia-Ukraine war (2022) and the Israeli-Palestinian conflict (2023). Consequently, this research aims to provide a briefly discussion and data-driven forecast of traditional safe-haven assets: Gold, under current circumstances. Factors considered include emerging safe-haven investments, risk-free investment assets, comparable investments, market returns, inflation on both demand and supply sides, broad money supply (M2), interest rates, unemployment rates and market volatility.
>
> **Keywords.** Bayesian VARs, Gold price, Inflation, Interest rate, Unemployment, US Bond Yield, Safe-haven Assets, Forecasting, Volatility, R, Quarto.

# Introduction

**Objective:**

This research project aims to provide a monthly based, data-driven forecast of Gold price(USD) in two years, utilizing a Bayesian VARs model.

**Question:**

Gold as a traditional safe-haven asset, what are the anticipated price movements for the next year or beyond within the current environment?

**Introduction:**

Ulrich Beck introduced the concept of the risk society in the late 20th century, highlighting how humans confront entirely different systemic risks and face challenges in risk allocation under industrial society. Concurrently, globalization has reshaped the world and transformed human perceptions and experiences. Increasingly, evidence suggests that humanity is entering the risk society as described by Beck. Globalization encompasses not only economic, finance, technologe, and culture, but also risks. A China's financial exchange restriction might influence Australia's housing prices, while decisions made by the U.S. Central Bank regarding interest rates can prompt Western central banks to follow suit simultaneously. Following the 2008 financial crisis, major economies mitigated its aftermath significantly through quantitative easing monetary policies. This injection of substantial liquidity propelled economic growth steadily, leading to unprecedented prosperity in specific industries. However, the limitations of quantitative easing became apparent as the Covid-19 pandemic drew to a close. The United States and Western countries experienced unprecedented hyperinflation, coupled with indicators such as rising unemployment rates inversely correlated with inflation, conflicting long-term government bond yields with short-term treasuries, and record-breaking composite indices, signaling an impending global recession.

Geopolitical conflicts, such as the Russia-Ukraine war and the Israeli-Palestinian conflict, have intensified various risks. Disruptions in oil supply, blockages in key waterways, and logistical challenges in transporting goods and agricultural products have further exacerbated commodity price hikes and inflationary pressures. Consequently, mitigating or hedging investment risks has become the primary focus for global investors and financial institutions.

Among various safe-haven investments, gold has regained popularity as a hedge against uncertainty. This research aims to provide investors with a data-supported prediction of gold price trends over the next two years using Bayesian VAR models. The study incorporates information on comparable and emerging hedging products, market risks, returns, unemployment rates, inflation, interest rates, and other relevant parameters to construct a robust Bayesian VAR model. Ultimately, this research aids investors in identifying gold price trends and confidence intervals under different uncertainties within the current environment, thereby mitigating investment risks effectively.

# Data and Data Properties

To enhance the accuracy of gold price predictions, a total selection of 12 variables has been chosen, encompassing gold competitors, risk-free assets, and the Nasdaq index. From a broader macroeconomic standpoint, the variables also include inflation, the Producer Price Index (PPI), unemployment rates, crude oil prices, volatility indices, the dollar index, changes in the M2 money supply level, and federal fund effective rates.

-   $GoldFutures_{t}$ : Gold future price in USD per ounce, as considering the expectations in further gold price movements and high liquidity safe haven currency than real product.

-   Competitors and Substitutes for Gold ：

    -   Risk-free assets: treasury bonds
        -   $13WeekNotes_{t}$ : Considering as short-term risk free assets return. More time using in short term risk hedging in portfolio.
        -   $Tbill(5Year)_{t}$ : Considering as mid-term risk free assets return.
        -   $Tbill(10Year)_{t}$ : Considering as long-term risk free assets return.

-   Market returns as opposed to safe-haven investments：

    -   $NasdaqIndex_{t}$ ：Index that include all stocks available in NASDAQ to present the market returns.

-   Macro environment：

    -   $M2_{t}$ : the Board money supply of United States cause price index goes up.
    -   $Infla_{t}$ : CPI Index that present whole price level changes of all goods and services in US. Gold price are highly correlated with inflation.
    -   $Unemp_{t}$ : unemployment rate provide by Bureau of Statistic of US to present a general environment of Labor market.
    -   $CrudeOil_{t}$ : to present as basic cost of production.
    -   $FFERs_{t}$ : As the Federal Funds Effective Rates aim to lower the inflation, decrease the M2 level.
    -   $USDIndex_{t}$ : to present as the purchasing power of USD cross world-wide. That increase the total amount of investors and financial institutions come into US market to earn higher returns.
    -   $VolatilityIndex_{t}$ : to present the risk cross whole market and expectations of further coming events.

```{r}
rm(list=ls())
```

```{r global options}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```

```{r Library}
library(readabs)
library(fredr)
library(blscrapeR)
library(readrba)
library(xts)
library(fUnitRoots)   # ADF test - adfTest
library(tidyverse)    # for table
library(kableExtra)   # for print table
library(dplyr)
library(corrplot)
library(tseries) # for adf test
library(GIGrvg)
library(zoo)
library(ggplot2)      # plot in same graph
library(GIGrvg)       # GIG distribution
library(mvtnorm)      # Bvars forecast
library(plot3D)
library(HDInterval)   # hdi plot
library(grDevices)
library(gridExtra)
library(mgcv)
set.seed(123)#set random seed for test
```

```{r}

blue1 = "deepskyblue"            # #0000FF
blue2 = "lightblue"       # #ADD8E6
blue3 = "royalblue"       # #4169E1
blue4 = "blue"     # #00BFFF
blue5 = "dodgerblue"      # #1E90FF
blue6 = "steelblue"       # #4682B4

blue1.rgb   = col2rgb(blue1)
blue1.shade1= rgb(blue1.rgb[1],blue1.rgb[2],blue1.rgb[3], alpha=120, maxColorValue=255)


green1  = "#05386B"
green2  = "#379683"
green3  = "#5CDB95"
green4  = "#8EE4AF"
green5  = "#EDF5E1"
green6  = "darkgreen"

green3.rgb   = col2rgb(green3)
green3.shade1= rgb(green3.rgb[1],green3.rgb[2],green3.rgb[3], alpha=120, maxColorValue=255)
```

```{r}

Data_collection <- function(data_download) {
  data           = data.frame(data_download[,1], data_download[,6])
  colnames(data) = c('date','price')
  
  data$date      = as.Date(as.character(data$date),format="%Y-%m-%d") 
  data <- data %>%
    filter(date >= as.Date("2000-08-01"))
  data <- data[data$price != "null", ]
  #data$price[data$price == "null"] <- NA
  #data           = na.omit(data)
  data           = xts(data$price, data$date)
  data_finally           = to.monthly(data, OHLC = FALSE)
  return(data_finally)
}

```

```{r}

Data_collection_US <- function(data_download) {
  data           = data.frame(data_download[,1], data_download[,2])
  colnames(data) = c('date','price')
  
  data$date      = as.Date(as.character(data$date),format="%Y-%m-%d") 
  data <- data %>%
    filter(date >= as.Date("2000-08-01"))
  data <- data[data$price != "null", ]
  #data$price[data$price == "null"] <- NA
  #data           = na.omit(data)
  data           = xts(data$price, data$date)
  data_finally           = to.monthly(data, OHLC = FALSE)
  return(data_finally)
}

```

```{r}

Date_collection <- function(data) {
  correct_dates       = seq(as.Date("2000-09-01"), by="month", length.out=length(data))
  correct_dates       = floor_date(correct_dates, "month") - days(1)
  return(correct_dates)
}

```

```{r}
# 1. Gold futures price in USD per ounce (data periods: 2000-8 to recent)
#daily data download

gold_link = "https://query1.finance.yahoo.com/v7/finance/download/GC%3DF?period1=967608000&period2=1715586098&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true" 
gold_download       = read.csv(gold_link)
gold_data           = Data_collection(gold_download)
index(gold_data)    = Date_collection(gold_data)


# 2. 13 Weeks treasury bond yield (data periods: 1960-1 to recent)
bond_yield_13weeks = "https://query1.finance.yahoo.com/v7/finance/download/%5EIRX?period1=-315312000&period2=1715610633&interval=1d&events=history&includeAdjustedClose=true"
bond_yield_13weeks_file = read.csv(bond_yield_13weeks)
bond_yield_13weeks_data           = Data_collection(bond_yield_13weeks_file)
index(bond_yield_13weeks_data)    = Date_collection(bond_yield_13weeks_data)

# 3. 5-Year treasury bond yield (data periods: 1962-2 to recent)
bond_yield_5years = "https://query1.finance.yahoo.com/v7/finance/download/%5EFVX?period1=-252326400&period2=1715610588&interval=1d&events=history&includeAdjustedClose=true"
bond_yield_5years_file = read.csv(bond_yield_5years)
bond_yield_5years_data           = Data_collection(bond_yield_5years_file)
index(bond_yield_5years_data)    = Date_collection(bond_yield_5years_data)

# 4. 10-Year treasury bond yield (data periods: 1962-2 to recent) TNX
bond_yield_10years = "https://query1.finance.yahoo.com/v7/finance/download/%5ETNX?period1=-252326400&period2=1715610541&interval=1d&events=history&includeAdjustedClose=true"
bond_yield_10_years = read.csv(bond_yield_10years)
bond_yield_10_years_data           = Data_collection(bond_yield_10_years)
index(bond_yield_10_years_data)    = Date_collection(bond_yield_10_years_data)


# 5. Nasdaq Index (data periods: 1971-2 to recent) ^IXIC
Nasdaq_IXIC = "https://query1.finance.yahoo.com/v7/finance/download/%5EIXIC?period1=34612200&period2=1715610493&interval=1d&events=history&includeAdjustedClose=true"
Nasdaq_IXIC_file = read.csv(Nasdaq_IXIC)
Nasdaq_IXIC_data           = Data_collection(Nasdaq_IXIC_file)
index(Nasdaq_IXIC_data)    = Date_collection(Nasdaq_IXIC_data)

# 6. M2 level
M2_rate = "https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23e1e9f0&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=on&txtcolor=%23444444&ts=12&tts=12&width=1318&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=WM2NS&scale=left&cosd=1980-11-03&coed=2024-05-04&line_color=%234572a7&link_values=false&line_style=solid&mark_type=none&mw=3&lw=2&ost=-99999&oet=99999&mma=0&fml=a&fq=Monthly&fam=eop&fgst=lin&fgsnd=2020-02-01&line_index=1&transformation=pch&vintage_date=2024-05-13&revision_date=2024-05-13&nd=1980-11-03"
M2_rate_file = read.csv(M2_rate)
M2_rate_data           = Data_collection_US(M2_rate_file)
index(M2_rate_data)    = Date_collection(M2_rate_data)

# 7. Inflation rate （******Questions******）
Inflation_rate = "https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23e1e9f0&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=on&txtcolor=%23444444&ts=12&tts=12&width=1318&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=CPIAUCSL&scale=left&cosd=1947-01-01&coed=2024-05-01&line_color=%234572a7&link_values=false&line_style=solid&mark_type=none&mw=3&lw=2&ost=-99999&oet=99999&mma=0&fml=a&fq=Monthly&fam=avg&fgst=lin&fgsnd=2020-02-01&line_index=1&transformation=pc1&vintage_date=2024-05-13&revision_date=2024-05-13&nd=1947-01-01"

Inflation_rate_file = read.csv(Inflation_rate)
Inflation_rate_data           = Data_collection_US(Inflation_rate_file)
index(Inflation_rate_data)    = Date_collection(Inflation_rate_data)

# 8. Unemployment rate
unemployment_rate = "https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23e1e9f0&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=on&txtcolor=%23444444&ts=12&tts=12&width=1318&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=UNRATE&scale=left&cosd=1948-01-01&coed=2024-05-01&line_color=%234572a7&link_values=false&line_style=solid&mark_type=none&mw=3&lw=2&ost=-99999&oet=99999&mma=0&fml=a&fq=Monthly&fam=avg&fgst=lin&fgsnd=2020-02-01&line_index=1&transformation=lin&vintage_date=2024-05-13&revision_date=2024-05-13&nd=1948-01-01"
unemployment_rate_file = read.csv(unemployment_rate)
unemployment_rate_data           = Data_collection_US(unemployment_rate_file)
index(unemployment_rate_data)    = Date_collection(unemployment_rate_data)


# 9. Crude oil price
Crude_oil_price = "https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23e1e9f0&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=off&txtcolor=%23444444&ts=12&tts=12&width=1318&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=DCOILBRENTEU&scale=left&cosd=1990-01-01&coed=2024-05-13&line_color=%234572a7&link_values=false&line_style=solid&mark_type=none&mw=3&lw=2&ost=-99999&oet=99999&mma=0&fml=a&fq=Monthly&fam=eop&fgst=lin&fgsnd=2020-02-01&line_index=1&transformation=lin&vintage_date=2024-05-13&revision_date=2024-05-13&nd=1987-05-20"
Crude_oil_price_file = read.csv(Crude_oil_price)
Crude_oil_price_data           = Data_collection_US(Crude_oil_price_file)
index(Crude_oil_price_data)    = Date_collection(Crude_oil_price_data)


# 10. Federal Funds Effective Rates
Federal_funds_rate = "https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23e1e9f0&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=on&txtcolor=%23444444&ts=12&tts=12&width=1318&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=FEDFUNDS&scale=left&cosd=1990-01-01&coed=2024-05-13&line_color=%234572a7&link_values=false&line_style=solid&mark_type=none&mw=3&lw=2&ost=-99999&oet=99999&mma=0&fml=a&fq=Monthly&fam=avg&fgst=lin&fgsnd=2020-02-01&line_index=1&transformation=lin&vintage_date=2024-05-13&revision_date=2024-05-13&nd=1954-07-01"
Federal_funds_rate_file = read.csv(Federal_funds_rate)
Federal_funds_rate_data           = Data_collection_US(Federal_funds_rate_file)
index(Federal_funds_rate_data)    = Date_collection(Federal_funds_rate_data)

# 11. US dollars Index DX-Y.NYB
US_dollars_Index = "https://query1.finance.yahoo.com/v7/finance/download/DX-Y.NYB?period1=31813200&period2=1715611000&interval=1d&events=history&includeAdjustedClose=true"
US_dollars_Index_file = read.csv(US_dollars_Index)
US_dollars_Index_data           = Data_collection(US_dollars_Index_file)
index(US_dollars_Index_data)    = Date_collection(US_dollars_Index_data)

# 12. Volatility Index
Volatility_Index = "https://query1.finance.yahoo.com/v7/finance/download/%5EVIX?period1=631267200&period2=1715610192&interval=1d&events=history&includeAdjustedClose=true"
Volatility_Index_file = read.csv(Volatility_Index)
Volatility_Index_data          = Data_collection(Volatility_Index_file)
index(Volatility_Index_data)   = Date_collection(Volatility_Index_data)

```

```{r Data Combine, fig.pos="H"}

# All Variables
all_data             = merge( gold_data,               bond_yield_13weeks_data, 
                                      bond_yield_5years_data,  bond_yield_10_years_data,  
                                      Nasdaq_IXIC_data,        M2_rate_data,            
                                      Inflation_rate_data,     unemployment_rate_data,  
                                      Crude_oil_price_data,    Federal_funds_rate_data, 
                                      US_dollars_Index_data,   Volatility_Index_data)

colnames(all_data)   = c("GFutures_data",    "TBill13W_data", 
                         "TBill5Y_data",     "TBill10Y_data", 
                         "Nasdaq_data",      "M2_data", 
                         "InflaR_data",      "UnempR_data",
                         "CrudeOil_data",    "InterestR_data",
                         "USDIndex_data",    "Volatility_data")
all_data <- all_data[complete.cases(all_data), ]
```

After download all variables as designed, merge all data sets into a new frame and change the column names. The date of data used in this research will start at 2004-08-31 as at 2024-03-31.

# Plots of all variables

First plot all 12 variables we have:

```{r}
data1 <- subset(all_data)

par(mfrow = c(4, 4), mar=c(2,2,2,2))
for (i in 1:12) { 
  ts.plot(data1[, i], main = colnames(data1)[i], 
          ylab = "", xlab = "", col="red")
}

```

```{r}

# Choose the columns
selected_columns <- c("GFutures_data", "Nasdaq_data")#, "CrudeOil_data", "USDIndex_data", "Volatility_data")
selected_columns2 <- c("TBill13W_data", "TBill5Y_data", "TBill10Y_data", "M2_data", "InflaR_data", "UnempR_data", "InterestR_data")

selected_data <- data1[, selected_columns]
selected_data2 <- data1[, selected_columns2]

colors1 <- rainbow(length(selected_columns))
colors2 <- rainbow(length(selected_columns2))

# plot first variables
plot.xts(selected_data, main="Selected Time Series Data", col=colors1)
legend("topright", legend=names(selected_data), col=colors1, lty=1, cex=0.8, bty="n")

# plot second variables
plot.xts(selected_data2, main="Selected Rates Series Data", col=colors2, lwd=2)
legend("topright", legend=names(selected_data2), col=colors2, lty=1, cex=0.8, bty="n")
```

As above, we could have all 12 variables in visualization format individually. As a quick overlook, 13 weeks bond has a similar trend with federal funds effective rates; other 2 treasury bonds move quite same; gold future, Nasdaq, crude oil price and US dollars Index have upper ward trends; Unemployment rates seems like opposite to federal rates; M2 and other 2 time series seems like stationary on mean but variance change over time.

**Correlation Table**

```{r}
correlation_matrix <- cor(data1)

corrplot(correlation_matrix, method = "color", type = "upper", tl.srt = 45,tl.col = "black",tl.cex = 0.5)
```

A simple correlation provide us a basic understanding of those 12 variables. Gold future as safe-haven is highly positive correlated with Nasdaq Index.

Therefore, we could use ACF and PACF test to indicate whether there is autocorrelations.

```{r}
par(mfrow = c(4, 4), mar=c(2,2,2,2))
for (i in 1:12){
acf = acf(data1[,i], plot = FALSE)[1:12]
plot(acf, main = "")
title(main = paste(colnames(data1)[i]), line = 0.5)
}

par(mfrow = c(4, 4), mar=c(2,2,2,2))
for (i in 1:12){
pacf = pacf(data1[,i], plot = FALSE)[1:12]
plot(pacf, main = "")
title(main = paste(colnames(data1)[i]), line = 0.5)
}
```

Both ACF and PACF suggest that variables are highly autocorrelated except M2: Board money change rate. That might because the policy has changed in earlier 2020 and 2022. Therefore, we further need ADF test(unit root test) to feed our time series are stationary or not.

```{r}

adf_test <- list()
for (i in 1:12) {
  adf_result = adf.test(data1[,i], k = 4)
  adf_test[[i]] <- adf_result
}
adf_table <- data.frame(p_value = numeric(length(adf_test)))
adf_table <- data.frame(p_value = numeric(length(adf_test)))
for (i in 1:length(adf_test)) {adf_table[i, "p_value"] = round(adf_test[[i]]$p.value,3)
}

rownames(adf_table)<- c("GFutures_data",    "TBill13W_data", 
                         "TBill5Y_data",     "TBill10Y_data", 
                         "Nasdaq_data",      "M2_data", 
                         "InflaR_data",      "UnempR_data",
                         "CrudeOil_data",    "InterestR_data",
                         "USDIndex_data",    "Volatility_data")
colnames(adf_table)<- c("P-value")
knitr::kable(adf_table)
```

```{r}

columns_to_log <- c("GFutures_data", "Nasdaq_data", "CrudeOil_data", "USDIndex_data", "Volatility_data")

all_data[, columns_to_log] <- log(all_data[, columns_to_log] + 1e-6)

names(all_data)[1] <- "Lgold"
names(all_data)[5] <- "LNasIndex"
```

```{r}

selected_columns <- c("Lgold", "LNasIndex", "CrudeOil_data", "USDIndex_data", "Volatility_data")

selected_data <- all_data[, selected_columns]

colors1 <- rainbow(length(selected_columns))

plot.xts(selected_data, main="Log INDEX Data", col=colors1, type="l", lty=1, lwd=2, grid=T, major.ticks="years", minor.ticks="quarters", xlab="Date", ylab="Log Value")
legend("bottom", legend=colnames(selected_data), col=colors1, lty=1, lwd=2, cex=0.8, bty="n")

```

# Modeling and Hypothesis

This research project based on **Bayesian VARs(p) model** [@wozniakBsvarsBayesianEstimation2022]to forecast the Gold price in next two years. For time $t$ = {1,2,3,...,$T-1$,$T$} :

```{=tex}
\begin{aligned}
y_t &= \mu_0 + A_1y_{t-1} + A_2y_{t-2}...+A_py_{t-p} +\epsilon_t\\

\epsilon_t|Y_{t-1} &\sim iid \mathcal{N}_{12}(0_{12}, \Sigma)
\end{aligned}
```
Where N = 12 and $y_{t}$ is a vector of 12 variables at time $t$:

```{=tex}
\begin{aligned}

y_{t}=\begin{pmatrix}
GoldFutures_{t}\\
13WeekNotes_{t}\\
Treasurybill(5Year)_{t} \\
Treasurybill(10Year)_{t} \\
NasdaqIndex_{t} \\
M2_{t} \\
Inflation_{t} \\
Unemployment_{t}\\
CrudeOil_{t}\\
FFERs_{t} \\
USDollarIndex_{t}\\
VolatilityIndex_{t}\\
\end{pmatrix}

\end{aligned}
```
For time $t$ = 1,2,.....,$T$：

-   $y_t$ is a $N(12)\times 1$ vector of observations at time $t$
-   $\mu_0$ is a $N(12)\times 1$ vector of constant terms
-   $A_i$ is a $N(12)\times N(12)$ matrix of autoregressive slope parameters
-   $\epsilon_t$ is a $N(12)\times 1$ vector of error terms which is a multivariate white nose process(time invariant)
-   $Y_{t-1}$ is the information set collecting observations on y up to time $t-1$
-   $\Sigma$ is a $N(12)\times N(12)$ covariance matrix of the error term

## Matrix Notation for the model:

Matrix form are used to simplify the notation and the derivations. Let $T$ be the available sample size for the variable $y$ and $K$ be the sum of lags and constant term ($K = 1 + pN$). Define a identity matrix of order $T$, $I_T$, as well as following matrix:

```{=tex}
\begin{aligned}

A=
\begin{bmatrix}
\mu_{0}' \\ A_{1}' \\
A_{2} '\\.\\.\\.\\A_{p}'
\end{bmatrix}_{K \times N}

Y=
\begin{bmatrix}
y_{1}' \\ y_{2}' \\
y_{3} '\\.\\.\\.\\y_{T}'
\end{bmatrix}_{T \times N}

x_t=
\begin{bmatrix}
1 \\ y_{t-1}' \\
y_{t-2} '\\.\\.\\.\\y_{t-p}'
\end{bmatrix}_{K \times 1}

X=
\begin{bmatrix}
x_{1}' \\ x_{2}' \\
x_{3} '\\.\\.\\.\\x_{T}'
\end{bmatrix}_{T \times K}

E=
\begin{bmatrix}
\epsilon_1' \\ \epsilon_2' \\
\epsilon_3 '\\.\\.\\.\\\epsilon_T'
\end{bmatrix}_{T \times N}

\end{aligned}
```
Then the model can be written in a concise notation as:

```{=tex}
\begin{aligned}

Y &= X A +E 
\\
\\
E|X &\sim \mathcal{MN}_{T\times N}(\textbf{0},\Sigma,I_T)

\end{aligned}
```
Given that the density function of Matrix-variate Normal distribution $Z\sim \mathcal{MN}_{T\times N}(M,Q,P)$ is:

```{=tex}
\begin{aligned}

\mathcal{MN}_{T\times N}(M,Q,P) & =c^{-1}_{mn}exp\left\{ -\frac{1}{2}\textbf{tr}\left[ Q^{-1}\left( Z-M \right)'P^{-1}\left( Z-M \right) \right] \right\}\\
c_{mn} & = \left( 2\pi \right)^{\frac{TN}{2}}det\left( Q \right)^{\frac{T}{2}}det\left( P \right)^{\frac{N}{2}}

\end{aligned}
```
Base on the model above, we could first turn B Vars(p) model into B Vars(1) model and easily regress to have the parameter matrix. Then we could have a $t+h$ period forward forecasting with increase of variance, in this case: $h$ = 24.

The main focus of estimate output is the conditional mean of Gold price, which base on current information set $Y_{t-1}$. It provide the average mean prediction of Gold price which investors and financial institutions interested in. Moreover, 1 standard deviation and 2 standard deviation will also produced in forecasting process to provide a 68% and 95% of confidence intervals of future Gold price movements in $h$ periods base on current information set.

Furthermore, different prior distribution might be used to provide different level of uncertainty of current environment(information set). Compare the difference of Gold price under different priors could help to prove the Gold as a high quality safe-haven investment and increase investors and financial institutions confidence and further expectations. (Competitors for golds might also be used under different priors, such as Nasdaq Index and short to mid-term treasury bills.)

## Likelihood Function

The model equation imply the predictive density of the data vector $Y$. We could consider the model equation is the linear transformation of the matrix-variate normal distribution $E$. Therefore, the data vector also follows a matrix-variate normal distribution given by:

```{=tex}
\begin{aligned}

Y|X,A,\Sigma &\sim \mathcal{MN}_{T\times N}(XA,\Sigma,I_T)

\end{aligned}
```
This distribution determines the shape of the likelihood function that is defined as the sampling data density:

```{=tex}
\begin{aligned}

L(A,\Sigma | Y,X)\equiv P(Y|X,A,\Sigma)

\end{aligned}
```
The likelihood function for the parameters estimation ($A,\Sigma$), and after plugging in data in place of $Y,X$, is considered a function of parameters $A$ and $\Sigma$ is given by:

```{=tex}
\begin{aligned}

L(A,\Sigma | Y,X) &= \left( 2\pi \right)^{-\frac{TN}{2}}det\left( \Sigma \right)^{-\frac{T}{2}}exp\left\{ -\frac{1}{2}\sum_{t=1}^{T}\epsilon_{t}'\Sigma^{-1}\epsilon_{t} \right\}
\\
&= \left( 2\pi \right)^{-\frac{TN}{2}}det\left( \Sigma \right)^{-\frac{T}{2}}exp\left\{ -\frac{1}{2}\sum_{t=1}^{T} \left( y_{t}-A'x_{t} \right)'\Sigma^{-1}\left( y_{t}-A'x_{t} \right)\right\}
\\ 
&= \left( 2\pi \right)^{-\frac{TN}{2}}det\left( \Sigma \right)^{-\frac{T}{2}}exp\left\{ -\frac{1}{2}vec\left(\left( Y-XA \right)'\right)' \left( I_T\otimes \Sigma^{-1} vec\left(\left( Y-XA \right)'\right)\right)\right\}
\\
&= \left( 2\pi \right)^{-\frac{TN}{2}}det\left( \Sigma \right)^{-\frac{T}{2}}exp\left\{ -\frac{1}{2}\textbf{tr}\left[\Sigma^{-1}\left( Y-XA \right)'I^{-1}_T\left(Y-XA \right) \right] \right\}
\\
&= \left( 2\pi \right)^{-\frac{TN}{2}}det\left( \Sigma \right)^{-\frac{T}{2}}exp\left\{ -\frac{1}{2}\textbf{tr}\left[\Sigma^{-1}\left(Y-XA\right)'\left(Y-XA\right) \right] \right\}

\end{aligned}
```
Given that the trace:

-   $\textbf{tr}\left( X \right) = \sum_{n=1}^{N}X_{nn} \ \ \ \ \text{for a }N \times N\text{ matrix }X$
-   $\textbf{tr}\left( ABCD \right) = vec(D')'\left( C'\otimes A \right)vec(B)$

## Prior Distribution

For the given model, we assume that unknown parameters have following distributions[@stock2001]:

```{=tex}
\begin{aligned}

A|\Sigma &\sim \mathcal{MN}_{K\times N}(M,\Sigma,P) 
\\ 
\Sigma &\sim \mathcal{IW}_{N}(S,\nu)

\end{aligned}
```
where $A_{K \times N}|\Sigma$ follow a matrix-variate normal distribution:

\- $M$ is the mean of matrix normal distribution

\- $\Sigma_{N \times N}$is the row specific covariance matrix

\- $P_{K \times K}$ is the column specific covariance matrix with the density given by:

```{=tex}
\begin{aligned}

\mathcal{MN}_{K\times N}(M,\Sigma,P) & =c^{-1}_{mn}exp\left\{ -\frac{1}{2}\textbf{tr}\left[ \Sigma^{-1}\left( A-M \right)'P^{-1}\left( A-M \right) \right] \right\}\\
c_{mn} & = \left( 2\pi \right)^{\frac{KN}{2}}det\left( \Sigma \right)^{\frac{K}{2}}det\left( P \right)^{\frac{N}{2}}

\end{aligned}
```
And $\Sigma$ follow a Inverse Wishart distribution:

\- $S$ is N × N positive definite symmetric matrix called the scale matrix.

\- $\nu$ \> N + 2 denotes degrees of freedom. with the density given by:

```{=tex}
\begin{aligned}

\mathcal{IW}_{N}(S,\nu) &= c^{-1}_{iw}det(\Sigma)^{-\frac{\nu + N+1}{2}}exp\left\{ -\frac{1}{2} \textbf{tr}\left[ \Sigma^{-1} S\right]\right\}\\
c_{iw} &= 2^{\frac{\nu N}{2}}\pi^{\frac{N(N-1)}{4}}\prod_{n=1}^{N}\Gamma\left( \frac{\nu + n + 1}{2} \right)det(S)^{-\frac{\nu}{2}}

\end{aligned}
```
Then the joint distribution of ($A,\Sigma$) is Normal-Inverse Wishart:

```{=tex}
\begin{aligned}

P(A,\Sigma) & \sim \mathcal{NIW}_{K \times N}(M,P,S,\nu)

\end{aligned}
```
with the density given by:

```{=tex}
\begin{aligned}

P(A,\Sigma) &= c^{-1}_{nw}det(\Sigma)^{-\frac{\nu + N + K + 1}{2}}\\
&\times exp\left\{ -\frac{1}{2}\textbf{tr}\left[ \Sigma^{-1}\left( A-M \right)'P^{-1}\left( A-M \right) \right] \right\}\\
&\times exp\left\{ -\frac{1}{2} \textbf{tr}\left[ \Sigma^{-1} S\right]\right\}
\\
\\
c_{nw} &= 2^{\frac{N(K + \nu)}{2}}\pi^{\frac{N(N + 2K -1)}{4}}[\prod_{n=1}^{N}\Gamma\left( \frac{\nu + 1 - n}{2} \right)]det\left( P \right)^{\frac{N}{2}}det(S)^{-\frac{\nu}{2}}

\end{aligned}
```
### Natural-conjugate prior distribution

Leads to joint posterior distribution for ($A,\Sigma$) has the same form as prior:

```{=tex}
\begin{aligned}

P(A,\Sigma) &= P(A|\Sigma)P(\Sigma)
\\
A|\Sigma &\sim \mathcal{MN}_{K\times N}(\underline{A},\Sigma,\underline{V}) 
\\ 
\Sigma &\sim \mathcal{IW}_{N}(\underline{S}, \underline{\nu})

\end{aligned}
```
with the **kernel** given by:

```{=tex}
\begin{aligned}

P(A,\Sigma) &\propto \det(\Sigma)^{-\frac{N+K+\underline{\nu}+1}{2}} \\
&\times exp\{-\frac{1}{2}tr[\Sigma^{-1}(A-\underline{A}) \underline{V}^{-1}(A-\underline{A})]\} \\
&\times exp\{-\frac{1}{2}tr[\Sigma^{-1}\underline{S}]\}

\end{aligned}
```
## Benchmark model with Minnesota prior

In real life, macroeconomic variables are more likely being unit-root non stationary and are well-characterized by a multivariate random walk process：

```{=tex}
\begin{aligned}

y_{t} = y_{t-1} + \epsilon_t

\end{aligned}
```
Therefore, our benchmark model uses Minnesota prior(1984) based on random walk process for our Bayesian forecasting.

Set the prior mean $A$ to:

```{=tex}
\begin{aligned}

\underline{A}=\left[ 0_{N\times1} \ \ \ \  I_{N} \ \ \ \ \ 0_{N\times(p-1)N} \right]'

\end{aligned}
```
which means the mean of first lag equal to 1 and mean of constant term and other lags are 0.

Set the column specific prior covariance of $A$ (prior shrinkage)to:

```{=tex}
\begin{aligned}

\underline{V} = diag\left[ \kappa_{2} \quad \kappa_{1}(\textbf{P} ^{-2}\otimes \textbf{i}^{'}_{N}) \right]

\end{aligned}
```
where:

-   $\textbf{P}$ is the list of legs, $\textbf{P} \ \ \ =\left[ 1 \quad 2 \quad 3\quad ... \quad p \right]$

-   $\textbf{i}^{'}_{N}$ is a $N \times 1$ vector of ones

-   $\kappa_{1}$: overall shrinkage level for autoregressive slopes

-   $\kappa_{2}$: overall shrinkage for the constant term

therefore, we could have the variance of $A$:

$$
VAR\left[ vec\left( A \right) \right]\quad = \quad \Sigma \ \ \otimes \ \ \underline{V}
$$

### Bayesian Estimations

Based on Bayes' Theorem:

```{=tex}
\begin{aligned}

P(A|B) & =\frac{P(B|A)P(A)}{P(B)}

\end{aligned}
```
We could have the kernel of conditional joint posterior distribution $P(A,\Sigma|Y,X)$, which is the proportion of the product between conditional distribution of data $P(Y|X,A,\Sigma)$ and joint prior distribution $P(A,\Sigma)$:

```{=tex}
\begin{aligned}

P(A,\Sigma|Y,X) &= \frac{P(Y|X,A,\Sigma) \ P(A,\Sigma)}{P(Y)}\\
&\propto P(Y|X,A,\Sigma) \ P(A,\Sigma)\\
&\propto L(A,\Sigma | Y,X) \ P(A|\Sigma) \ P(\Sigma) \\\\

\end{aligned}
```
And the kernel of conditional joint posterior distribution is given by:

```{=tex}
\begin{aligned}

P(A,\Sigma|Y,X) &\propto L(A,\Sigma | Y,X) \ P(A,\Sigma)\\
&\propto det(\Sigma)^{-\frac{T}{2}} \\
&\times exp\{-\frac{1}{2}tr[\Sigma^{-1}(Y-XA)'(Y-XA)]\} \\
&\times \det(\Sigma)^{-\frac{N+K+\underline{\nu}+1}{2}} \\
&\times exp\{-\frac{1}{2}tr[\Sigma^{-1}(A-\underline{A}) \underline{V}^{-1}(A-\underline{A})]\} \\
&\times exp\{-\frac{1}{2}tr[\Sigma^{-1}\underline{S}]\}

\end{aligned}
```
Then, the kernel could be represent as the normal-inverse Wishart distribution[@wozniakBsvarsBayesianEstimation2022]:

```{=tex}
\begin{aligned}

P(A,\Sigma|Y,X) &\sim \mathcal{NIW}_{K\times N}(\bar{A},\bar{V},\bar{S},\bar{\nu})
\\
\\
\bar{V} &= (X'X + \underline{V}^{-1})^{-1} \\
\bar{A} &= \bar{V}(X'Y + \underline{V}^{-1}\underline{A}) \\
\bar{\nu} &= T + \underline{\nu}\\
\bar{S} &= \underline{S} + Y'Y + \underline{A}'\underline{V}^{-1}\underline{A} - \bar{A}'\bar{V}^{-1}\bar{A}

\end{aligned}
```
### Gibbs sampler: Function Proofing

Consider Bi-variate Gaussian random walk process:

$$
\begin{align}
y_t &= 
\begin{bmatrix}
y_{t,1} \\
y_{t,2}
\end{bmatrix} = 
\begin{bmatrix}
y_{t-1,1} \\
y_{t-1,2}
\end{bmatrix} + 
\begin{bmatrix}
\epsilon_{t,1} \\
\epsilon_{t,2}
\end{bmatrix} \\
\epsilon_{t,1} &\sim \mathcal{N}(0,1)  \\ 
\epsilon_{t,2} &\sim \mathcal{N}(0,1)
\end{align}
$$

Variables in Matrix Notation:

$$ 
Y = \begin{bmatrix}
y_2' \\
y_3' \\
\vdots \\
y_n'
\end{bmatrix},
\quad
X = \begin{bmatrix}
1 \quad y_1' \\
1 \quad y_2' \\
\vdots \quad \vdots \\
1 \quad y_{n-1}'
\end{bmatrix}
$$

Therefore, we could basic set up for this Bi-variate Gaussian random walk process with Minnesota prior:

```{r Basic Model proof set up, fig.align='center',fig.pos='H'}
#| echo: true

e1 = cumsum(rnorm(1000, 0, sd=1))
e2 = cumsum(rnorm(1000, 0, sd=1))
e  = cbind(e1,e2)

## Define data X, Y 
Y = ts(e[2:nrow(e),], frequency=1)
X = matrix(1,nrow(Y),1)
X = cbind(X,e[2:nrow(e)-1,])


## Test on basic model
N           = ncol(Y)                          # N=2
p           = frequency(Y)
A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)

# Prior distribution (with Minnesota prior)
kappa.1             = 0.02^2                              # shrinkage for A1 to Ap
kappa.2             = 200                                  # shrinkage for constant 
A.prior             = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2:(N + 1),] = diag(N)
V.prior             = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior             = diag(diag(Sigma.hat))
nu.prior            = N+2
I.matrix            = diag(1,nrow(Y),nrow(Y))

```

```{r}
df <- data.frame(Time = 1:nrow(Y), e1 = Y[,1], e2 = Y[,2])

plot_b <- ggplot(df, aes(x = Time)) + 
  geom_line(aes(y = e1, color = "e1")) + 
  geom_line(aes(y = e2, color = "e2")) + 
  ggtitle("Time Series e1 and e2") + 
  xlab("Time") + 
  ylab("Value") + 
  scale_color_manual(name = "Series", values = c("e1" = "blue", "e2" = "red"))
```

Function below is `posterior.draws`:

```{r Basic Model, fig.align='center',fig.pos='H'}
#| echo: true

## Posterior sample draw function for basic model(posterior.draws)
posterior.draws       = function (S, Y, X){
  
    # normal-inverse Wishard posterior parameters
    V.bar.inv         = t(X)%*%X + diag(1/diag(V.prior))
    V.bar             = solve(V.bar.inv)
    A.bar             = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)
    nu.bar            = nrow(Y) + nu.prior
    S.bar             = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
    S.bar.inv         = solve(S.bar)
    
    # posterior draws 
    Sigma.posterior   = rWishart(1, df=nu.bar, Sigma=S.bar.inv)
    Sigma.posterior   = apply(Sigma.posterior,3,solve)
    Sigma.posterior   = array(Sigma.posterior,c(N,N,S))
    A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))
    L                 = t(chol(V.bar))
    
    for (s in 1:S){
      A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])
    }

    output            = list(A.posterior=A.posterior, Sigma.posterior=Sigma.posterior)
    return(output)
}
```

```{r Basic Model Function Proof, fig.align='center',fig.pos='H'}

# Applying function 
posterior.sample.draws.p = posterior.draws(S=10000, Y=Y, X=X)

```

-   The posterior mean of the $A$ is:

```{r Basic Model Proof A, fig.align='center',fig.pos='H'}


basic.model.proof.A <- 
  tibble( "A" = c("Constant term", "Y1 lag","Y2 lag"),
          "Simulation Parameter Y1" 
          = c(round(mean(posterior.sample.draws.p[["A.posterior"]][1,1,]),4),
              round(mean(posterior.sample.draws.p[["A.posterior"]][2,1,]),4),
              round(mean(posterior.sample.draws.p[["A.posterior"]][3,1,]),4)),
           "Simulation Parameter Y2" 
          = c(round(mean(posterior.sample.draws.p[["A.posterior"]][1,2,]),4),
              round(mean(posterior.sample.draws.p[["A.posterior"]][2,2,]),4),
              round(mean(posterior.sample.draws.p[["A.posterior"]][3,2,]),4))

  )

kable(basic.model.proof.A, align = "c") %>% 
  kable_styling(font_size = 8, 
                fixed_thead = TRUE, 
                full_width = FALSE, 
                position = "center",
                latex_options = c("HOLD_position"),
                bootstrap_options = c("striped", "hover", "bordered", "responsive", "dark"))

```

-   The posterior mean of the $\Sigma$ is:

```{r}

basic.model.proof.Sigma <-
  tibble( "Sigma" = c("Y1 lag","Y2 lag"),
          "Simulation Parameter Y1" 
          = c(round(mean(posterior.sample.draws.p[["Sigma.posterior"]][1,1,]),4),
              round(mean(posterior.sample.draws.p[["Sigma.posterior"]][2,1,]),4)),
           "Simulation Parameter Y2" 
          = c(round(mean(posterior.sample.draws.p[["Sigma.posterior"]][1,2,]),4),
              round(mean(posterior.sample.draws.p[["Sigma.posterior"]][2,2,]),4))

  )

kable(basic.model.proof.Sigma, align = "c") %>% 
  kable_styling(font_size = 8, 
                fixed_thead = TRUE, 
                full_width = FALSE, 
                position = "center",
                latex_options = c("HOLD_position"),
                bootstrap_options = c("striped", "hover", "bordered", "responsive", "dark"))

```

## The extended model: Laplace distribution of error term

The **Basic Model** is the standard VARs model that assume the error terms $U$ are independent and identically distributed($iid$) as $U \ \sim N_{TN}(0 \ , \ \Sigma)$. In other formation, it could be presented as $vec(U) \ \sim N(0 \ , \ \Sigma \ \otimes \  I_{T})$. Where $\Sigma$ is a $N\times N$ cross sectional covariance matrix, $I_{t}$ is a $T\times T$ identity matrix present serial covariance, $\otimes$ is the Kronecker product and the operator $vec(.)$ is vectorization that inverts the matrix into the column vector by stacking the columns.

Therefore, we could consider a more general serial covariance structure:

```{=tex}
\begin{align}

vec(U) \ \sim N(0 \ , \ \Sigma \ \otimes \  \Omega)\\

\end{align}
```
Where:

```{=tex}
\begin{align}

\Omega \ &= \ \ diag\left[ \lambda_{1}, \lambda_{2},...,\lambda_{t}  \right]\\
\lambda \ &\sim \ Exp \ (\alpha)\\

\end{align}
```
And then, the distribution of error terms in extension model will be **Laplace distribution** instead of the normally distributed errors assumption. The Laplace distribution is well-suited for describing financial anomalies due to its sharp peaks and heavy tails. Utilizing this distribution enhances the model's robustness against anomalies, making it particularly appropriate for financial time series analysis. Given that most of our variables are financial time series data, applying a Laplace distribution to the error term is more appropriate.

Following [Eltoft,Kim, and Lee 2006b](https://ieeexplore.ieee.org/document/1618702), for covariance with a general Kronecker structure, if each ${\lambda}$ has an independent exponential distribution with mean ${\alpha}$, then marginally ${U_t}$ has a multivariate Laplace distribution with mean vector 0 and covariance matrix ${\alpha\Sigma}$.

```{=tex}
\begin{align}

U &\sim \ Laplace(0 \ , \  \alpha\Sigma) \\
U_t|\lambda_t &\sim \mathcal{MN}(0 \ , \  \Sigma \ , \ \Omega) \\
\Omega \ &= \ \ diag\left[ \lambda_{1}, \lambda_{2},...,\lambda_{t}  \right] \ = \ \lambda_t \times \textbf{I}_T\\
\lambda_t &\sim \ Exp(\alpha)

\end{align}
```
Therefore, the prior distribution of lambda which is following exponential distribution defined as:

```{=tex}
\begin{align}

P(\lambda_t|\alpha) &= \frac{1}{\alpha}exp\{ -\frac{1}{\alpha}\lambda_t \}\\

\end{align} 
```
> > Following graphs describe the difference between alpha values, we could see that as the mean, alpha, increase. The rates of exponetial function decrease.

```{r}

par(mfrow = c(1, 2), mar = c(4, 2, 4, 2))  


lambdas <- c(0.5, 1, 2)


colors <- c("red", "blue", "green")


curve(dexp(x, rate = lambdas[1]), from = 0, to = 5, ylim = c(0, 2), lwd = 2,
      xlab = "x", ylab = "Density", col = colors[1], main = "Exponential Distributions")
for (i in 2:length(lambdas)) {
  curve(dexp(x, rate = lambdas[i]), from = 0, to = 5, col = colors[i], add = TRUE, lwd = 2)
}


legend("topright", legend = paste("lambda =", lambdas), col = colors, lty = 1, lwd = 2, cex = 0.6)

library(extraDistr)

parameters <- list(
  list(mean = 0, beta = 2),
  list(mean = 0, beta = 1),
  list(mean = 0, beta = 0.5)
)


colors <- c("red", "blue", "green")


plot(NULL, xlim = c(-10, 10), ylim = c(0, 1), xlab = "x", ylab = "Density",
     main = "Laplace Distributions")
for (i in seq_along(parameters)) {
  curve(dlaplace(x, mu = parameters[[i]]$mean, s = parameters[[i]]$beta),
        col = colors[i], lwd = 2, add = TRUE)
}


legend("topright", legend = sapply(parameters, function(p) paste("mean =", p$mean, ", beta =", p$beta)),
       col = colors, lty = 1, lwd = 2, bty = "n", cex = 0.6)
```

### Bayesian Estimations

Then, the kernel of the likelihood function could be rewritten as to:

```{=tex}
\begin{align}

L(A,\Sigma,\Omega |Y,X) &\propto \det(\Sigma)^{-\frac{T}{2}} \det(\Omega)^{-\frac{N}{2}} exp\{-\frac{1}{2}\textbf{tr}[\Sigma^{-1} (Y-XA)' \Omega^{-1} (Y-XA) ]\}\\
&= \det(\Sigma)^{-\frac{T}{2}} \det(\left[ \lambda_{1}, \lambda_{2},...,\lambda_{t} \right])^{-\frac{N}{2}} exp\{-\frac{1}{2} \textbf{tr}[\Sigma^{-1} (Y-XA)' (\left[ \lambda_{1}, \lambda_{2},...,\lambda_{t} \right])^{-1} (Y-XA) ]\}\\
&=\det(\Sigma)^{-\frac{T}{2}}(\prod^{T}_{t = 1} \lambda_t)^{-\frac{N}{2}} exp\left\{{-\frac{1}{2}}\sum^{T}_{t =1}{\frac{1}{\lambda_t}} \textbf{tr}[(Y_t-X_tA)' \Sigma^{-1}(Y_t-X_tA)]\right\}\\
&=\det(\Sigma)^{-\frac{T}{2}}\prod^{T}_{t = 1} \lambda_t^{-\frac{N}{2}} exp\left\{{-\frac{1}{2}}\sum^{T}_{t =1}{\frac{1}{\lambda_t}} \textbf{tr}[\epsilon_t' \Sigma^{-1}\epsilon_t)]\right\}\\
&=\det(\Sigma)^{-\frac{T}{2}}\prod^{T}_{t = 1}(\lambda_t^{-\frac{N}{2}} exp\left\{{-\frac{1}{2}}{\frac{1}{\lambda_t}} \textbf{tr}[\epsilon_t' \Sigma^{-1}\epsilon_t])\right\})

\end{align}
```
Therefore at each time t, we have the likelihood function be a proportion of lambda:

```{=tex}
\begin{align}

\propto\det(\Sigma)^{-\frac{T}{2}}\lambda_t^{-\frac{N}{2}} exp\left\{{-\frac{1}{2}}{\frac{1}{\lambda_t}} \textbf{tr}[\epsilon_t' \Sigma^{-1}\epsilon_t])\right\}

\end{align}
```
For joint posteriors distribution, $A$, $\Sigma$ can then be derived using the likelihood and the prior distributions as follows:

```{=tex}
\begin{align}

P(A,\Sigma|Y,X) &\propto L(A,\Sigma,\Omega|Y,X) \ P(A,\Sigma) \\
\\
&= \det(\Sigma)^{-\frac{T}{2}} \det(\Omega)^{-\frac{N}{2}} exp\{-\frac{1}{2} \textbf{tr}[\Sigma^{-1} (Y-XA)' \Omega^{-1} (Y-XA) ]\} \\
&\times \det(\Sigma)^{-\frac{N+k+\underline{\nu}+1}{2}} exp\{-\frac{1}{2}\textbf{tr}[\Sigma^{-1}(A-\underline{A})'(\underline{V})^{-1}(A-\underline{A})]\} \\
&\times exp\{-\frac{1}{2}\textbf{tr}[\Sigma^{-1}\underline{S}]\} \\

&= \det(\Sigma)^{-\frac{T+N+K+\underline{\nu}+1}{2}} \det(\Omega)^{-\frac{N}{2}} \\
&\times exp\{-\frac{1}{2} \textbf{tr}[\Sigma^{-1}(Y'\Omega^{-1}Y - 2A'X'\Omega^{-1}Y + A'X'\Omega^{-1}XA \\
&+ A'\underline{V}^{-1}A -2A'\underline{V}^{-1}\underline{A} + \underline{A}'\underline{V}^{-1}\underline{A} + \underline{S})]\}

\end{align}
```
The kernel also could be rearranged in the form of the **Normal-inverse Wishart distribution** and given by:

```{=tex}
\begin{align}

P(A,\Sigma|Y,X,\Omega) &\sim \mathcal{NIW}(\bar{A},\bar{V},\bar{S},\bar{\nu}) \\
&\\
\bar{V} &= (X'\Omega^{-1}X + \underline{V}^{-1})^{-1} \\
\bar{A} &= \bar{V}(X'\Omega^{-1}Y + \underline{V}^{-1}\underline{A}) \\
\bar{\nu} &= T + \underline{\nu}\\
\bar{S} &= \underline{S} + Y'\Omega^{-1}Y + \underline{A}'\underline{V}^{-1}\underline{A} - \bar{A}'\bar{V}^{-1}\bar{A}

\end{align}
```
The kernel of the fully conditional posterior distribution of $\lambda_t$ is then derived as follows:

```{=tex}
\begin{align}

P(\lambda_t|Y,X,A,\Sigma) &\propto L(A,\Sigma,\lambda_t|Y,X)P(\lambda_t) \\
\\
&\propto \lambda_t^{-\frac{N}{2}}exp({-\frac{1}{2}}{\frac{1}{\lambda_t}}\textbf{tr}[\epsilon_t' \Sigma^{-1}\epsilon_t]) \\
&\times \frac{1}{\alpha}exp\{ -\frac{1}{\alpha}\lambda_t \}\\
&\propto \lambda_t^{-\frac{N}{2}+1-1} exp\{-\frac{1}{2}[\frac{\textbf{tr}[
\epsilon_t' \Sigma^{-1}\epsilon_t]} {\lambda_t} +\frac{2}{\alpha}\lambda_t]\} 

\end{align}
```
The above expression can be rearranged in the form of a Generalized inverse Gaussian distribution kernel as follows:

```{=tex}
\begin{align}

\lambda_t|Y,A,\Sigma &\sim \mathcal{GIG}(a,b,p) \\
\\
a &=\frac{2}{\alpha} \\
b &= \textbf{tr}[\epsilon_t' \Sigma^{-1}\epsilon_t] \\
p &= -\frac{N}{2}+1

\end{align}
```
### Gibbs Sampler: Function proving

The Gibbs sampler method will be applied to generate random draws from the full conditional posterior distribution:

1.  Draw $\Sigma^{(s)}$ from the $IW(\bar{S},\bar{\nu})$ distribution.
2.  Draw $A^{(s)}$ from the $MN(\bar{A},\Sigma^{(s)}, \bar{V})$ distribution.
3.  Draw $\lambda_t^{(s)}$ from $GIG(a,b,p)$.

Repeat steps 1, step 2 and 3 for $S_1$+$S_2$times.

Discard the first draws that allowed the algorithm to converge to the stationary posterior distribution.

Output is $\left\{ {A^{(s)}, \Sigma^{(s)}}, \lambda_t^{(s)}\right\}^{S_1+S_2}_{s=S_1+1}$.

Function below is `posterior.draws.extended`:

```{r}
# setup 
S1                = 500                            # determine the burn-in draws
S2                = 9500                           # number of draws from the final simulation
total_S           = S1+S2
A.posterior       = array(NA, dim = c((1+N*p),N,S1+S2))
Sigma.posterior   = array(NA, dim = c(N,N,S1+S2))
lambda.posterior  = matrix(NA, S1+S2, 1)
```

```{r}
#| echo: True
posterior.draws.extended <- function(total_S,Y, X){
  S=total_S
  T <- nrow(Y)
  N = ncol(Y) 
  K = 1 + (p*N)
  alpha <- 2
  lambda.0 <- rexp(T, rate = 1/alpha)
  lambda.priors = list(alpha = 2)
  
  # Initialize arrays to store posterior draws
  Sigma.posterior.draws = array(NA, c(N,N,S))
  A.posterior.draws = array(NA, c((1+p*N),N,S))
  
  lambda.posterior.draws = array(NA,c(T,S+1))
  b = array(NA,c(T,S))
  
  #lambda.posterior.draws <- array(NA,c(T,S+1))
   for (s in 1:S){
    
    if (s == 1) {
      lambda.s = lambda.0
    } else {
      lambda.s    = lambda.posterior.draws[,s]
    }
    
    Omega.inv = diag(1/lambda.s)
    
    V.bar.inv.ext   = t(X)%*%Omega.inv%*%X + solve(V.prior)
    V.bar.ext       = solve(V.bar.inv.ext)
    A.bar.ext       = V.bar.ext%*%(t(X)%*%Omega.inv%*%Y + diag(1/diag(V.prior))%*%A.prior)
    nu.bar.ext      = T + nu.prior
    S.bar.ext       = S.prior + t(Y)%*%Omega.inv%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar.ext)%*%V.bar.inv.ext%*%A.bar.ext
    S.bar.ext.inv   = solve(S.bar.ext)
    
    Sigma.inv.draw = rWishart(1, df = nu.bar.ext, Sigma = S.bar.ext.inv)[,,1]
    Sigma.posterior.draws[,,s] = solve(Sigma.inv.draw)
    A.posterior.draws[,,s] = matrix(mvtnorm::rmvnorm(1, mean=as.vector(A.bar.ext), sigma = Sigma.posterior.draws[,,s] %x% V.bar.ext), ncol=N)
    
    u.t = Y-X%*%A.posterior.draws[,,s]
    #    ---- loop lambda posterior ----   #
    c                      = -N/2 + 1          # N=12
    a                      = 2 / lambda.priors$alpha
    for (x in 1:T){
      b                  = sum(diag(t((u.t)[x,])%*%Sigma.inv.draw%*%(u.t)[x,]))
      lambda.posterior.draws[x,s+1] = GIGrvg::rgig(1, lambda = c, chi = b, psi = a)
    } # END x loop
  } # END s loop
  #}
  
  output                 = (list(A.posterior.exten = A.posterior.draws[,,S1:total_S], 
                                Sigma.posterior.exten = Sigma.posterior.draws[,,S1:total_S], 
                                lambda.posterior.exten = lambda.posterior.draws[,(S1+1):(S+1)]))
  
}

# conduct simulation
posterior.extended = posterior.draws.extended(total_S = total_S, Y=Y, X=X)
```

```{r}
A_posterior_means <- apply(posterior.extended$A.posterior.exten, 1:2, mean)

A_df <- as.data.frame(A_posterior_means)
colnames(A_df) <- c("Simulation_Y1", "Simulation_Y2")
rownames(A_df) <- c("Constant", "Y1-Lag", "Y2-Lag")
knitr::kable(A_df, caption = "Posterior mean of the autoregressive coefficient matrix A")
```

```{r}
Sigma_posterior_mean <- apply(posterior.extended$Sigma.posterior.exten, 1:2, mean)

Sigma_df <- as.data.frame(Sigma_posterior_mean)
colnames(Sigma_df) <- c("Simulation_Y1", "Simulation_Y2")
rownames(Sigma_df) <- c("Y1-Lag", "Y2-Lag")
knitr::kable(Sigma_df, caption = "Posterior mean of the covariance matrix Sigma")
```

```{r}

lambda_mean <- mean(posterior.extended$lambda.posterior.exten)

lambda_df <- data.frame(Mean = lambda_mean)
rownames(lambda_df) <- "Lambda"

kable(lambda_df, caption = "Posterior Mean of Lambda")
```

## Stochastic Volatility Heteroskedasticity

However, in the real world, the shocks are often passing to next few periods. Therefore, for the same general serial covariance structure:

```{=tex}
\begin{align}

vec(U) \ \sim N(0 \ , \ \Sigma \ \otimes \  \Omega)\\

\end{align}
```
This time we have $h_t$ follows a simple stochastic volatility process:

```{=tex}
\begin{align}

\Omega \ &= diag\left[ \sigma^2_1,..., \sigma^2_T\right]\\ 
&=\ diag\left[ exp\left\{ h_1 \right\},exp\left\{ h_2 \right\},...,exp\left\{ h_T \right\}  \right]\\
h_t &= h_{t-1}+v_t\\

\end{align}
```
Then, the kernel of the likelihood function could be rewritten as to:

```{=tex}
\begin{align}

L(A,\Sigma,\Omega |Y,X) &\propto \det(\Sigma)^{-\frac{T}{2}} \det(\Omega)^{-\frac{N}{2}} exp\{-\frac{1}{2}\textbf{tr}[\Sigma^{-1} (Y-XA)' \Omega^{-1} (Y-XA) ]\}\\
&=\det(\Sigma)^{-\frac{T}{2}}\left( \prod^{T}_{t = 1} exp\left\{ h_t \right\} \right)^{-\frac{N}{2}} exp\left\{{-\frac{1}{2}}\sum^{T}_{t =1}exp\left\{ h_t \right\}^{-1} \textbf{tr}[\epsilon_t' \Sigma^{-1}\epsilon_t)]\right\}\\
&=\det(\Sigma)^{-\frac{T}{2}}exp\left\{{-\frac{N}{2}}\sum^{T}_{t =1}h_t{-\frac{1}{2}}\sum^{T}_{t =1}exp\left\{ h_t \right\}^{-1} \textbf{tr}[\epsilon_t' \Sigma^{-1}\epsilon_t)]\right\}\\

\end{align}
```
### Matrix Notation for Stochastic Volatility model

Recall the matrix notation:

```{=tex}
\begin{align}

Y&=XA \  +  \ E\\
E|X &\sim MN(0,\Sigma,\Omega) \\
\Omega \ &=\ diag\left[ exp\left\{ h_1 \right\},exp\left\{ h_2 \right\},...,exp\left\{h_T \right\}  \right]\\
h_t &= h_{t-1}+\sigma_{v}v_t\\

\end{align}
```
Given that:

```{=tex}
\begin{align}

\epsilon_t &\sim N(0,1)\\
v_t&\sim N(0,1)\\

\end{align}
```
We could rewrite the equation as:

```{=tex}
\begin{align}

y_t &= y_{t-1}A_1 \ \ + ...+y_{t-p}A_p+\ \ exp\left\{ \frac{1}{2} h_t\right\}\epsilon_t\\
y_t - y_{t-1}A_1 \ \ + ...+y_{t-p}A_p &= exp\left\{ \frac{1}{2} h_t\right\}\epsilon_t\\
y_{u.t}&=exp\left\{ \frac{1}{2} h_t\right\}\epsilon_t\\

\end{align}
```
Then, taking the square and the logarithm of both sides of the equation, we could have:

```{=tex}
\begin{align}

y_{u.t}&=exp\left\{ \frac{1}{2} h_t\right\}\epsilon_t\\
log \ y_{u.t}^2&=h_t + log \ \epsilon_t^2\\
\tilde{y}_t&=h_t+\tilde{\epsilon}_t

\end{align}
```
And then:

```{=tex}
\begin{align}

\tilde{\epsilon}_t&\sim \log \chi^2_1

\end{align}
```
Define the following $T × 1$ matrices:

```{=tex}
\begin{aligned}

\tilde{y}=
\begin{bmatrix}
\tilde{y}_1'\\ \tilde{y}_2'
\\.\\.\\.\\\tilde{y}'_T
\end{bmatrix}_{T \times 1}

h=
\begin{bmatrix}
h_{1} \\ h_{2} \\
.\\.\\.\\h_{T}
\end{bmatrix}_{T \times 1}

\tilde{\epsilon}=
\begin{bmatrix}
\tilde{\epsilon}_1'\\ \tilde{\epsilon}_2' \\
.\\.\\.\\\tilde{\epsilon}_T'
\end{bmatrix}_{T \times 1}

v=
\begin{bmatrix}
v_{1} \\ v_{2} \\
.\\.\\.\\v_{T}
\end{bmatrix}_{T \times 1}

e_{1.T}=
\begin{bmatrix}
1 \\ 0 \\
.\\.\\.\\0
\end{bmatrix}_{T \times 1}

\end{aligned}
```
And a $T × T$ matrix, H:

```{=tex}
\begin{align}

H = \begin{pmatrix}
1 &  &  &  &  & \\
-1 & 1 &  &  &  & \\
0 & -1 & 1 &  &  & \\
\vdots & \ddots & \ddots & \ddots &  & \\
0 & \cdots & 0 & -1 & 1 \\
\end{pmatrix}_{T \times T}

\end{align}
```
Therefore, we could have the simple matrix notation for Stochastic Volatility model:

```{=tex}
\begin{align}

\tilde{y}=h+\tilde{\varepsilon}\\
Hh=h_0e_{1.T}+\sigma_vv\\
\tilde{\epsilon}\sim \log \chi^2_1\\
v\sim\mathcal{N}_T(0_T,I_T)\\ 

\end{align}
```
And approximate the $\log \chi^2_1$ distribution by a mixture of ten normal distributions given by:

```{=tex}
\begin{align}

\log \chi^2_1\approx \sum_{m=1}^{10}Pr(s_t = m)\mathcal{N}(\mu_m,\sigma^2_m)

\end{align}
```
where:

-   s_t ∈ {1, . . . , 10} is a discrete-valued random indicator of the mixture component

-   $\mu_m,\sigma^2_m$ ,Pr(st = m) are predetermined

Therefore, we could rewrite $\tilde{\epsilon}$ in a **Normal distribution**:

```{=tex}
\begin{align}

\tilde{\epsilon}|s \sim\mathcal{N}(\mu_s,diag(\sigma^2_s))

\end{align}
```
### Priors distribution

Hierarchical prior structure is given by:

```{=tex}
\begin{align}

P(h,s,h_0,\sigma^2_v)=P(h|h_0,\sigma^2_v)P(h_0)P(\sigma^2_v)P(s)

\end{align}
```
Therefore, conditional prior distribution for $h$ is:

```{=tex}
\begin{align}

P(h|h_0,\sigma^2_v)\sim\mathcal{N}(h_0H^{-1}e_{1.T},\sigma^2_v(H'H)^{-1})

\end{align}
```
with kernel given by:

```{=tex}
\begin{align}

\propto det(\sigma^2_v\textbf{I}_T)^{-\frac{1}{2}}exp\left( -\frac{1}{2}\sigma^{-2}_v\left( Hh-h_0e_{1.T} \right)'\left( Hh-h_0e_{1.T} \right) \right)

\end{align}
```
Prior distribution for $h_0$ is:

```{=tex}
\begin{align}

P(h_0)\sim\mathcal{N}(0,\underline{\sigma}^2_h)

\end{align}
```
with kernel given by:

```{=tex}
\begin{align}

\propto \left( \underline{\sigma}^2_h \right)^{-\frac{1}{2}}exp\left( -\frac{1}{2}\underline{\sigma}^{-2}_hh_0h_0 \right)

\end{align}
```
Prior distribution for $\sigma^2_v$ is:

```{=tex}
\begin{align}

P(\sigma^2_v)\sim\mathcal{IG2}(\underline{s},\underline{\nu})

\end{align}
```
with kernel given by:

```{=tex}
\begin{align}

\propto \left( \underline{\sigma}^2_v \right)^{-\frac{\underline{\nu}+2}{2}}exp\left( -\frac{1}{2}\frac{\underline{s}}{\underline{\sigma}^{2}_v} \right)

\end{align}
```
Prior distribution for $s$ is:

```{=tex}
\begin{align}

P(s_t)\sim\mathcal{Multinomial}(\mathrm{\left\{ m \right\}}_{m=1}^{10},\mathrm{\left\{Pr(s_t=m) \right\}}_{m=1}^{10})

\end{align}
```
### Bayesian Estimations

For joint posteriors distribution, $A$, $\Sigma$ can then be derived using the likelihood and the prior distributions as follows:

```{=tex}
\begin{align}

P(A,\Sigma|Y,X,\Omega) &\propto L(A,\Sigma,\Omega|Y,X) \ P(A,\Sigma) \\
\\
&= \det(\Sigma)^{-\frac{T}{2}} \det(\Omega)^{-\frac{N}{2}} exp\{-\frac{1}{2} \textbf{tr}[\Sigma^{-1} (Y-XA)' \Omega^{-1} (Y-XA) ]\} \\
&\times \det(\Sigma)^{-\frac{N+k+\underline{\nu}+1}{2}} exp\{-\frac{1}{2}\textbf{tr}[\Sigma^{-1}(A-\underline{A})'(\underline{V})^{-1}(A-\underline{A})]\} \\
&\times exp\{-\frac{1}{2}\textbf{tr}[\Sigma^{-1}\underline{S}]\} \\

&= \det(\Sigma)^{-\frac{T+N+K+\underline{\nu}+1}{2}} \det(\Omega)^{-\frac{N}{2}} \\
&\times exp\{-\frac{1}{2} \textbf{tr}[\Sigma^{-1}(Y'\Omega^{-1}Y - 2A'X'\Omega^{-1}Y + A'X'\Omega^{-1}XA \\
&+ A'\underline{V}^{-1}A -2A'\underline{V}^{-1}\underline{A} + \underline{A}'\underline{V}^{-1}\underline{A} + \underline{S})]\}

\end{align}
```
Then, we have same kernel presentation as **Laplace Distribution** and could be rearranged in the form of the **Normal-inverse Wishart distribution** and given by:

```{=tex}
\begin{align}

P(A,\Sigma|Y,X,\Omega) &\sim \mathcal{NIW}(\bar{A},\bar{V},\bar{S},\bar{\nu}) \\
&\\
\bar{V} &= (X'\Omega^{-1}X + \underline{V}^{-1})^{-1} \\
\bar{A} &= \bar{V}(X'\Omega^{-1}Y + \underline{V}^{-1}\underline{A}) \\
\bar{\nu} &= T + \underline{\nu}\\
\bar{S} &= \underline{S} + Y'\Omega^{-1}Y + \underline{A}'\underline{V}^{-1}\underline{A} - \bar{A}'\bar{V}^{-1}\bar{A}

\end{align}
```
The kernel of the fully conditional posterior distribution of $h$ is then derived as follows:

```{=tex}
\begin{align}

P(h|h_0,\sigma^2_v,s,\tilde{y}) &\propto L(h,h_0,\sigma^2_v,s|\tilde{y})P(h) \\
&\propto exp\left( -\frac{1}{2}\left( h-(\tilde{y}-\mu_s) \right)'diag\left(\sigma^2_s  \right)^{-1}\left( h-(\tilde{y}-\mu_s) \right) \right)\\
&\times exp\left( -\frac{1}{2}\sigma^{-2}_v\left( Hh-h_0e_{1.T} \right)'\left( Hh-h_0e_{1.T} \right) \right)\\ 

\end{align}
```
The above expression can be rearranged in the form of a Normal distribution kernel as follows:

```{=tex}
\begin{align}

P(h|h_0,\sigma^2_v,s,\tilde{y}) &\sim \mathcal{N}(\overline{h},\overline{V}_h)\\
\overline{V}_h&=\left[ diag\left(\sigma^2_s \right)^{-1}+ \sigma^{-2}_vH'H\right]^{-1}\\
\overline{h}&=\overline{V}_h\left[ diag\left(\sigma^2_s \right)^{-1}(\tilde{y}-\mu_s)+ \sigma^{-2}_vh_0e_{1.T}\right]\\

\end{align}
```
The kernel of the fully conditional posterior distribution of $h_0$ is then derived as follows:

```{=tex}
\begin{align}

P(h_0|h,\sigma^2_v,s,\tilde{y}) &\propto L(h_0,\sigma^2_v|h)P(h_0) \\
&\propto exp\left( -\frac{1}{2}\sigma^{-2}_v\left( Hh-h_0e_{1.T} \right)'\left( Hh-h_0e_{1.T} \right) \right)\\
&\times exp\left( -\frac{1}{2}\underline{\sigma}^{-2}_hh_0h_0 \right)

\end{align}
```
The above expression can be rearranged in the form of a Normal distribution kernel as follows:

```{=tex}
\begin{align}

P(h_0|h,\sigma^2_v,s,\tilde{y}) &\sim \mathcal{N}(\overline{h}_0,\overline{\sigma}^2_h)\\
\overline{\sigma}^2_h&=\left(\underline{\sigma}^{-2}_h+ \sigma^{-2}_v\right)^{-1}\\
\overline{h}_0&=\overline{\sigma}^2_h\left(\sigma^{-2}_ve_{1.T}'Hh\right)^{-1}\\

\end{align}
```
The kernel of the fully conditional posterior distribution of $\sigma^2_v$ is then derived as follows:

```{=tex}
\begin{align}

P(\sigma^2_v|h,h_0,s,\tilde{y}) &\propto L(h_0,\sigma^2_v|h)P(\sigma^2_v) \\
&\propto exp\left( -\frac{1}{2}\sigma^{-2}_v\left( Hh-h_0e_{1.T} \right)'\left( Hh-h_0e_{1.T} \right) \right)\\
&\times \left( \underline{\sigma}^2_v \right)^{-\frac{\underline{\nu}+2}{2}}exp\left( -\frac{1}{2}\frac{\underline{s}}{\underline{\sigma}^{2}_v} \right)

\end{align}
```
The above expression can be rearranged in the form of a Inverse-Gamma 2 distribution kernel as follows:

```{=tex}
\begin{align}

P(\sigma^2_v|h,h_0,s,\tilde{y}) &\sim \mathcal{IG2}(\overline{s},\overline{\nu})\\
\overline{\nu}&=\underline{\nu}+ T\\
\overline{s}&=\underline{s}+\left( Hh-h_0e_{1.T} \right)'\left( Hh-h_0e_{1.T} \right)\\

\end{align}
```
The fully conditional posterior distribution of $s$ is a multinomial distribution with the probabilities proportional to:

```{=tex}
\begin{align}

\omega_{m.t} = Pr[s_t=m]p(\tilde{y}|h_t,s_t=m),\text{for} \ m=1,...,10

\end{align}
```
For each $t$ and $m$ obtain $\omega_{m.t}$ using parallel computations and compute the probabilities of the multinomial full conditional posterior distribution by:

```{=tex}
\begin{align}

Pr[s_t=m|\tilde{y},h_t]=\frac{\omega_{m.t}}{\sum_{i=1}^{10}\omega_{m.t}}

\end{align}
```
### Gibbs Sampler

Considering draws from this posterior involves a Gibbs sampler, which follows the following algorithm:

1.  Initialize $h^{(0)}$, $s^{(0)}$, and $\sigma^{2(0)}_v$

2.  Draw $h_0^{(s)}\sim\mathcal{N}(\bar{h}_0,\bar{\sigma}^2_h)$

3.  Draw $\sigma_v^{2(s)}\sim\mathcal{IG2}(\bar{s},\bar{\nu})$

4.  Draw $s_t^{(s)}\sim\mathcal{Multiomial}(\{m\}^{10}_{m=1},\{Pr[s_t=m|\tilde{y},h_t^{(s)}]\}^{10}_{m=1})$ for all $s_t$ in $s$.

5.  Draw $h^{(s)}\sim\mathcal{N}_T(\bar{h},\bar{V}_h)$

6.  Draw $(A,\Sigma)\sim\mathcal{MNIW}(\bar{A},\bar{V},\bar{S},\bar{\nu})$

Steps 2 to 7 are repeated for $S=S_1+S_2$ draws, where $S_1$ draws are discarded as burn-in and the latter $S_2$ draws are kept as posterior draws.

First Hierarchical draws based on following function `SVcommon.Gibbs.iteration`:

```{r}
#| echo: true
SVcommon.Gibbs.iteration <- function(aux, priors){
  # A single iteration of the Gibbs sampler for the SV component

  # aux is a list containing:
  #   Y         - a TxN matrix
  #   X         - a TxK matrix
  #   H         - a Tx1 matrix                 
  #   h0        - a scalar
  #   sigma.v2  - a scalar
  #   s         - a Tx1 matrix
  #   A         - a KxN matrix
  #   Sigma     - an NxN matrix
  #   sigma2    - a Tx1 matrix
  
  # priors is a list containing:
  #   h0.v      - a positive scalar
  #   h0.m      - a scalar
  #   sigmav.s  - a positive scalar
  #   sigmav.nu - a positive scalar
  #   HH        - a TxT matrix

  T             = dim(aux$Y)[1]
  N             = dim(aux$Y)[2]
  alpha.st      = c(1.92677,1.34744,0.73504,0.02266,0-0.85173,-1.97278,-3.46788,-5.55246,-8.68384,-14.65000)
  sigma.st      = c(0.11265,0.17788,0.26768,0.40611,0.62699,0.98583,1.57469,2.54498,4.16591,7.33342)
  pi.st         = c(0.00609,0.04775,0.13057,0.20674,0.22715,0.18842,0.12047,0.05591,0.01575,0.00115)

  Lambda        = solve(chol(aux$Sigma))
  Z             = rowSums( ( aux$Y - aux$X %*% aux$A ) %*% Lambda ) / sqrt(N)
  Y.tilde       = as.vector(log((Z + 0.0000001)^2))
  Ytilde.alpha  = as.matrix(Y.tilde - alpha.st[as.vector(aux$s)])

  # sampling initial condition
  V.h0.bar      = 1/((1 / priors$h0.v) + (1 / aux$sigma.v2))
  m.h0.bar      = V.h0.bar*((priors$h0.m / priors$h0.v) + (aux$H[1] / aux$sigma.v2))
  h0.draw       = rnorm(1, mean = m.h0.bar, sd = sqrt(V.h0.bar))
  aux$h0        = h0.draw    

  # sampling sigma.v2
  sigma.v2.s    = priors$sigmav.s + sum(c(aux$H[1] - aux$h0, diff(aux$H))^2)
  sigma.v2.draw = sigma.v2.s / rchisq(1, priors$sigmav.nu + T)
  aux$sigma.v2  = sigma.v2.draw    

  # sampling auxiliary states
  Pr.tmp        = simplify2array(lapply(1:10,function(x){
    dnorm(Y.tilde, mean = as.vector(aux$H + alpha.st[x]), sd = sqrt(sigma.st[x]), log = TRUE) + log(pi.st[x])
  }))
  Pr            = t(apply(Pr.tmp, 1, function(x){exp(x - max(x)) / sum(exp(x - max(x)))}))
  s.cum         = t(apply(Pr, 1, cumsum))
  r             = matrix(rep(runif(T), 10), ncol = 10)
  ss            = apply(s.cum < r, 1, sum) + 1
  aux$s         = as.matrix(ss)       


  # sampling log-volatilities using functions for tridiagonal precision matrix
  Sigma.s.inv   = diag(1 / sigma.st[as.vector(aux$s)])
  D.inv         = Sigma.s.inv + (1 / aux$sigma.v2) * priors$HH
  b             = as.matrix(Ytilde.alpha / sigma.st[as.vector(aux$s)] + (aux$h0/aux$sigma.v2)*diag(T)[,1])
  lead.diag     = diag(D.inv)
  sub.diag      = mgcv::sdiag(D.inv, -1)
  D.chol        = mgcv::trichol(ld = lead.diag, sd = sub.diag)
  D.L           = diag(D.chol$ld)
  mgcv::sdiag(D.L,-1) = D.chol$sd
  x             = as.matrix(rnorm(T))
  a             = forwardsolve(D.L, b)
  draw          = backsolve(t(D.L), a + x)
  aux$H         = as.matrix(draw)         
  aux$sigma2    = as.matrix(exp(draw))     

  return(aux)
}
```

Therefore, we could have the function `DrawPosterior.sv` to draw $A$ and $\Sigma$:

```{r}
#| echo: true
DrawPosterior.sv <- function (Y,X,S1,S2){
  
  N <- ncol(Y)
  K <- ncol(X)
  T <- nrow(Y)
  
  A.posterior     <- array(NA,c(K,N,(S1+S2)))
  Sigma.posterior <- array(NA,c(N,N,(S1+S2))) 
  H.posterior     <- array(NA,c(nrow(Y),(S1+S2+1)))  
  B.posterior     <- array(NA,c(N,N,(S1+S2)))
  B1.tilde.s      <- array(NA,c(N,K,(S1+S2)))
  
  # Initialize h^{0}, which is  T by 1 matrix 
  H.posterior[,1]       <- matrix(1,T,1) 
  
  nu.bar                <- nrow(Y) + nu.prior
  
  HH                    <-  2*diag(T)
  mgcv::sdiag(HH,-1)    <-  -1
  mgcv::sdiag(HH,1)     <-  -1
  
  priors = list(HH       = HH,
                h0.m     = 0,
                h0.v     = 1,
                sigmav.s = 1,
                sigmav.nu= 1
              )
  
  for (s in 1:(S1+S2)){
    # STEP 1: draw (A Sigma).s from MNIW(A.bar,V.bar,S.bar,nu.bar)
    # setting up parameters 
    
    V.bar.inv       <- t(X)%*%diag(1/H.posterior[,s])%*%X + diag(1/diag(V.prior))
    V.bar           <- solve(V.bar.inv)
    A.bar           <- V.bar%*%(t(X)%*%diag(1/H.posterior[,s])%*%Y + diag(1/diag(V.prior))%*%A.prior)
    S.bar           <- S.prior+t(Y)%*%diag(1/H.posterior[,s])%*%Y+t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
    
    draw.sigma.inv  <- solve(rWishart(1, df=nu.bar, Sigma=solve(S.bar))[,,1])
    Sigma.posterior[,,s] = draw.sigma.inv
      
      # start drawing 
    cholSigma.s      <- chol(Sigma.posterior[,,s])
    A.posterior[,,s] <- matrix(MASS::mvrnorm(1,as.vector(A.bar),Sigma.posterior[,,s]%x%V.bar),ncol=N)      
    
    # STEP 2: draw H from SVcommon.Gibbs.iteration
    if (s == 1){ # initializing input arguments 
        aux = list( 
              Y             = Y,
              X             = X,
              H             = matrix(1,T,1),
              h0            = 0,
              sigma.v2      = 1,
              s             = matrix(1,T,1),
              Sigma         = Sigma.posterior[,,s],
              A             = A.posterior[,,s],
              sigma2        = matrix(1,T,1)
              )
    }else{ # updating input arguments  
      aux = list(
                Y           = Y,
                X           = X,
                H           = tmp$H,
                h0          = tmp$h0,
                sigma.v2    = tmp$sigma.v2,
                s           = tmp$s,
                Sigma       = Sigma.posterior[,,s],
                A           = A.posterior[,,s],
                sigma2      = tmp$sigma2
              )
    }

    # H                     <- diag(T)
    # sdiag(H,-1)           <- -1     
    tmp                     <- SVcommon.Gibbs.iteration(aux,priors)
    H.posterior[,s+1]      <- as.matrix(tmp$sigma2)
  }
  
  return(list(Sigma.posterior = Sigma.posterior[,,(S1+1):(S1+S2)],
              A.posterior     = A.posterior[,,(S1+1):(S1+S2)],
              H.sv            = H.posterior[,(S1+2):(S1+S2+1)]))
  }
```

```{r}
results.h <-DrawPosterior.sv(Y,X,S1,S2)
```

The below posterior mean provides a time series plot of posterior draws mean of $\sigma^2$ at each time spot. From where we can observe the $\sigma^2$ oscillating around 2100.

```{r}
sigma2.mean.vec <- apply(results.h$H.sv,1,mean) 
plot.ts(sigma2.mean.vec,main=expression(paste("Posterior Mean of ",sigma^2)),
        ylab="",type='l',lwd=2,col="#9E9AC8",xlab="T")
abline(h=mean(sigma2.mean.vec),lty=2)
```

Following plots the draws of Gibbs Sampler of the Stochastic Volatility model, and it shown to be stationary for A and $\Sigma$.From where, we can observe the mean of $A_{21},A_{32}$ varying around value of 1, $\Sigma_{21},\Sigma_{22}$ oscillating around 0.0018-0.0020.

```{r}
#| label: fig-Gibbs-Sampler-SV 
#| fig-cap: "Gibbs Sampler draws of A and Sigma-SV model"
par(mfrow=c(2,2),mar=c(2,2,2,2))
plot(results.h$A.posterior[2,1,],type='l',col="#662099",ylab="",xlab="",main=expression(A[21]), lwd = 0.1)
plot(results.h$A.posterior[3,2,],type='l',col="#CC16CC",ylab="",xlab="",main=expression(A[32]), lwd = 0.1)
plot(results.h$Sigma.posterior[1,1,],type='l',col="#6229CC",ylab="",xlab="",main=expression(Sigma[21]), lwd = 0.1)
plot(results.h$Sigma.posterior[2,2,],type='l',col="#0079CC",ylab="",xlab="",main=expression(Sigma[22]), lwd = 0.1)
```

## Combinations of Extension

Due to complications in the real world, some of common shocks to the market are deep influenced and some are disappeared in a decided time period. For example, the GFC, Covid-19 and Russian-Ukraine war influence for a long time, even right now, we are underlying a quantitative easing scenario and suffering a common inflation risk. That reflects that the effects to serial covariance some are independently and some part are autoregressive on the other hand. Therefore, we could combine the Laplace model and Stochastic Volatility model to present, estimate and forecast under this complicated scenario.

Therefore, we could have the model present as:

```{=tex}
\begin{align}

Y &= X A +U 
\\
\\
U|X &\sim \mathcal{MN}_{T\times N}(\textbf{0},\Sigma,\Omega)\\
vec(U) \ &\sim N(0 \ , \ \Sigma \ \otimes \  \Omega)\\
\Omega \ &= diag\left[\lambda_{1} \sigma^2_1, \lambda_{2}\sigma^2_2..., \lambda_{t}\sigma^2_T\right]\\ 
&=\ diag\left[ \lambda_{1}exp\left\{ h_1 \right\},\lambda_{2}exp\left\{ h_2 \right\},...,\lambda_{T}exp\left\{ h_T \right\}  \right]\\
\lambda \ &\sim \ Exp \ (\alpha)\\
h_t &= h_{t-1}+\sigma_{v}v_t\\
v_t&\sim N(0,1)\\

\end{align}
```
Still have that:

```{=tex}
\begin{align}

\epsilon_t &\sim N(0,1)\\

\end{align}
```
Now we could rewrite the equation into:

```{=tex}
\begin{align}

y_t &= y_{t-1}A_1 \ \ + ...+y_{t-p}A_p+\ \ \sqrt{\lambda_t}exp\left\{ \frac{1}{2} h_t\right\}\epsilon_t\\
\frac{y_t - y_{t-1}A_1 \ \ + ...+y_{t-p}A_p}{\sqrt{\lambda_t}} &= exp\left\{ \frac{1}{2} h_t\right\}\epsilon_t\\
y_{\lambda.t}&=exp\left\{ \frac{1}{2} h_t\right\}\epsilon_t\\
\\
y_{\lambda.t}&=exp\left\{ \frac{1}{2} h_t\right\}\epsilon_t\\
log \ y_{\lambda.t}^2&=h_t + log \ \epsilon_t^2\\
\tilde{y}_{\lambda.t}&=h_t+\tilde{\epsilon}_t

\end{align}
```
Same to the standard Stochastic Volatility model, and the Hierarchical prior structure are same:

```{=tex}
\begin{align}

\tilde{y}_{\lambda}=h+\tilde{\varepsilon}\\
Hh=h_0e_{1.T}+\sigma_vv\\
\tilde{\epsilon}\sim \log \chi^2_1\\
v\sim\mathcal{N}_T(0_T,I_T)\\ 

P(h,s,h_0,\sigma^2_v)=P(h|h_0,\sigma^2_v)P(h_0)P(\sigma^2_v)P(s)

\end{align}
```
### Likelihood Function

The general likelihood function remain unchanged due to continues of serial covariance symbol $\Omega$ is used:

```{=tex}
\begin{align}

L(A,\Sigma,\Omega |Y,X) &\propto \det(\Sigma)^{-\frac{T}{2}} \det(\Omega)^{-\frac{N}{2}} exp\{-\frac{1}{2}\textbf{tr}[\Sigma^{-1} (Y-XA)' \Omega^{-1} (Y-XA) ]\}\\

\end{align}
```
After simplify we could have:

```{=tex}
\begin{align}

L(A,\Sigma,\Omega |Y,X) &\propto \det(\Sigma)^{-\frac{T}{2}} \det(\Omega)^{-\frac{N}{2}} exp\{-\frac{1}{2}\textbf{tr}[\Sigma^{-1} (Y-XA)' \Omega^{-1} (Y-XA) ]\}\\
&=\det(\Sigma)^{-\frac{T}{2}}\left( \prod^{T}_{t = 1} \lambda_t\prod^{T}_{t = 1} exp\left\{ h_t \right\} \right)^{-\frac{N}{2}} exp\left\{{-\frac{1}{2}}\sum^{T}_{t =1}\lambda_texp\left\{ h_t \right\}^{-1} \textbf{tr}[\epsilon_t' \Sigma^{-1}\epsilon_t)]\right\}\\
&=\det(\Sigma)^{-\frac{T}{2}}\prod^{T}_{t = 1} \lambda_t^{-\frac{N}{2}} exp\left\{{-\frac{N}{2}}\sum^{T}_{t =1}h_t{-\frac{1}{2}}\sum^{T}_{t =1}\lambda_texp\left\{ h_t \right\}^{-1} \textbf{tr}[\epsilon_t' \Sigma^{-1}\epsilon_t)]\right\}\\

\end{align}
```
### Bayesian Estimations

As the unchanged likelihood function, we still have:

```{=tex}
\begin{align}

P(A,\Sigma|Y,X,\Omega) &\propto L(A,\Sigma,\Omega|Y,X) \ P(A,\Sigma) \\
\\
&= \det(\Sigma)^{-\frac{T}{2}} \det(\Omega)^{-\frac{N}{2}} exp\{-\frac{1}{2} \textbf{tr}[\Sigma^{-1} (Y-XA)' \Omega^{-1} (Y-XA) ]\} \\
&\times \det(\Sigma)^{-\frac{N+k+\underline{\nu}+1}{2}} exp\{-\frac{1}{2}\textbf{tr}[\Sigma^{-1}(A-\underline{A})'(\underline{V})^{-1}(A-\underline{A})]\} \\
&\times exp\{-\frac{1}{2}\textbf{tr}[\Sigma^{-1}\underline{S}]\} \\

&= \det(\Sigma)^{-\frac{T+N+K+\underline{\nu}+1}{2}} \det(\Omega)^{-\frac{N}{2}} \\
&\times exp\{-\frac{1}{2} \textbf{tr}[\Sigma^{-1}(Y'\Omega^{-1}Y - 2A'X'\Omega^{-1}Y + A'X'\Omega^{-1}XA \\
&+ A'\underline{V}^{-1}A -2A'\underline{V}^{-1}\underline{A} + \underline{A}'\underline{V}^{-1}\underline{A} + \underline{S})]\}

\end{align}
```
Which is the kernel represent as **Normal-inverse Wishart distribution** and given by:

```{=tex}
\begin{align}

P(A,\Sigma|Y,X,\Omega) &\sim \mathcal{NIW}(\bar{A},\bar{V},\bar{S},\bar{\nu}) \\
&\\
\bar{V} &= (X'\Omega^{-1}X + \underline{V}^{-1})^{-1} \\
\bar{A} &= \bar{V}(X'\Omega^{-1}Y + \underline{V}^{-1}\underline{A}) \\
\bar{\nu} &= T + \underline{\nu}\\
\bar{S} &= \underline{S} + Y'\Omega^{-1}Y + \underline{A}'\underline{V}^{-1}\underline{A} - \bar{A}'\bar{V}^{-1}\bar{A}

\end{align}
```
The posterior of $\lambda_t$ could be precent in the form of $\sigma^2$ which is:

```{=tex}
\begin{align}

P(\lambda_t|Y,X,A,\Sigma,h,h_0,\sigma^2_v) &\propto L(A,\Sigma,\lambda_t,h,h_0,\sigma^2_v|Y,X)P(\lambda_t) \\
\\
&\propto \lambda_t^{-\frac{N}{2}}exp({-\frac{1}{2}}{\frac{1}{\lambda_t}}\textbf{tr}[\Sigma^{-1}\epsilon_t' diag(\sigma^2)\epsilon_t]) \\
&\times \frac{1}{\alpha}exp\{ -\frac{1}{\alpha}\lambda_t \}\\
&\propto \lambda_t^{-\frac{N}{2}+1-1} exp\{-\frac{1}{2}[\frac{\textbf{tr}[\Sigma^{-1}
\epsilon_t'diag(\sigma^2)\epsilon_t]} {\lambda_t} +\frac{2}{\alpha}\lambda_t]\} 

\end{align}
```
The above expression can be rearranged in the form of a Generalized inverse Gaussian distribution kernel as follows:

```{=tex}
\begin{align}

\lambda_t|Y,A,\Sigma &\sim \mathcal{GIG}(a,b,p) \\
\\
a &=\frac{2}{\alpha} \\
b &= \textbf{tr}[\Sigma^{-1}\epsilon_t'diag(\sigma^2)\epsilon_t] \\
p &= -\frac{N}{2}+1

\end{align}
```
The posterior for Hierarchical structure $h,h_0,s,\sigma^2_v$ remain unchanged:

```{=tex}
\begin{align}

P(h|h_0,\sigma^2_v,s,\tilde{y}) &\sim \mathcal{N}(\overline{h},\overline{V}_h)\\
\overline{V}_h&=\left[ diag\left(\sigma^2_s \right)^{-1}+ \sigma^{-2}_vH'H\right]^{-1}\\
\overline{h}&=\overline{V}_h\left[ diag\left(\sigma^2_s \right)^{-1}(\tilde{y}-\mu_s)+ \sigma^{-2}_vh_0e_{1.T}\right]\\
\\
P(h_0|h,\sigma^2_v,s,\tilde{y}) &\sim \mathcal{N}(\overline{h}_0,\overline{\sigma}^2_h)\\
\overline{\sigma}^2_h&=\left(\underline{\sigma}^{-2}_h+ \sigma^{-2}_v\right)^{-1}\\
\overline{h}_0&=\overline{\sigma}^2_h\left(\sigma^{-2}_ve_{1.T}'Hh\right)^{-1}\\
\\
P(\sigma^2_v|h,h_0,s,\tilde{y}) &\sim \mathcal{IG2}(\overline{s},\overline{\nu})\\
\overline{\nu}&=\underline{\nu}+ T\\
\overline{s}&=\underline{s}+\left( Hh-h_0e_{1.T} \right)'\left( Hh-h_0e_{1.T} \right)\\
\\
Pr[s_t=m|\tilde{y},h_t]&=\frac{\omega_{m.t}}{\sum_{i=1}^{10}\omega_{m.t}}

\end{align}
```
### Gibbs Sampler

```{r}
invfunc = function(inv){
eigen_decomp <- eigen(inv, symmetric = TRUE)
      eigen_decomp$values[eigen_decomp$values <= 0] <- 1
      inv <- eigen_decomp$vectors %*% diag(eigen_decomp$values) %*% t(eigen_decomp$vectors)
}
```

Given the hyper-parameters $h_0,s,\sigma^{2}_v$, we could use same Hierarchical draws based on function `SVcommon.Gibbs.iteration` and then follow 3 steps:

1.  Draw $(h^{(s)}, s^{(s)}, \sigma^{2(s)}_v,h^{(s)}$

2.  Draw $\lambda^{(s)}$

3.  Draw $(A,\Sigma)\sim\mathcal{MNIW}(\bar{A},\bar{V},\bar{S},\bar{\nu})$

Therefore, we could have the function `posterior.draws.exten.hetero` to draw $A$ and $\Sigma$:

```{r Extend Heteroskedasticity Model, fig.align='center',fig.pos='H'}
#| echo: true
posterior.draws.exten.hetero = function (total_S, Y, X){
  
  ## Pre-setup 
     
  N             = ncol(Y) 
  t             = N + 1
  p             = frequency(Y)                      
  A.hat         = solve(t(X)%*%X)%*%t(X)%*%Y
  Sigma.hat     = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)
  T             = dim(Y)[1] 
  K             = dim(X)[2] 
  H             = diag(T)
  sdiag(H,-1)   = -1
  HH            = 2*diag(T)
  sdiag(HH,-1)  = -1
  sdiag(HH,1)   = -1
  
  # Prior distribution (with Minnesota prior)

  A.prior           = matrix(0,nrow(A.hat),ncol(A.hat))
  A.prior[2:t,]     = diag(N)
  V.prior           = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
  S.prior           = diag(diag(Sigma.hat))
  nu.prior          = N + 2
  lambda.priors     = list(alpha=2)
  # 
  HH                = HH
  h0.m              = 0
  h0.v              = 1
  sigmav.s          = 1
  sigmav.nu         = 1


  # Define posterior
  posteriors    = list(                        
    H           = matrix(NA, T, total_S),
    sigma2      = matrix(NA, T, total_S),
    s           = matrix(NA, T, total_S),
    h0          = rep(NA, total_S),
    sigma.v2    = rep(NA, total_S),
    A           = array(NA, c((1 + N * p), N, total_S)),
    Sigma       = array(NA, c(N, N, total_S)),
    lambda      = rep(NA, total_S)  
)
  
  # Define auxiliary
  aux        = list(
    Y        = Y,
    X        = X,
    H        = matrix(1, T, 1),
    h0       = 0,
    sigma.v2 = 1,
    s        = matrix(1, T, 1),
    A        = matrix(0, K, N),
    Sigma    = matrix(0, N, N),             
    sigma2   = matrix(1, T, 1),
    lambda   = 2
  )

   # ----------------------posterior.draws.exten.hetero------------------------------

 for (s in 1:total_S){ 

  # normal-inverse Wishard posterior 
      V.bar.inv              = t(aux$X)%*%(aux$lambda*diag(1/as.vector(aux$sigma2)))%*%aux$X + solve(V.prior)
      V.bar.inv              = invfunc(V.bar.inv)
      V.bar                  = solve(V.bar.inv)
      A.bar                  = V.bar%*%(t(aux$X)%*%(aux$lambda*diag(1/as.vector(aux$sigma2)))%*%aux$Y + solve(V.prior)%*%A.prior) 
      nu.bar                 = T + nu.prior
      S.bar                  = S.prior + t(aux$Y)%*%(aux$lambda*diag(1/as.vector(aux$sigma2)))%*%aux$Y + t(A.prior)%*%solve(V.prior)%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar 
      S.bar                  = invfunc(S.bar)
      S.bar.inv              = solve(S.bar)
      
    # posterior draws for A and Sigma
      Sigma.posterior.IW     = rWishart(1, df=nu.bar, Sigma=S.bar.inv)
      Sigma.posterior.draw   = apply(Sigma.posterior.IW, 3 ,solve)
      aux$Sigma              = array(Sigma.posterior.draw,c(N,N,1))
      
      for (i in 1:dim(aux$Sigma)[3]) {
          eigen_decomp <- eigen(aux$Sigma[, , i], symmetric = TRUE)
          eigen_decomp$values[eigen_decomp$values <= 0] <- 1
          aux$Sigma[, , i] <- eigen_decomp$vectors %*% diag(eigen_decomp$values) %*% t(eigen_decomp$vectors)
      }
      A.norm                 = array(rnorm(prod(c(K,N,1))),c(K,N,1))
      L                      = t(chol(V.bar))      
      aux$A                  = A.bar + L%*%A.norm[,,1]%*%chol(aux$Sigma[,,1])
      
      u.t = Y-X%*%aux$A
      
      matrix_2x2 <- matrix(aux$Sigma, nrow = 2, ncol = 2)
    # posterior draws for lambda
      lambda.p = -N/2 + 1 
      chi.b    = sum(diag(solve(matrix_2x2)%*%t(u.t)%*%diag(1/as.vector(aux$sigma2))%*%u.t))
      psi.a    =  2 / lambda.priors$alpha
      
      lambda.draw = GIGrvg::rgig(n = 1, lambda = lambda.p, chi = chi.b, psi = psi.a)
      aux$lambda = lambda.draw
  
      # posterior draw for sigma2   
      N             = dim(aux$Y)[2] 
      alpha.st      = c(1.92677,1.34744,0.73504,0.02266,0-0.85173,-1.97278,-3.46788,-5.55246,-8.68384,-14.65000)
      sigma.st      = c(0.11265,0.17788,0.26768,0.40611,0.62699,0.98583,1.57469,2.54498,4.16591,7.33342)
      pi.st         = c(0.00609,0.04775,0.13057,0.20674,0.22715,0.18842,0.12047,0.05591,0.01575,0.00115)
      
      Lambda        = solve(chol(aux$Sigma[,,1]))
      Z             = rowSums( ( aux$Y - aux$X %*% aux$A ) %*% Lambda ) / sqrt(N)
      Y.tilde       = as.vector(log((Z + 0.0000001)^2))
      Ytilde.alpha  = as.matrix(Y.tilde - alpha.st[as.vector(aux$s)])
        
      # sampling initial condition
      V.h0.bar      = 1/((1 / h0.v) + (1 / aux$sigma.v2))
      m.h0.bar      = V.h0.bar*((h0.m / h0.v) + (aux$H[1] / aux$sigma.v2))
      h0.draw       = rnorm(1, mean = m.h0.bar, sd = sqrt(V.h0.bar))
      aux$h0        = h0.draw
      
      # sampling sigma.v2
      sigma.v2.s    = sigmav.s + sum(c(aux$H[1] - aux$h0, diff(aux$H))^2)
      sigma.v2.draw = sigma.v2.s / rchisq(1, sigmav.nu + T)
      aux$sigma.v2  = sigma.v2.draw
      
      # sampling auxiliary states
      Pr.tmp        = simplify2array(lapply(1:10,function(x){
        dnorm(Y.tilde, mean = as.vector(aux$H + alpha.st[x]), sd = sqrt(sigma.st[x]), log = TRUE) + log(pi.st[x])
      }))
      Pr            = t(apply(Pr.tmp, 1, function(x){exp(x - max(x)) / sum(exp(x - max(x)))}))
      s.cum         = t(apply(Pr, 1, cumsum))
      r             = matrix(rep(runif(T), 10), ncol = 10)
      ss            = apply(s.cum < r, 1, sum) + 1
      aux$s         = as.matrix(ss)
      
      # sampling log-volatilities using functions for tridiagonal precision matrix
      Sigma.s.inv   = diag(1 / sigma.st[as.vector(aux$s)])
      D.inv         = Sigma.s.inv + (1 / aux$sigma.v2) * HH
      b             = as.matrix(Ytilde.alpha / sigma.st[as.vector(aux$s)] + (aux$h0/aux$sigma.v2)*diag(T)[,1])
      lead.diag     = diag(D.inv)
      sub.diag      = mgcv::sdiag(D.inv, -1)
      D.chol        = mgcv::trichol(ld = lead.diag, sd = sub.diag)
      D.L           = diag(D.chol$ld)
      mgcv::sdiag(D.L,-1) = D.chol$sd
      x             = as.matrix(rnorm(T))
      a             = forwardsolve(D.L, b)
      draw          = backsolve(t(D.L), a + x)
      aux$H         = as.matrix(draw)
      aux$sigma2    = as.matrix(exp(draw))
        
      # output list
      posteriors$H[,s]             = aux$H
      posteriors$sigma2[,s]        = aux$sigma2
      posteriors$s[,s]             = aux$s
      posteriors$h0[s]             = aux$h0
      posteriors$sigma.v2[s]       = aux$sigma.v2
      posteriors$A[,,s]            = aux$A
      posteriors$Sigma[,,s]        = aux$Sigma
      posteriors$lambda[s]         = aux$lambda
      
  }
        
  return(posteriors)
  
}
```

```{r}
results.hh <-posterior.draws.exten.hetero(total_S,Y,X)
```

Then we also could have A and $\Sigma$. From where, we can observe the mean of $A_{21},A_{32}$ varying around value of 1.

```{r}
par(mfrow=c(2,2),mar=c(2,2,2,2))
plot(results.hh$A[2,1,],type='l',col="#662099",ylab="",xlab="",main=expression(A[21]), lwd = 0.1)
plot(results.hh$A[3,2,],type='l',col="#CC16CC",ylab="",xlab="",main=expression(A[32]), lwd = 0.1)
plot(results.hh$Sigma[1,1,],type='l',col="#6229CC",ylab="",xlab="",main=expression(Sigma[21]), lwd = 0.1)
plot(results.hh$Sigma[2,2,],type='l',col="#0079CC",ylab="",xlab="",main=expression(Sigma[22]), lwd = 0.1)
```

# Empirical Analysis: Model Applying and Forecasing

## Basic Model

First, set up and read our data:

```{r}
#| echo: true
## Create Y and X
y             = ts(all_data[,1:ncol(all_data)])     # 10col, 135row
Y             = ts(y[13:nrow(y),], frequency=12)      # 10col, 131row
X             = matrix(1,nrow(Y),1)
for (i in 1:frequency(Y)){
  X           = cbind(X,y[13:nrow(y)-i,])            # 10*4+1=41col, 131row
}
 
## Pre-setup 
N             = ncol(Y)                             # N=10
p             = frequency(Y)                        # p=4
A.hat         = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat     = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)

# Prior distribution (with Minnesota prior)
kappa.1       = 0.02^2                                # shrinkage for A1 to Ap
kappa.2       = 200                                   # shrinkage for constant 
A.prior       = matrix(0,nrow(A.hat),ncol(A.hat))
# A.prior[2,1]  = 1
# A.prior[7,6]  = 1  
A.prior[2:13,] = diag(12)
V.prior       = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior       = diag(diag(Sigma.hat))
nu.prior      = N+2 
I.matrix            = diag(1,nrow(Y),nrow(Y))
```

```{r}
## Function Applying
posterior.sample.draws = posterior.draws(S=50000, Y=Y, X=X) 
A.posterior.simu       = posterior.sample.draws$A.posterior
Sigma.posterior.simu   = posterior.sample.draws$Sigma.posterior
```

```{r}
A.posterior.simu.mean = head(round(apply(A.posterior.simu, 1:2, mean), 6),12)
Sigma.posterior.simu.mean = round(apply(Sigma.posterior.simu, 1:2, mean),6)      
```

Then, we could have the mean of A:

```{r }
kable(A.posterior.simu.mean, align = "c") %>% 
  kable_styling(font_size = 8, 
                fixed_thead = TRUE, 
                full_width = FALSE, 
                position = "center",
                latex_options = c("HOLD_position"),
                bootstrap_options = c("striped", "hover", "bordered", "responsive", "dark"))
```

And, we could have the mean of $\Sigma$:

```{r}
kable(Sigma.posterior.simu.mean, align = "c") %>% 
  kable_styling(font_size = 8, 
                fixed_thead = TRUE, 
                full_width = FALSE, 
                position = "center",
                latex_options = c("HOLD_position"),
                bootstrap_options = c("striped", "hover", "bordered", "responsive", "dark"))
```

Therefore, we could have histogram and time series of A for Gibbs sampler:

```{r}
#| fig-cap: "Figure 6 Histograms and trace plots on key variables using basic model"

par(mfrow=c(2,2), mar=c(4,4,2,2))
plot.ts(A.posterior.simu[2,1,], xlab = "Simulation times S", ylab = "lgold", col = blue1) 
hist(A.posterior.simu[2,1,], xlab = "lgold", col = blue1, main = '')
plot.ts(A.posterior.simu[2,5,], xlab = "Simulation times S", ylab = "lNesdaq", col = green1) 
hist(A.posterior.simu[2,5,], xlab = "lNesdaq", col = green1, main = '')

```

```{r}
par(mfrow=c(2,2), mar=c(4,4,2,2))
plot.ts(A.posterior.simu[2,3,], xlab = "Simulation times S", ylab = "TreasuryBill5Years", col = blue2) 
hist(A.posterior.simu[2,3,], xlab = "TreasuryBill5Years", col = blue2, main = '')
plot.ts(A.posterior.simu[2,6,], xlab = "Simulation times S", ylab = "M2 Rates", col = green3) 
hist(A.posterior.simu[2,6,], xlab = "M2 Rates", col = green3, main = '')

```

```{r}
par(mfrow=c(2,2), mar=c(4,4,2,2))
plot.ts(A.posterior.simu[6,1,], xlab = "Simulation times S", ylab = "lgold", col = blue1) 
hist(A.posterior.simu[6,1,], xlab = "lgold", col = blue1, main = '')
plot.ts(A.posterior.simu[6,5,], xlab = "Simulation times S", ylab = "lNesdaq", col = green1) 
hist(A.posterior.simu[6,5,], xlab = "lNesdaq", col = green1, main = '')

```

we could clearly see that underling our Minnesota prior assumption, a high level of shrinkage of random walk process, Gold have its first lag coefficient really close to 1, and other variables close to 0. This support our basic understanding of Gold nature, a safe assets.

```{r}
## Two-year ahead forecasting h=24
# set up
h                      = 24
S                      = 50000
Y.h                    = array(NA,c(h,N,S))

# sampling predictive density
for (s in 1:S){
  A.posterior.draw     = A.posterior.simu[,,s]
  Sigma.posterior.draw = Sigma.posterior.simu[,,s]
    x.Ti               = Y[(nrow(Y)-p+1):nrow(Y),]
    x.Ti               = x.Ti[p:1,]
  for (i in 1:h){
    x.T                = c(1,as.vector(t(x.Ti)))
    Y.f                = rmvnorm(1, mean = x.T%*%A.posterior.draw, sigma=Sigma.posterior.draw)
    x.Ti               = rbind(Y.f,x.Ti[1:(p-1),])
    Y.h[i,,s]          = Y.f[1:N]
  }
}

```

Following presents a 3D visualization of the density intervals for the log Gold Future Prices and Log Nasdaq Index points. From past trends we could clearly see that negative correlations between those two. When the market suffered from dot-com and global financial crisis during 2000 to 2012, Gold futures increase sharply as expectation of risks, market returns fluctuations a lot. For next 24 months forecasting based on benchmark model, we could see Log gold future price more sharply increase then Log Nasdaq Index points. The varying heights of the intervals reflect the level of prediction certainty; as we project further into the future, the intervals become wider and more dispersed due to increased uncertainty.

```{r}
par(mfcol = c(1, 2), mar=c(2,2,2,2)-0.1)

# Log gold forecasting 
limits.Lgold      = range(Y.h[,1,])
point.Lgold.f     = apply(Y.h[,1,],1,mean)

interval.Lgold.f  = apply(Y.h[,1,],1,hdi,credMass=0.90)
theta            = 180
phi              = 15.5

x                = seq(from=limits.Lgold[1], to=limits.Lgold[2], length.out=100)
z                = matrix(NA,h,99)
for (i in 1:h){
  z[i,]          = hist(Y.h[i,1,], breaks=x, plot=FALSE)$density
}
x                = hist(Y.h[i,1,], breaks=x, plot=FALSE)$mids
yy               = 1:h
z                = t(z)

f4               = persp3D(x=x, y=yy, z=z, phi=phi, theta=theta, 
                           xlab="\ngold[t+h|t]", ylab="h", zlab="\npredictive densities of cpi",
                           shade=NA, border=NA, ticktype="detailed", nticks=3,cex.lab=1,
                           col=NA,plot=FALSE)
perspbox (x=x, y=yy, z=z, bty="f", col.axis="black", phi=phi, theta=theta, 
          xlab="\nlog.gold[t+h|t]", ylab="h", zlab="\npredictive densities of gold", 
          ticktype="detailed", nticks=3,cex.lab=1, col = NULL, plot = TRUE)

polygon3D(x=c(interval.Lgold.f[1,],interval.Lgold.f[2,h:1]),  y=c(1:h,h:1), 
          z=rep(0,2*h), 
          col = blue2, NAcol = blue5, border = NA, add = TRUE, plot = TRUE)

for (i in 1:h){
  f4.l = trans3d(x=x, y=yy[i], z=z[,i], pmat=f4)
  lines(f4.l, lwd=1, col=blue5)
}
f4.l1 = trans3d(x=point.Lgold.f, y=yy, z=0, pmat=f4)
lines(f4.l1, lwd=1.5, col=blue4)

# Log NASDAQ INDEX forecasting 
limits.LNasIndex      = range(Y.h[,5,])
point.LNasIndex.f     = apply(Y.h[,5,],1,mean)

interval.LNasIndex.f  = apply(Y.h[,5,],1,hdi,credMass=0.90)
theta            = 180
phi              = 15.5

x                = seq(from=limits.LNasIndex[1], to=limits.LNasIndex[2], length.out=100)
z                = matrix(NA,h,99)
for (i in 1:h){
  z[i,]          = hist(Y.h[i,5,], breaks=x, plot=FALSE)$density
}
x                = hist(Y.h[i,5,], breaks=x, plot=FALSE)$mids
yy               = 1:h
z                = t(z)

f4               = persp3D(x=x, y=yy, z=z, phi=phi, theta=theta, 
                           xlab="\nLNasIndex[t+h|t]", ylab="h", zlab="\npredictive densities of LNasIndex",
                           shade=NA, border=NA, ticktype="detailed", nticks=3,cex.lab=1,
                           col=NA,plot=FALSE)
perspbox (x=x, y=yy, z=z, bty="f", col.axis="black", phi=phi, theta=theta, 
          xlab="\nlog.NasIndex[t+h|t]", ylab="h", zlab="\npredictive densities of LNasIndex", 
          ticktype="detailed", nticks=3,cex.lab=1, col = NULL, plot = TRUE)

polygon3D(x=c(interval.LNasIndex.f[1,],interval.LNasIndex.f[2,h:1]),  y=c(1:h,h:1), 
          z=rep(0,2*h), 
          col = blue2, NAcol = blue5, border = NA, add = TRUE, plot = TRUE)

for (i in 1:h){
  f4.l = trans3d(x=x, y=yy[i], z=z[,i], pmat=f4)
  lines(f4.l, lwd=1, col=blue5)
}
f4.l1 = trans3d(x=point.LNasIndex.f, y=yy, z=0, pmat=f4)
lines(f4.l1, lwd=1.5, col=blue4)

```

```{r}
lgold.point.f    = apply(Y.h[,1,],1,mean)
lgold.interval.f = apply(Y.h[,1,],1,hdi,credMass=0.90)
lgold.range      = range(y[,1],lgold.interval.f)

par(mfrow=c(1,1), mar=rep(3,4),cex.axis=1.5)
plot(1:(length(y[,1])+h),c(y[,1],lgold.point.f), type="l", ylim=lgold.range, axes=FALSE, xlab="", ylab="", lwd=2, col=blue1)
axis(1,c(3,51,111,171,231,267,nrow(y),nrow(y)+h),c("2000","2005","2010","2015","2020","2023","",""), col=blue1)
axis(2,c(lgold.range[1],mean(lgold.range),lgold.range[2]),c("","lgold",""), col=blue1)
abline(v=284, col=blue1)
polygon(c(length(y[,1]):(length(y[,1])+h),(length(y[,1]):(length(y[,1])+h))[21:1]),
        c(y[284,1],lgold.interval.f[1,],lgold.interval.f[2,20:1],y[284,1]),
        col=blue1.shade1, border=blue1.shade1)

LNasIndex.point.f    = apply(Y.h[,5,],1,mean)
LNasIndex.interval.f = apply(Y.h[,5,],1,hdi,credMass=0.90)
LNasIndex.range      = range(y[,5],LNasIndex.interval.f)

par(mfrow=c(1,1), mar=rep(3,4),cex.axis=1.5)
plot(1:(length(y[,5])+h),c(y[,5],LNasIndex.point.f), type="l", ylim=LNasIndex.range, axes=FALSE, xlab="", ylab="", lwd=2, col=blue1)
axis(1,c(3,51,111,171,231,267,nrow(y),nrow(y)+h),c("2000","2005","2010","2015","2020","2023","",""), col=blue1)
axis(2,c(LNasIndex.range[1],mean(LNasIndex.range),LNasIndex.range[2]),c("","LNasIndex",""), col=blue1)
abline(v=284, col=blue1)
polygon(c(length(y[,5]):(length(y[,5])+h),(length(y[,5]):(length(y[,5])+h))[21:1]),
        c(y[284,1],LNasIndex.interval.f[1,],LNasIndex.interval.f[2,20:1],y[284,1]),
        col=blue1.shade1, border=blue1.shade1)

```

```{r}

combined_range = range(c(y[,1], y[,5], lgold.interval.f, LNasIndex.interval.f))

par(mfrow=c(1,1), mar=rep(3,4), cex.axis=1.5)

plot(1:(length(y[,1])+h), c(y[,1], lgold.point.f), type="l", ylim=combined_range, col="blue", lwd=2, xlab="", ylab="")
polygon(c(length(y[,1]):(length(y[,1])+h), (length(y[,1]):(length(y[,1])+h))[21:1]),
        c(y[284,1], lgold.interval.f[1,], lgold.interval.f[2,20:1], y[284,1]),
        col=rgb(0, 0, 255, 100, max=255), border=rgb(0, 0, 255, 100, max=255))

lines(1:(length(y[,5])+h), c(y[,5], LNasIndex.point.f), type="l", ylim=combined_range, col="red", lwd=2, xlab="", ylab="")
polygon(c(length(y[,5]):(length(y[,5])+h), (length(y[,5]):(length(y[,5])+h))[21:1]),
        c(y[284,5], LNasIndex.interval.f[1,], LNasIndex.interval.f[2,20:1], y[284,5]),
        col=rgb(255, 0, 0, 100, max=255), border=rgb(255, 0, 0, 100, max=255))
abline(v=284, col=green6)

```

Above graphs we conducted a time series forecast analysis on the gold price (lgold) and the Nasdaq index (LNasIndex) and plotted a comparison chart between the actual data and the forecast results.

Gold price (lgold) From 2000 to 2024, the gold price showed an overall upward trend. The blue shaded area represents the 90% of confidence interval of the forecast value, which gradually widens over time, indicating an increase in forecast uncertainty. The forecast trend line (solid blue line) indicates that the gold price may continue to rise, but the dispersion of the forecast values indicates that future price fluctuations are large.

Nasdaq index (LNasIndex) Same as lgold, the Nasdaq index also showed an upward trend, although it fluctuated greatly during the period. The red shaded area represents the 90% of confidence interval of the forecast value, which indicating an increase in forecast uncertainty. The forecast trend line (solid red line) shows that the Nasdaq index may continue to rise in long run, but there is also great uncertainty.

The width of the confidence interval shows that the range of future changes will increase as the forecast period increases. Investors should take these uncertainties into account when making decisions and conduct corresponding risk management.

## Extension Model

Due to the tediousness of computer calculations, we simplified the number of iterations to extend the predictions of the basic model. Although the results are not satisfactory, they also assist and strengthen the views we have obtained in the basic model.

### Laplace Ditribution of errors

```{r}
## Function Applying
S1=1
posterior.extend.draws     = posterior.draws.extended(total_S = 100, Y=Y, X=X)
A.posterior.ext.simu       = posterior.extend.draws[["A.posterior.exten"]]
Sigma.posterior.ext.simu   = posterior.extend.draws[["Sigma.posterior.exten"]]

```

```{r}
A.posterior.simu.mean = head(round(apply(A.posterior.ext.simu, 1:2, mean), 6),12)
Sigma.posterior.simu.mean = round(apply(Sigma.posterior.ext.simu, 1:2, mean),6)      
```

```{r }
kable(A.posterior.simu.mean, align = "c") %>% 
  kable_styling(font_size = 8, 
                fixed_thead = TRUE, 
                full_width = FALSE, 
                position = "center",
                latex_options = c("HOLD_position"),
                bootstrap_options = c("striped", "hover", "bordered", "responsive", "dark"))
```

```{r}
kable(Sigma.posterior.simu.mean, align = "c") %>% 
  kable_styling(font_size = 8, 
                fixed_thead = TRUE, 
                full_width = FALSE, 
                position = "center",
                latex_options = c("HOLD_position"),
                bootstrap_options = c("striped", "hover", "bordered", "responsive", "dark"))
```

```{r }

par(mfrow=c(2,2), mar=c(4,4,2,2))
plot.ts(A.posterior.ext.simu [2,1,], xlab = "Simulation times S", ylab = "lgold", col = "blue") 
hist(A.posterior.ext.simu [2,1,], xlab = "lgold", col = "blue", main = '')
plot.ts(A.posterior.ext.simu [2,5,], xlab = "Simulation times S", ylab = "lNesdaq", col = "green") 
hist(A.posterior.ext.simu [2,5,], xlab = "lNesdaq", col = "green", main = '')

```

```{r}
## two-year ahead forecasting h=24
# set up
h                 = 24
S                 = 100
Y.h.ext           = array(NA,c(h,N,S))

rate <- 1/2  

# sampling predictive density
for (s in 1:S){
  A.posterior.draw         = A.posterior.ext.simu[,,s]
  Sigma.posterior.draw     = Sigma.posterior.ext.simu[,,s]
    x.Ti                   = Y[(nrow(Y)-p+1):nrow(Y),]
    x.Ti                   = x.Ti[p:1,]
  for (i in 1:h){
    x.T                    = c(1,as.vector(t(x.Ti)))
    random_number <- rexp(1, rate = rate)
    Y.f                    = rmvnorm(1, mean = x.T%*%A.posterior.draw, sigma=random_number*Sigma.posterior.draw)
      x.Ti                 = rbind(Y.f,x.Ti[1:(p-1),])
    Y.h.ext[i,,s]          = Y.f[1:N]
  }
}
```

```{r}
lgold.point.f    = apply(Y.h.ext[,1,],1,mean)
lgold.interval.f = apply(Y.h.ext[,1,],1,hdi,credMass=0.90)
lgold.range      = range(y[,1],lgold.interval.f)

par(mfrow=c(1,1), mar=rep(3,4),cex.axis=1.5)
plot(1:(length(y[,1])+h),c(y[,1],lgold.point.f), type="l", ylim=lgold.range, axes=FALSE, xlab="", ylab="", lwd=2, col=blue1)
axis(1,c(3,51,111,171,231,267,nrow(y),nrow(y)+h),c("2000","2005","2010","2015","2020","2023","",""), col=blue1)
axis(2,c(lgold.range[1],mean(lgold.range),lgold.range[2]),c("","lgold",""), col=blue1)
abline(v=284, col=blue1)
polygon(c(length(y[,1]):(length(y[,1])+h),(length(y[,1]):(length(y[,1])+h))[21:1]),
        c(y[284,1],lgold.interval.f[1,],lgold.interval.f[2,20:1],y[284,1]),
        col=blue1.shade1, border=blue1.shade1)

LNasIndex.point.f    = apply(Y.h.ext[,5,],1,mean)
LNasIndex.interval.f = apply(Y.h.ext[,5,],1,hdi,credMass=0.90)
LNasIndex.range      = range(y[,5],LNasIndex.interval.f)

par(mfrow=c(1,1), mar=rep(3,4),cex.axis=1.5)
plot(1:(length(y[,5])+h),c(y[,5],LNasIndex.point.f), type="l", ylim=LNasIndex.range, axes=FALSE, xlab="", ylab="", lwd=2, col=blue1)
axis(1,c(3,51,111,171,231,267,nrow(y),nrow(y)+h),c("2000","2005","2010","2015","2020","2023","",""), col=blue1)
axis(2,c(LNasIndex.range[1],mean(LNasIndex.range),LNasIndex.range[2]),c("","LNasIndex",""), col=blue1)
abline(v=284, col=blue1)
polygon(c(length(y[,5]):(length(y[,5])+h),(length(y[,5]):(length(y[,5])+h))[21:1]),
        c(y[284,1],LNasIndex.interval.f[1,],LNasIndex.interval.f[2,20:1],y[284,1]),
        col=blue1.shade1, border=blue1.shade1)
```

```{r}
combined_range = range(c(y[,1], y[,5], lgold.interval.f, LNasIndex.interval.f))

par(mfrow=c(1,1), mar=rep(3,4), cex.axis=1.5)

plot(1:(length(y[,1])+h), c(y[,1], lgold.point.f), type="l", ylim=combined_range, col="blue", lwd=2, xlab="", ylab="")
polygon(c(length(y[,1]):(length(y[,1])+h), (length(y[,1]):(length(y[,1])+h))[21:1]),
        c(y[284,1], lgold.interval.f[1,], lgold.interval.f[2,20:1], y[284,1]),
        col=rgb(0, 0, 255, 100, max=255), border=rgb(0, 0, 255, 100, max=255))

lines(1:(length(y[,5])+h), c(y[,5], LNasIndex.point.f), type="l", ylim=combined_range, col="red", lwd=2, xlab="", ylab="")
polygon(c(length(y[,5]):(length(y[,5])+h), (length(y[,5]):(length(y[,5])+h))[21:1]),
        c(y[284,5], LNasIndex.interval.f[1,], LNasIndex.interval.f[2,20:1], y[284,5]),
        col=rgb(255, 0, 0, 100, max=255), border=rgb(255, 0, 0, 100, max=255))
abline(v=284, col=green6)

```

### Stochastic Volatility

```{r}
## Function Applying
S1 = 1
S2 = 100
posterior.sample.draws = DrawPosterior.sv(Y,X,S1,S2)
A.posterior.simu       = posterior.sample.draws$A.posterior
Sigma.posterior.simu   = posterior.sample.draws$Sigma.posterior
```

```{r}
A.posterior.simu.mean = head(round(apply(A.posterior.simu, 1:2, mean), 6),12)
Sigma.posterior.simu.mean = round(apply(Sigma.posterior.simu, 1:2, mean),6)      
```

```{r }
kable(A.posterior.simu.mean, align = "c") %>% 
  kable_styling(font_size = 8, 
                fixed_thead = TRUE, 
                full_width = FALSE, 
                position = "center",
                latex_options = c("HOLD_position"),
                bootstrap_options = c("striped", "hover", "bordered", "responsive", "dark"))
```

```{r}
kable(Sigma.posterior.simu.mean, align = "c") %>% 
  kable_styling(font_size = 8, 
                fixed_thead = TRUE, 
                full_width = FALSE, 
                position = "center",
                latex_options = c("HOLD_position"),
                bootstrap_options = c("striped", "hover", "bordered", "responsive", "dark"))
```

```{r}
par(mfrow=c(2,2), mar=c(4,4,2,2))
plot.ts(A.posterior.simu[2,1,], xlab = "Simulation times S", ylab = "lgold", col = blue1) 
hist(A.posterior.simu[2,1,], xlab = "lgold", col = blue1, main = '')
plot.ts(A.posterior.simu[2,5,], xlab = "Simulation times S", ylab = "lNesdaq", col = green1) 
hist(A.posterior.simu[2,5,], xlab = "lNesdaq", col = green1, main = '')

```

```{r}
par(mfrow=c(2,2), mar=c(4,4,2,2))
plot.ts(A.posterior.simu[2,3,], xlab = "Simulation times S", ylab = "TreasuryBill5Years", col = blue2) 
hist(A.posterior.simu[2,3,], xlab = "TreasuryBill5Years", col = blue2, main = '')
plot.ts(A.posterior.simu[2,6,], xlab = "Simulation times S", ylab = "M2 Rates", col = green3) 
hist(A.posterior.simu[2,6,], xlab = "M2 Rates", col = green3, main = '')
```

```{r}
## Two-year ahead forecasting h=24
# set up
h                      = 24
S                      = 100
Y.h                    = array(NA,c(h,N,S))

# sampling predictive density
for (s in 1:S){
  A.posterior.draw     = A.posterior.simu[,,s]
  Sigma.posterior.draw = Sigma.posterior.simu[,,s]
    x.Ti               = Y[(nrow(Y)-p+1):nrow(Y),]
    x.Ti               = x.Ti[p:1,]
  for (i in 1:h){
    x.T                = c(1,as.vector(t(x.Ti)))
    Y.f                = rmvnorm(1, mean = x.T%*%A.posterior.draw, sigma=Sigma.posterior.draw)
    x.Ti               = rbind(Y.f,x.Ti[1:(p-1),])
    Y.h[i,,s]          = Y.f[1:N]
  }
}
```

```{r}
lgold.point.f    = apply(Y.h[,1,],1,mean)
lgold.interval.f = apply(Y.h[,1,],1,hdi,credMass=0.90)
lgold.range      = range(y[,1],lgold.interval.f)

par(mfrow=c(1,1), mar=rep(3,4),cex.axis=1.5)
plot(1:(length(y[,1])+h),c(y[,1],lgold.point.f), type="l", ylim=lgold.range, axes=FALSE, xlab="", ylab="", lwd=2, col=blue1)
axis(1,c(3,51,111,171,231,267,nrow(y),nrow(y)+h),c("2000","2005","2010","2015","2020","2023","",""), col=blue1)
axis(2,c(lgold.range[1],mean(lgold.range),lgold.range[2]),c("","lgold",""), col=blue1)
abline(v=284, col=blue1)
polygon(c(length(y[,1]):(length(y[,1])+h),(length(y[,1]):(length(y[,1])+h))[21:1]),
        c(y[284,1],lgold.interval.f[1,],lgold.interval.f[2,20:1],y[284,1]),
        col=blue1.shade1, border=blue1.shade1)

LNasIndex.point.f    = apply(Y.h[,5,],1,mean)
LNasIndex.interval.f = apply(Y.h[,5,],1,hdi,credMass=0.90)
LNasIndex.range      = range(y[,5],LNasIndex.interval.f)

par(mfrow=c(1,1), mar=rep(3,4),cex.axis=1.5)
plot(1:(length(y[,5])+h),c(y[,5],LNasIndex.point.f), type="l", ylim=LNasIndex.range, axes=FALSE, xlab="", ylab="", lwd=2, col=blue1)
axis(1,c(3,51,111,171,231,267,nrow(y),nrow(y)+h),c("2000","2005","2010","2015","2020","2023","",""), col=blue1)
axis(2,c(LNasIndex.range[1],mean(LNasIndex.range),LNasIndex.range[2]),c("","LNasIndex",""), col=blue1)
abline(v=284, col=blue1)
polygon(c(length(y[,5]):(length(y[,5])+h),(length(y[,5]):(length(y[,5])+h))[21:1]),
        c(y[284,1],LNasIndex.interval.f[1,],LNasIndex.interval.f[2,20:1],y[284,1]),
        col=blue1.shade1, border=blue1.shade1)

```

```{r}

combined_range = range(c(y[,1], y[,5], lgold.interval.f, LNasIndex.interval.f))

par(mfrow=c(1,1), mar=rep(3,4), cex.axis=1.5)

plot(1:(length(y[,1])+h), c(y[,1], lgold.point.f), type="l", ylim=combined_range, col="blue", lwd=2, xlab="", ylab="")
polygon(c(length(y[,1]):(length(y[,1])+h), (length(y[,1]):(length(y[,1])+h))[21:1]),
        c(y[284,1], lgold.interval.f[1,], lgold.interval.f[2,20:1], y[284,1]),
        col=rgb(0, 0, 255, 100, max=255), border=rgb(0, 0, 255, 100, max=255))

lines(1:(length(y[,5])+h), c(y[,5], LNasIndex.point.f), type="l", ylim=combined_range, col="red", lwd=2, xlab="", ylab="")
polygon(c(length(y[,5]):(length(y[,5])+h), (length(y[,5]):(length(y[,5])+h))[21:1]),
        c(y[284,5], LNasIndex.interval.f[1,], LNasIndex.interval.f[2,20:1], y[284,5]),
        col=rgb(255, 0, 0, 100, max=255), border=rgb(255, 0, 0, 100, max=255))
abline(v=284, col=green6)

```

Both models percent not enough smooth in A and $\Sigma$, but support our results in base models. Under extension models, the serial variance become variate over time and then the lNasdaq performance is much worse than the lgold. This provide a basic understanding of our current society, a high level of uncertainty and worse economies environment.

# Conclusion:

In Conclusion, we could clearly see that there is a significant rising in log gold price in our BVARs models under different scenario. For next 24 periods, gold futures as a safe haven increase much more then Nasdaq Index, which treated as market returns in this research, and became flat (decrease in rate) in log price. Even the log Nasdaq Index in short run does not have a good performance, there still a strong increase in long run within 90% of confidence intervals. This quite reflects to the real world at this time, a boom of uncertainty and turn down of global economies.

However, due to lack of data sources and update, our predictions and forecasting are much behind of real world. Our theories still proved by results of real world and prove our reliable and robustness of our BVARs model.

In our models underling different scenarios assumptions, Gold still provide a strong safe-haven assets' nature. That's suggest the topic that Gold still is a good safe assets with evidence base. Those forecasting with 90% of confidence intervals provide a nature of current global economies: a high level of uncertainty and risk. This provide a sight for global investors for risk management.

# References {.unnumbered}
